[{"body":"","categories":"","description":"","excerpt":"","ref":"/blog/","tags":"","title":"Blog"},{"body":"\nApache HoraeDB™ (incubating) is a high-performance, distributed, cloud native time-series database.\nMotivation In the classic timeseries database, the Tag columns (InfluxDB calls them Tag and Prometheus calls them Label) are normally indexed by generating an inverted index. However, it is found that the cardinality of Tag varies in different scenarios. And in some scenarios the cardinality of Tag is very high, and it takes a very high cost to store and retrieve the inverted index. On the other hand, it is observed that scanning+pruning often used by the analytical databases can do a good job to handle such these scenarios.\nThe basic design idea of HoraeDB is to adopt a hybrid storage format and the corresponding query method for a better performance in processing both timeseries and analytic workloads.\n","categories":"","description":"","excerpt":"\nApache HoraeDB™ (incubating) is a high-performance, distributed, …","ref":"/docs/","tags":"","title":"Docs"},{"body":"This page shows you how to get started with HoraeDB quickly. You’ll start a standalone HoraeDB server, and then insert and read some sample data using SQL.\nStart server HoraeDB docker image is the easiest way to get started, if you haven’t installed Docker, go there to install it first.\nNote: please choose tag version \u003e= v1.0.0, others are mainly for testing.\nYou can use command below to start a standalone server\n1 2 3 4 5 docker run -d --name horaedb-server \\ -p 8831:8831 \\ -p 3307:3307 \\ -p 5440:5440 \\ ghcr.io/apache/horaedb-server:nightly-20231222-f57b3827 HoraeDB will listen three ports when start:\n8831, gRPC port 3307, MySQL port 5440, HTTP port The easiest to use is HTTP, so sections below will use it for demo. For production environments, gRPC/MySQL are recommended.\nCustomize docker configuration Refer the command as below, you can customize the configuration of horaedb-server in docker, and mount the data directory /data to the hard disk of the docker host machine.\nwget -c https://raw.githubusercontent.com/apache/incubator-horaedb/main/docs/minimal.toml -O horaedb.toml sed -i 's/\\/tmp\\/horaedb/\\/data/g' horaedb.toml docker run -d --name horaedb-server \\ -p 8831:8831 \\ -p 3307:3307 \\ -p 5440:5440 \\ -v ./horaedb.toml:/etc/horaedb/horaedb.toml \\ -v ./data:/data \\ ghcr.io/apache/horaedb-server:nightly-20231222-f57b3827 Write and read data Create table 1 2 3 4 5 6 7 8 9 10 11 curl --location --request POST 'http://127.0.0.1:5440/sql' \\ -d ' CREATE TABLE `demo` ( `name` string TAG, `value` double NOT NULL, `t` timestamp NOT NULL, timestamp KEY (t)) ENGINE=Analytic with (enable_ttl=\"false\") ' Write data 1 2 3 4 5 curl --location --request POST 'http://127.0.0.1:5440/sql' \\ -d ' INSERT INTO demo (t, name, value) VALUES (1651737067000, \"horaedb\", 100) ' Read data 1 2 3 4 5 6 7 curl --location --request POST 'http://127.0.0.1:5440/sql' \\ -d ' SELECT * FROM `demo` ' Show create table 1 2 3 4 curl --location --request POST 'http://127.0.0.1:5440/sql' \\ -d ' SHOW CREATE TABLE `demo` ' Drop table 1 2 3 4 curl --location --request POST 'http://127.0.0.1:5440/sql' \\ -d ' DROP TABLE `demo` ' Using the SDKs See sdk\nNext Step Congrats, you have finished this tutorial. For more information about HoraeDB, see the following:\nSQL Syntax Deployment Operation ","categories":"","description":"","excerpt":"This page shows you how to get started with HoraeDB quickly. You’ll …","ref":"/docs/getting-started/","tags":"","title":"Getting Started"},{"body":"本章介绍如何快速启动 HoraeDB。在这里你将会学到启动一个单机模式的 HoraeDB，然后使用 SQL 写入一些数据并查询结果。\n启动 使用 HoraeDB docker 镜像 是一种最简单的启动方式；如果你还没有安装 Docker，请首先参考 这里 安装 Docker。\n注意：请选择一个大于等于 v1.0.0 的 tag 镜像。\n使用如下命令安装并启动一个单机版 HoraeDB。\n1 2 3 4 5 docker run -d --name horaedb-server \\ -p 8831:8831 \\ -p 3307:3307 \\ -p 5440:5440 \\ ghcr.io/apache/horaedb-server:nightly-20231222-f57b3827 启动后 HoraeDB 会监听如下端口：\n8831, gRPC port 3307, MySQL port 5440, HTTP port HTTP 协议是最简单的交互方式，接下来的演示会使用 HTTP 协议进行介绍。不过在生产环境，我们推荐使用 gRPC/MySQL。\n自定义 docker 的配置 参考如下命令，可以自定义 docker 中 horaedb-server 的配置，并把数据目录 /data 挂载到 docker 母机的硬盘上。\nwget -c https://raw.githubusercontent.com/apache/incubator-horaedb/main/docs/minimal.toml -O horaedb.toml sed -i 's/\\/tmp\\/horaedb/\\/data/g' horaedb.toml docker run -d --name horaedb-server \\ -p 8831:8831 \\ -p 3307:3307 \\ -p 5440:5440 \\ -v ./horaedb.toml:/etc/horaedb/horaedb.toml \\ -v ./data:/data \\ ghcr.io/apache/horaedb-server:nightly-20231222-f57b3827 写入和查询数据 建表 1 2 3 4 5 6 7 8 9 10 11 curl --location --request POST 'http://127.0.0.1:5440/sql' \\ -d ' CREATE TABLE `demo` ( `name` string TAG, `value` double NOT NULL, `t` timestamp NOT NULL, timestamp KEY (t)) ENGINE=Analytic with (enable_ttl=\"false\") ' 写数据 1 2 3 4 5 curl --location --request POST 'http://127.0.0.1:5440/sql' \\ -d ' INSERT INTO demo (t, name, value) VALUES (1651737067000, \"horaedb\", 100) ' 查询 1 2 3 4 5 6 7 curl --location --request POST 'http://127.0.0.1:5440/sql' \\ -d ' SELECT * FROM `demo` ' 展示建表语句 1 2 3 4 curl --location --request POST 'http://127.0.0.1:5440/sql' \\ -d ' SHOW CREATE TABLE `demo` ' 删除表 1 2 3 4 curl --location --request POST 'http://127.0.0.1:5440/sql' \\ -d ' DROP TABLE `demo` ' 使用 SDK 当前我们支持多种开发语言 SDK，例如 Java，Rust，Python, Go 等, 具体使用方式请参考 sdk。\n下一步 恭喜你，你已经学习了 HoraeDB 的简单使用。关于 HoraeDB 的更多信息，请参见以下内容。\nSQL 语法 部署文档 运维文档 ","categories":"","description":"","excerpt":"本章介绍如何快速启动 HoraeDB。在这里你将会学到启动一个单机模式的 HoraeDB，然后使用 SQL 写入一些数据并查询结果。\n启动  …","ref":"/cn/docs/getting-started/","tags":"","title":"快速开始"},{"body":"\nApache HoraeDB™ (incubating) 是一款高性能、分布式的云原生时序数据库。\n愿景 在经典的时序数据库中，Tag 列（InfluxDB 称为 Tag，Prometheus 称为 Label）通常使用倒排来进行索引。 我们发现在不同的情况下，Tag 的基数差异很大。在某些情况下，Tag 的基数非常高，存储和检索倒排索引的成本非常高。 同时，我们发现分析型数据库经常使用的扫描+剪枝可以很好地处理这些场景。\nHoraeDB 的基础设计思想是采用混合存储格式和相应的查询方法，以便在处理时序和分析场景时都获得更好的性能。\n如何使用 HoraeDB？ 查看 快速开始 掌握快速使用 HoraeDB 的方式 HoraeDB 支持的数据模型请查看 Data Model SQL 使用相关请查看这里 SDK 使用请查看这里 ","categories":"","description":"","excerpt":"\nApache HoraeDB™ (incubating) 是一款高性能、分布式的云原生时序数据库。\n愿景 在经典的时序数据库中，Tag  …","ref":"/cn/docs/","tags":"","title":"文档"},{"body":"This chapter introduces the data model of HoraeDB.\n","categories":"","description":"","excerpt":"This chapter introduces the data model of HoraeDB.\n","ref":"/docs/user-guide/sql/model/","tags":"","title":"Data Model"},{"body":"本文目标 为想了解更多关于 HoraeDB 但不知道从何入手的开发者提供 HoraeDB 的概览 简要介绍 HoraeDB 的主要模块以及这些模块之间的联系，但不涉及它们实现的细节 动机 HoraeDB 是一个时序数据库，与经典时序数据库相比，HoraeDB 的目标是能够同时处理时序型和分析型两种模式的数据，并提供高效的读写。\n在经典的时序数据库中，Tag 列（ InfluxDB 称之为 Tag，Prometheus 称之为 Label）通常会对其生成倒排索引，但在实际使用中，Tag 的基数在不同的场景中是不一样的 ———— 在某些场景下，Tag 的基数非常高（这种场景下的数据，我们称之为分析型数据），而基于倒排索引的读写要为此付出很高的代价。而另一方面，分析型数据库常用的扫描 + 剪枝方法，可以比较高效地处理这样的分析型数据。\n因此 HoraeDB 的基本设计理念是采用混合存储格式和相应的查询方法，从而达到能够同时高效处理时序型数据和分析型数据。\n架构 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 ┌──────────────────────────────────────────┐ │ RPC Layer (HTTP/gRPC/MySQL) │ └──────────────────────────────────────────┘ ┌──────────────────────────────────────────┐ │ SQL Layer │ │ ┌─────────────────┐ ┌─────────────────┐ │ │ │ Parser │ │ Planner │ │ │ └─────────────────┘ └─────────────────┘ │ └──────────────────────────────────────────┘ ┌───────────────────┐ ┌───────────────────┐ │ Interpreter │ │ Catalog │ └───────────────────┘ └───────────────────┘ ┌──────────────────────────────────────────┐ │ Query Engine │ │ ┌─────────────────┐ ┌─────────────────┐ │ │ │ Optimizer │ │ Executor │ │ │ └─────────────────┘ └─────────────────┘ │ └──────────────────────────────────────────┘ ┌──────────────────────────────────────────┐ │ Pluggable Table Engine │ │ ┌────────────────────────────────────┐ │ │ │ Analytic │ │ │ │┌────────────────┐┌────────────────┐│ │ │ ││ Wal ││ Memtable ││ │ │ │└────────────────┘└────────────────┘│ │ │ │┌────────────────┐┌────────────────┐│ │ │ ││ Flush ││ Compaction ││ │ │ │└────────────────┘└────────────────┘│ │ │ │┌────────────────┐┌────────────────┐│ │ │ ││ Manifest ││ Object Store ││ │ │ │└────────────────┘└────────────────┘│ │ │ └────────────────────────────────────┘ │ │ ┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ │ │ Another Table Engine │ │ │ └ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ │ └──────────────────────────────────────────┘ 上图展示了 HoraeDB 单机版本的架构，下面将会介绍重要模块的细节。\nRPC 层 模块路径：https://github.com/apache/incubator-horaedb/tree/main/server\n当前的 RPC 支持多种协议，包括 HTTP、gRPC、MySQL。\n通常 HTTP 和 MySQL 用于调试 HoraeDB，手动查询和执行 DDL 操作（如创建、删除表等）。而 gRPC 协议可以被看作是一种用于高性能的定制协议，更适用于大量的读写操作。\nSQL 层 模块路径：https://github.com/apache/incubator-horaedb/tree/main/query_frontend\nSQL 层负责解析 SQL 并生成查询计划。\nHoraeDB 基于 sqlparser 提供了一种 SQL 方言，为了更好的适配时序数据，引入一些概念，包括 Tag 和 Timestamp。此外，利用 DataFusion，HoraeDB 不仅可以生成常规的逻辑计划，还可以生成自定义的计划来实现时序场景要求的特殊算子，例如为了适配 PromQL 协议而做的工作就是利用了这个特性。\nInterpreter 模块路径：https://github.com/apache/incubator-horaedb/tree/main/interpreters\nInterpreter 模块封装了 SQL 的 CRUD 操作。在查询流程中，一个 SQL 语句会经过解析，生成出对应的查询计划，然后便会在特定的解释器中执行，例如 SelectInterpreter、InsertInterpreter 等。\nCatalog 模块路径：https://github.com/apache/incubator-horaedb/tree/main/catalog_impls\nCatalog 实际上是管理元数据的模块，HoraeDB 采用的元数据分级与 PostgreSQL 类似：Catalog \u003e Schema \u003e Table，但目前它们只用作命名空间。\n目前，Catalog 和 Schema 在单机模式和分布式模式存在两种不同实现，因为一些生成 id 和持久化元数据的策略在这两种模式下有所不同。\n查询引擎 模块路径：https://github.com/apache/incubator-horaedb/tree/main/query_engine\n查询引擎负责优化和执行由 SQL 层解析出来的 SQL 计划，目前查询引擎实际上基于 DataFusion 来实现的。\n除了 SQL 的基本功能外，HoraeDB 还通过利用 DataFusion 提供的扩展接口，为某些特定的查询（比如 PromQL）构建了一些定制的查询协议和优化规则。\nPluggable Table Engine 模块路径：https://github.com/apache/incubator-horaedb/tree/main/table_engine\nTable Engine 是 HoraeDB 中用于管理表的存储引擎，其可插拔性是 HoraeDB 的一个核心设计，对于实现我们的一些长远目标（比如增加 Log 或 Tracing 类型数据的存储引擎）至关重要。HoraeDB 将会有多种 Table Engine 用于不同的工作负载，根据工作负载模式，应该选择最合适的存储引擎。\n现在对 Table Engine 的要求是：\n管理引擎下的所有共享资源： 内存 存储 CPU 管理表的元数据，如表的结构、表的参数选项； 能够提供 Table 实例，该实例可以提供 read 和 write 的能力； 负责 Table 实例的创建、打开、删除和关闭； …. 实际上，Table Engine 需要处理的事情有点复杂。现在在 HoraeDB 中，只提供了一个名为 Analytic 的 Table Engine，它在处理分析工作负载方面做得很好，但是在时序工作负载上还有很大的进步空间（我们计划通过添加一些帮助处理时序工作负载的索引来提高性能）。\n以下部分描述了 Analytic Table Engine 的详细信息。\nWAL 模块路径：https://github.com/apache/incubator-horaedb/tree/main/wal\nHoraeDB 处理数据的模型是 WAL + MemTable，最近写入的数据首先被写入 WAL，然后写入 MemTable，在 MemTable 中累积了一定数量的数据后，该数据将以便于查询的形式被重新构建，并存储到持久化设备上。\n目前，为 standalone 模式和分布式模式提供了三种 WAL 实现：\n对于 standalone 模式，WAL 基于 RocksDB，数据存储在本地磁盘上。 对于分布式模式，需要 WAL 作为一个分布式组件，负责新写入数据的可靠性，因此，我们现在提供了基于 OceanBase 的实现。 对于分布式模式，除了 OceanBase，我们还提供了一个更轻量级的基于 Apache Kafka 实现。 MemTable 模块路径：https://github.com/apache/incubator-horaedb/tree/main/analytic_engine/src/memtable\n由于 WAL 无法提供高效的查询，因此新写入的数据会存储一份到 Memtable 用于查询，并且在积累了一定数量后，HoraeDB 将 MemTable 中的数据组织成便于查询的存储格式（SST）并存储到持久化设备中。\nMemTable 的当前实现基于 agatedb 的 skiplist。它允许并发读取和写入，并且可以根据 Arena 控制内存使用。\nFlush 模块路径：https://github.com/apache/incubator-horaedb/blob/main/analytic_engine/src/instance/flush_compaction.rs\n当 MemTable 的内存使用量达到阈值时，Flush 操作会选择一些老的 MemTable，将其中的数据组织成便于查询的 SST 存储到持久化设备上。\n在刷新过程中，数据将按照一定的时间段（由表选项 Segment Duration 配置）进行划分，保证任何一个 SST 的所有数据的时间戳都属于同一个 Segment。实际上，这也是大多数时序数据库中常见的操作，按照时间维度组织数据，以加速后续的时间相关操作，如查询一段时间内的数据，清除超出 TTL 的数据等。\nCompaction 模块路径：https://github.com/apache/incubator-horaedb/tree/main/analytic_engine/src/compaction\nMemTable 的数据被刷新为 SST 文件，但最近刷新的 SST 文件可能非常小，而过小或过多的 SST 文件会导致查询性能不佳，因此，引入 Compaction 来重新整理 SST 文件，使多个较小的 SST 文件可以合并成较大的 SST 文件。\nManifest 模块路径：https://github.com/apache/incubator-horaedb/tree/main/analytic_engine/src/meta\nManifest 负责管理每个表的元数据，包括：\n表的结构和表的参数选项； 最新 Flush 过的 sequence number； 表的所有 SST 文件的信息。 现在 Manifest 是基于 WAL 和 Object Store 来实现的，新的改动会直接写入到 WAL，而为了避免元数据无限增长（实际上每次 Flush 操作都会触发更新），会对其写入的记录做快照，生成的快照会被持久化道 Object Store。\nObject Storage 模块路径：https://github.com/apache/incubator-horaedb/tree/main/components/object_store\nFlush 操作产生的 SST 文件需要持久化存储，而用于抽象持久化存储设备的就是 Object Storage，其中包括多种实现：\n基于本地文件系统； 基于阿里云 OSS。 HoraeDB 的分布式架构的一个核心特性就是存储和计算分离，因此要求 Object Storage 是一个高可用的服务，并独立于 HoraeDB。因此，像Amazon S3、阿里云 OSS等存储系统是不错的选择，未来还将计划实现在其他云服务提供商的存储系统上。\nSST 模块路径：https://github.com/apache/incubator-horaedb/tree/main/analytic_engine/src/sst\nSST 本身实际上是一种抽象，可以有多种具体实现。目前的实现是基于 Parquet，它是一种面向列的数据文件格式，旨在实现高效的数据存储和检索。\nSST 的格式对于数据检索非常关键，也是决定查询性能的关键所在。目前，我们基于 Parquet 的 SST 实现在处理分析型数据时表现良好，但目前在处理时序型数据上还有较高的提升空间。在我们的路线图中，我们将探索更多的存储格式，以便在两种类型的数据处理上都取得良好的性能。\nSpace 模块路径：https://github.com/apache/incubator-horaedb/blob/main/analytic_engine/src/space.rs\n在 Analytic Engine 中，有一个叫做 space 的概念，这里着重解释一下，以解决阅读源代码时出现的一些歧义。 实际上，Analytic Engine 没有 catalog 和 schema 的概念，只提供两个层级的关系：space 和 table。在实现中，上层的 schema id（要求在所有的 catalogs 中都应该是唯一的）实际上会直接映射成 space id。\nAnalytic Engine 中的 space 主要用于隔离不同租户的资源，如内存的使用。\nCritical Path 简要介绍了 HoraeDB 的一些重要模块后，我们将对代码中的一些关键路径进行描述，希望为有兴趣的开发人员在阅读代码时提供一些帮助。\nQuery 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 ┌───────┐ ┌───────┐ ┌───────┐ │ │──1──▶│ │──2──▶│ │ │Server │ │ SQL │ │Catalog│ │ │◀─10──│ │◀─3───│ │ └───────┘ └───────┘ └───────┘ │ ▲ 4│ 9│ │ │ ▼ │ ┌─────────────────────────────────────┐ │ │ │ Interpreter │ │ │ └─────────────────────────────────────┘ │ ▲ 5│ 8│ │ │ ▼ │ ┌──────────────────┐ │ │ │ Query Engine │ │ │ └──────────────────┘ │ ▲ 6│ 7│ │ │ ▼ │ ┌─────────────────────────────────────┐ │ │ │ Table Engine │ │ │ └─────────────────────────────────────┘ 以 SELECT SQL 为例，上图展示了查询过程，其中的数字表示模块之间调用的顺序。\n以下是详细流程：\nServer 模块根据请求使用的协议选择合适的 rpc 模块（可能是 HTTP、gRPC 或 mysql）来处理请求； 使用 parser 解析请求中的 sql ； 根据解析好的 sql 以及 catalog/schema 提供的元信息，通过 DataFusion 可以生成逻辑计划； 根据逻辑计划创建相应的 Interpreter，并由其执行逻辑计划； 对于正常 SELECT SQL 的逻辑计划，它将通过 SelectInterpreter 执行； 在 SelectInterpreter 中，特定的查询逻辑由 Query Engine 执行： 优化逻辑计划； 生成物理计划； 优化物理计划； 执行物理计划； 执行物理计划涉及到 Analytic Engine： 通过 Analytic Engine 提供的 Table 实例的 read 方法获取数据； 表数据的来源是 SST 和 Memtable，可以通过谓词下推进行提前过滤； 在检索到表数据后，Query Engine 将完成具体计算并生成最终结果； SelectInterpreter 获取结果并将其传输给 Protocol 模块； 协议模块完成转换结果后，Server 模块将其响应给客户端。 以下是v1.2.2的函数调用流程:\n┌───────────────────────◀─────────────┐ ┌───────────────────────┐ │ handle_sql │────────┐ │ │ parse_sql │ └───────────────────────┘ │ │ └────────────────┬──────┘ │ ▲ │ │ ▲ │ │ │ │ │ │ │ │ │ │ └36───┐ │ 11 1│ │ │ │ │ │ │ 8│ │ │ │ │ │ │ │ │ 10 │ │ │ │ │ │ │ ▼ │ │ │ │ ▼ ┌─────────────────┴─────┐ 9│ ┌┴─────┴────────────────┐───────12─────────▶┌───────────────────────┐ │maybe_forward_sql_query│ └────────▶│fetch_sql_query_output │ │ statement_to_plan │ └───┬───────────────────┘ └────┬──────────────────┘◀───────19─────────└───────────────────────┘ │ ▲ │ ▲ │ ▲ │ │ │ │ │ │ │ │ │ │ │ │ │ │ │ 35 13 18 2│ 7│ 20 │ │ │ │ │ │ │ │ │ │ │ │ │ │ │ │ │ │ │ ▼ │ ▼ │ ▼ │ ┌───────────────────────┐ ┌───────────────────────┐───────────6───────▶┌─────────────────┴─────┐ ┌─────────────────┴─────┐ │Planner::statement_to_p│ │ forward_with_endpoint │ │ forward │ │execute_plan_involving_│ │ lan │ └───────────────────────┘◀────────5──────────└───┬───────────────────┘ ┌──│ partition_table │◀────────┐ └───┬───────────────────┘ │ ▲ │ └───────────────────────┘ │ │ ▲ │ │ │ │ ▲ │ │ │ │ │ │ │ │ │ 14 17 ┌───────────────────────┐ │ 4│ │ │ │ │ │ │ ┌─────│ PhysicalPlan::execute │ 3│ │ │ 21 │ │ │ │ │ └───────────────────────┘◀──┐ │ │ │ │ 22 │ │ │ │ │ │ │ │ │ │ │ ▼ │ │ │ │ │ │ │ │ │ ┌────────────────────────┐ │ │ ▼ │ │ ▼ │ 34 │sql_statement_to_datafus│ │ ┌───────────────────────┐ 30 ┌─────────────────┴─────┐ │ ┌─────────────────┴─────┐ │ │ ion_plan │ 31 │ build_df_session_ctx │ │ │ route │ │ │ build_interpreter │ │ └────────────────────────┘ │ └────┬──────────────────┘ │ └───────────────────────┘ │ └───────────────────────┘ │ │ ▲ │ │ ▲ │ │ │ │ │ │ 27 26 │ 23 │ 15 16 │ ▼ │ │ │ │ │ │ └────▶┌────────────────┴──────┐ │ ┌───────────────────────┐ │ │ │ │ │ execute_logical_plan ├───┴────32────────▶│ execute │──────────┐ │ ┌───────────────────────┐ │ ▼ │ └────┬──────────────────┘◀────────────25────┴───────────────────────┘ 33 │ │interpreter_execute_pla│ │ ┌────────────────────────┐ │ ▲ ▲ └──────┴──▶│ n │────────┘ │SqlToRel::sql_statement_│ 28 │ └──────────24────────────────┴───────────────────────┘ │ to_datafusion_plan │ │ 29 └────────────────────────┘ ▼ │ ┌────────────────┴──────┐ │ optimize_plan │ └───────────────────────┘ 收到请求经过各种协议转换会转到handle_sql中执行,由于该请求可能是非本节点处理的，可能需要转发，进入maybe_forward_sql_query处理转发逻辑。 在maybe_forward_sql_query中构造好ForwardRequest后，调用forward 在forward中构造好RouteRequest,后调用route 使用route获取目的节点endpoint后回到forward 调用forward_with_endpoint将请求进行转发 回到forward 回到maybe_forward_sql_query 回到handle_sql 此时若是Local请求，调用fetch_sql_query_output进行处理 调用parse_sql将sql解析成Statment 回到fetch_sql_query_output 使用Statment调用statement_to_plan 在其中使用ctx和Statment构造Planner,调用Planner的statement_to_plan方法 planner中会对于请求的类别调用对应的planner方法，此时我们的sql是查询，会调用sql_statement_to_plan 调用sql_statement_to_datafusion_plan,其中会生成datafusion的对象，然后调用SqlToRel::sql_statement_to_plan SqlToRel::sql_statement_to_plan中会返回生成的逻辑计划 返回 返回 返回 调用execute_plan_involving_partition_table（使用默认配置情况下）进行该逻辑计划的后续优化和执行 调用build_interpreter生成Interpreter 返回 调用Interpreter的interpreter_execute_plan方法进行逻辑计划的执行。 调用对应执行函数，此时sql是查询，所以会调用SelectInterpreter的execute 调用execute_logical_plan，其中会调用build_df_session_ctx生成优化器 build_df_session_ctx中会使用config信息生成对应上下文,首先使用datafusion和自定义的一些优化规则(在logical_optimize_rules()中)生成逻辑计划优化器, 使用 apply_adapters_for_physical_optimize_rules生成物理计划优化器 将优化器返回 调用optimize_plan，使用刚刚生成的优化器首先进行逻辑计划的优化随后进行物理计划的优化 返回优化后的物理计划 执行物理计划 执行后返回 收集所有分片的结果后，返回 返回 返回 返回 返回给上层进行网络协议转化，最后返回给请求发送方 Write 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 ┌───────┐ ┌───────┐ ┌───────┐ │ │──1──▶│ │──2──▶│ │ │Server │ │ SQL │ │Catalog│ │ │◀─8───│ │◀─3───│ │ └───────┘ └───────┘ └───────┘ │ ▲ 4│ 7│ │ │ ▼ │ ┌─────────────────────────────────────┐ │ │ │ Interpreter │ │ │ └─────────────────────────────────────┘ │ ▲ │ │ │ │ │ │ │ │ ┌──────────────────┐ │ │ │ │ 5│ 6│ │ Query Engine │ │ │ │ │ │ │ └──────────────────┘ │ │ │ │ │ │ ▼ │ ┌─────────────────────────────────────┐ │ │ │ Table Engine │ │ │ └─────────────────────────────────────┘ 以 INSERT SQL 为例，上图展示了查询过程，其中的数字表示模块之间调用的顺序。\n以下是详细流程：\nServer 模块根据请求使用的协议选择合适的 rpc 模块（可能是 HTTP、gRPC 或 mysql）来处理请求； 使用 parser 解析请求中的 sql； 根据解析好的 sql 以及 catalog/schema 提供的元信息，通过 DataFusion 可以生成逻辑计划； 根据逻辑计划创建相应的 Interpreter ，并由其执行逻辑计划； 对于正常 INSERT SQL 的逻辑计划，它将通过 InsertInterpreter 执行； 在 InsertInterpreter 中，调用 Analytic Engine 提供的 Table 的 write 方法： 首先将数据写入 WAL； 然后写入 MemTable； 在写入 MemTable 之前，会检查内存使用情况。如果内存使用量过高，则会触发 Flush： 将一些旧的 MemTable 持久化为 SST； 将新的 SST 信息记录到 Manifest； 记录最新 Flush 的 WAL 序列号； 删除相应的 WAL 日志； Server 模块将执行结果响应给客户端。 ","categories":"","description":"","excerpt":"本文目标 为想了解更多关于 HoraeDB 但不知道从何入手的开发者提供 HoraeDB 的概览 简要介绍 HoraeDB 的主要模块以及这 …","ref":"/cn/docs/design/architecture/","tags":"","title":"HoraeDB 架构介绍"},{"body":"Note: This feature is for testing use only, not recommended for production use, related features may change in the future.\nThis guide shows how to deploy a HoraeDB cluster without HoraeMeta, but with static, rule-based routing.\nThe crucial point here is that HoraeDB server provides configurable routing function on table name so what we need is just a valid config containing routing rules which will be shipped to every HoraeDB instance in the cluster.\nTarget First, let’s assume that our target is to deploy a cluster consisting of two HoraeDB instances on the same machine. And a large cluster of more HoraeDB instances can be deployed according to the two-instance example.\nPrepare Config Basic Suppose the basic config of HoraeDB is:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 [server] bind_addr = \"0.0.0.0\" http_port = 5440 grpc_port = 8831 [logger] level = \"info\" [tracing] dir = \"/tmp/horaedb\" [analytic.storage.object_store] type = \"Local\" data_dir = \"/tmp/horaedb\" [analytic.wal] type = \"RocksDB\" data_dir = \"/tmp/horaedb\" In order to deploy two HoraeDB instances on the same machine, the config should choose different ports to serve and data directories to store data.\nSay the HoraeDB_0’s config is:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 [server] bind_addr = \"0.0.0.0\" http_port = 5440 grpc_port = 8831 [logger] level = \"info\" [tracing] dir = \"/tmp/horaedb_0\" [analytic.storage.object_store] type = \"Local\" data_dir = \"/tmp/horaedb_0\" [analytic.wal] type = \"RocksDB\" data_dir = \"/tmp/horaedb_0\" Then the HoraeDB_1’s config is:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 [server] bind_addr = \"0.0.0.0\" http_port = 15440 grpc_port = 18831 [logger] level = \"info\" [tracing] dir = \"/tmp/horaedb_1\" [analytic.storage.object_store] type = \"Local\" data_dir = \"/tmp/horaedb_1\" [analytic.wal] type = \"RocksDB\" data_dir = \"/tmp/horaedb_1\" Schema\u0026Shard Declaration Then we should define the common part – schema\u0026shard declaration and routing rules.\nHere is the config for schema\u0026shard declaration:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 [cluster_deployment] mode = \"NoMeta\" [[cluster_deployment.topology.schema_shards]] schema = 'public_0' [[cluster_deployment.topology.schema_shards.shard_views]] shard_id = 0 [cluster_deployment.topology.schema_shards.shard_views.endpoint] addr = '127.0.0.1' port = 8831 [[cluster_deployment.topology.schema_shards.shard_views]] shard_id = 1 [cluster_deployment.topology.schema_shards.shard_views.endpoint] addr = '127.0.0.1' port = 8831 [[cluster_deployment.topology.schema_shards]] schema = 'public_1' [[cluster_deployment.topology.schema_shards.shard_views]] shard_id = 0 [cluster_deployment.topology.schema_shards.shard_views.endpoint] addr = '127.0.0.1' port = 8831 [[cluster_deployment.topology.schema_shards.shard_views]] shard_id = 1 [cluster_deployment.topology.schema_shards.shard_views.endpoint] addr = '127.0.0.1' port = 18831 In the config above, two schemas are declared:\npublic_0 has two shards served by HoraeDB_0. public_1 has two shards served by both HoraeDB_0 and HoraeDB_1. Routing Rules Provided with schema\u0026shard declaration, routing rules can be defined and here is an example of prefix rule:\n1 2 3 4 [[cluster_deployment.route_rules.prefix_rules]] schema = 'public_0' prefix = 'prod_' shard = 0 This rule means that all the table with prod_ prefix belonging to public_0 should be routed to shard_0 of public_0, that is to say, HoraeDB_0. As for the other tables whose names are not prefixed by prod_ will be routed by hash to both shard_0 and shard_1 of public_0.\nBesides prefix rule, we can also define a hash rule:\n1 2 3 [[cluster_deployment.route_rules.hash_rules]] schema = 'public_1' shards = [0, 1] This rule tells HoraeDB to route public_1’s tables to both shard_0 and shard_1 of public_1, that is to say, HoraeDB0 and HoraeDB_1. And actually this is default routing behavior if no such rule provided for schema public_1.\nFor now, we can provide the full example config for HoraeDB_0 and HoraeDB_1:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 [server] bind_addr = \"0.0.0.0\" http_port = 5440 grpc_port = 8831 mysql_port = 3307 [logger] level = \"info\" [tracing] dir = \"/tmp/horaedb_0\" [analytic.storage.object_store] type = \"Local\" data_dir = \"/tmp/horaedb_0\" [analytic.wal] type = \"RocksDB\" data_dir = \"/tmp/horaedb_0\" [cluster_deployment] mode = \"NoMeta\" [[cluster_deployment.topology.schema_shards]] schema = 'public_0' [[cluster_deployment.topology.schema_shards.shard_views]] shard_id = 0 [cluster_deployment.topology.schema_shards.shard_views.endpoint] addr = '127.0.0.1' port = 8831 [[cluster_deployment.topology.schema_shards.shard_views]] shard_id = 1 [cluster_deployment.topology.schema_shards.shard_views.endpoint] addr = '127.0.0.1' port = 8831 [[cluster_deployment.topology.schema_shards]] schema = 'public_1' [[cluster_deployment.topology.schema_shards.shard_views]] shard_id = 0 [cluster_deployment.topology.schema_shards.shard_views.endpoint] addr = '127.0.0.1' port = 8831 [[cluster_deployment.topology.schema_shards.shard_views]] shard_id = 1 [cluster_deployment.topology.schema_shards.shard_views.endpoint] addr = '127.0.0.1' port = 18831 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 [server] bind_addr = \"0.0.0.0\" http_port = 15440 grpc_port = 18831 mysql_port = 13307 [logger] level = \"info\" [tracing] dir = \"/tmp/horaedb_1\" [analytic.storage.object_store] type = \"Local\" data_dir = \"/tmp/horaedb_1\" [analytic.wal] type = \"RocksDB\" data_dir = \"/tmp/horaedb_1\" [cluster_deployment] mode = \"NoMeta\" [[cluster_deployment.topology.schema_shards]] schema = 'public_0' [[cluster_deployment.topology.schema_shards.shard_views]] shard_id = 0 [cluster_deployment.topology.schema_shards.shard_views.endpoint] addr = '127.0.0.1' port = 8831 [[cluster_deployment.topology.schema_shards.shard_views]] shard_id = 1 [cluster_deployment.topology.schema_shards.shard_views.endpoint] addr = '127.0.0.1' port = 8831 [[cluster_deployment.topology.schema_shards]] schema = 'public_1' [[cluster_deployment.topology.schema_shards.shard_views]] shard_id = 0 [cluster_deployment.topology.schema_shards.shard_views.endpoint] addr = '127.0.0.1' port = 8831 [[cluster_deployment.topology.schema_shards.shard_views]] shard_id = 1 [cluster_deployment.topology.schema_shards.shard_views.endpoint] addr = '127.0.0.1' port = 18831 Let’s name the two different config files as config_0.toml and config_1.toml but you should know in the real environment the different HoraeDB instances can be deployed across different machines, that is to say, there is no need to choose different ports and data directories for different HoraeDB instances so that all the HoraeDB instances can share one exactly same config file.\nStart HoraeDBs After the configs are prepared, what we should to do is to start HoraeDB container with the specific config.\nJust run the commands below:\n1 2 sudo docker run -d -t --name horaedb_0 -p 5440:5440 -p 8831:8831 -v $(pwd)/config_0.toml:/etc/horaedb/horaedb.toml horaedb/horaedb-server sudo docker run -d -t --name horaedb_1 -p 15440:15440 -p 18831:18831 -v $(pwd)/config_1.toml:/etc/horaedb/horaedb.toml horaedb/horaedb-server After the two containers are created and starting running, read and write requests can be served by the two-instances HoraeDB cluster.\n","categories":"","description":"","excerpt":"Note: This feature is for testing use only, not recommended for …","ref":"/docs/user-guide/cluster_deployment/no_meta/","tags":"","title":"NoMeta"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/user-guide/sql/","tags":"","title":"SQL Syntax"},{"body":"本章介绍 HoraeDB 的 SQL 使用语法。\n","categories":"","description":"","excerpt":"本章介绍 HoraeDB 的 SQL 使用语法。\n","ref":"/cn/docs/user-guide/sql/","tags":"","title":"SQL 语法"},{"body":"作为一个开源的数据库，HoraeDB 可以部署在基于英特尔 /ARM 架构的服务器，以及常见的虚拟环境。\nOS status Ubuntu LTS 16.06 or later ✅ CentOS 7.3 or later ✅ Red Hat Enterprise Linux 7.3 or later 7.x releases ✅ macOS 11 or later ✅ Windows ❌ 生产环境下 , Linux 是首选平台。 macOS 主要用在开发环境。 ","categories":"","description":"","excerpt":"作为一个开源的数据库，HoraeDB 可以部署在基于英特尔 /ARM 架构的服务器，以及常见的虚拟环境。\nOS status Ubuntu …","ref":"/docs/dev/platform/","tags":"","title":"Supported Platform"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/user-guide/","tags":"","title":"User Guide"},{"body":"作为一个开源的数据库，HoraeDB 可以部署在基于英特尔 /ARM 架构的服务器，以及常见的虚拟环境。\nOS status Ubuntu LTS 16.06 or later ✅ CentOS 7.3 or later ✅ Red Hat Enterprise Linux 7.3 or later 7.x releases ✅ macOS 11 or later ✅ Windows ❌ 生产环境下 , Linux 是首选平台。 macOS 主要用在开发环境。 ","categories":"","description":"","excerpt":"作为一个开源的数据库，HoraeDB 可以部署在基于英特尔 /ARM 架构的服务器，以及常见的虚拟环境。\nOS status Ubuntu …","ref":"/cn/docs/dev/platform/","tags":"","title":"支持平台"},{"body":"本章介绍 HoraeDB 的数据模型。\n","categories":"","description":"","excerpt":"本章介绍 HoraeDB 的数据模型。\n","ref":"/cn/docs/user-guide/sql/model/","tags":"","title":"数据模型"},{"body":"","categories":"","description":"","excerpt":"","ref":"/cn/docs/user-guide/","tags":"","title":"用户指南"},{"body":"为了编译 HoraeDB, 首先需要安装相关的依赖（包括 Rust 的工具链)。\n依赖 Ubuntu 假设我们的开发环境是 Ubuntu20.04, 可以执行如下命令来安装所需的依赖。\n1 sudo apt install git curl gcc g++ libssl-dev pkg-config cmake protobuf-compiler 需要注意的是，项目的编译对 cmake 有版本要求。如果你的开发环境是旧的 Linux 发行版，有必要手动安装这些依赖项的高版本。\nmacOS 如果你的开发环境是 MacOS ，可以使用如下命令手动安装这些依赖项的高版本。\n安装命令行工具： 1 xcode-select --install 安装 cmake: 1 brew install cmake 安装 protobuf: 1 brew install protobuf Rust Rust 可以使用 rustup 来安装。 安装 Rust 后，进入 HoraeDB 工程目录，根据工具链文件指定的 Rust 版本会被自动下载。\n执行后，你需要添加环境变量来使用 Rust 工具链。只要把下面的命令放到你的~/.bashrc或~/.bash_profile中即可。\n1 source $HOME/.cargo/env 编译运行 编译 HoraeDB 命令如下:\ncargo build 然后可以使用特定的配置文件运行 HoraeDB。\n1 ./target/debug/horaedb-server --config ./docs/minimal.toml ","categories":"","description":"","excerpt":"为了编译 HoraeDB, 首先需要安装相关的依赖（包括 Rust 的工具链)。\n依赖 Ubuntu …","ref":"/cn/docs/dev/compile_run/","tags":"","title":"编译"},{"body":"In the Quick Start section, we have introduced the deployment of single HoraeDB instance.\nBesides, as a distributed timeseries database, multiple HoraeDB instances can be deployed as a cluster to serve with high availability and scalability.\nCurrently, work about the integration with kubernetes is still in process, so HoraeDB cluster can only be deployed manually. And there are two modes of cluster deployment:\n","categories":"","description":"","excerpt":"In the Quick Start section, we have introduced the deployment of …","ref":"/docs/user-guide/cluster_deployment/","tags":"","title":"Cluster Deployment"},{"body":"This document describes how we use conventional commit in our development.\nStructure We would like to structure our commit message like this:\n\u003ctype\u003e[optional scope]: \u003cdescription\u003e There are three parts. type is used to classify which kind of work this commit does. scope is an optional field that provides additional contextual information. And the last field is your description of this commit.\nType Here we list some common types and their meanings.\nfeat: Implement a new feature. fix: Patch a bug. docs: Add document or comment. build: Change the build script or configuration. style: Style change (only). No logic involved. refactor: Refactor an existing module for performance, structure, or other reasons. test: Enhance test coverage or sqlness. chore: None of the above. Scope The scope is more flexible than type. And it may have different values under different types.\nFor example, In a feat or build commit we may use the code module to define scope, like\nfeat(cluster): feat(server): build(ci): build(image): And in docs or refactor commits the motivation is prefer to label the scope, like\ndocs(comment): docs(post): refactor(perf): refactor(usability): But you don’t need to add a scope every time. This isn’t mandatory. It’s just a way to help describe the commit.\nAfter all There are many other rules or scenarios in conventional commit’s website. We are still exploring a better and more friendly workflow. Please do let us know by open an issue if you have any suggestions ❤️\n","categories":"","description":"","excerpt":"This document describes how we use conventional commit in our …","ref":"/docs/dev/conventional_commit/","tags":"","title":"Conventional Commit Guide"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/dev/","tags":"","title":"Development"},{"body":"Identifier in HoraeDB can be used as table name, column name etc. It cannot be preserved keywords or start with number and punctuation symbols. HoraeDB allows to quote identifiers with back quotes (`). In this case it can be any string like 00_table or select.\n","categories":"","description":"","excerpt":"Identifier in HoraeDB can be used as table name, column name etc. It …","ref":"/docs/user-guide/sql/identifier/","tags":"","title":"Identifier"},{"body":"This guide shows how to deploy a HoraeDB cluster with HoraeMeta. And with the HoraeMeta, the whole HoraeDB cluster will feature: high availability, load balancing and horizontal scalability if the underlying storage used by HoraeDB is separated service.\nDeploy HoraeMeta Introduce HoraeMeta is one of the core services of HoraeDB distributed mode, it is used to manage and schedule the HoraeDB cluster. By the way, the high availability of HoraeMeta is ensured by embedding ETCD. Also, the ETCD service is provided for HoraeDB servers to manage the distributed shard locks.\nBuild Golang version \u003e= 1.19. run make build in root path of HoraeMeta. Deploy Config At present, HoraeMeta supports specifying service startup configuration in two ways: configuration file and environment variable. We provide an example of configuration file startup. For details, please refer to config. The configuration priority of environment variables is higher than that of configuration files. When they exist at the same time, the environment variables shall prevail.\nDynamic or Static Even with the HoraeMeta, the HoraeDB cluster can be deployed with a static or a dynamic topology. With a static topology, the table distribution is static after the cluster is initialized while with the dynamic topology, the tables can migrate between different HoraeDB nodes to achieve load balance or failover. However, the dynamic topology can be enabled only if the storage used by the HoraeDB node is remote, otherwise the data may be corrupted when tables are transferred to a different HoraeDB node when the data of HoraeDB is persisted locally.\nCurrently, the dynamic scheduling over the cluster topology is disabled by default in HoraeMeta, and in this guide, we won’t enable it because local storage is adopted here. If you want to enable the dynamic scheduling, the TOPOLOGY_TYPE can be set as dynamic (static by default), and after that, load balancing and failover will work. However, don’t enable it if what the underlying storage is local disk.\nWith the static topology, the params DEFAULT_CLUSTER_NODE_COUNT, which denotes the number of the HoraeDB nodes in the deployed cluster and should be set to the real number of machines for HoraeDB server, matters a lot because after cluster initialization the HoraeDB nodes can’t be changed any more.\nStart HoraeMeta Instances HoraeMeta is based on etcd to achieve high availability. In product environment, we usually deploy multiple nodes, but in local environment and testing, we can directly deploy a single node to simplify the entire deployment process.\nStandalone 1 2 3 docker run -d --name horaemeta-server \\ -p 2379:2379 \\ ghcr.io/apache/horaemeta-server:nightly-20231225-ab067bf0 Cluster 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 wget https://horaedb.apache.org/config-horaemeta-cluster0.toml docker run -d --network=host --name horaemeta-server0 \\ -v $(pwd)/config-horaemeta-cluster0.toml:/etc/horaemeta/horaemeta.toml \\ ghcr.io/apache/horaemeta-server:nightly-20231225-ab067bf0 wget https://horaedb.apache.org/config-horaemeta-cluster1.toml docker run -d --network=host --name horaemeta-server1 \\ -v $(pwd)/config-horaemeta-cluster1.toml:/etc/horaemeta/horaemeta.toml \\ ghcr.io/apache/horaemeta-server:nightly-20231225-ab067bf0 wget https://horaedb.apache.org/config-horaemeta-cluster2.toml docker run -d --network=host --name horaemeta-server2 \\ -v $(pwd)/config-horaemeta-cluster2.toml:/etc/horaemeta/horaemeta.toml \\ ghcr.io/apache/horaemeta-server:nightly-20231225-ab067bf0 And if the storage used by the HoraeDB is remote and you want to enable the dynamic schedule features of the HoraeDB cluster, the -e TOPOLOGY_TYPE=dynamic can be added to the docker run command.\nDeploy HoraeDB In the NoMeta mode, HoraeDB only requires the local disk as the underlying storage because the topology of the HoraeDB cluster is static. However, with HoraeMeta, the cluster topology can be dynamic, that is to say, HoraeDB can be configured to use a non-local storage service for the features of a distributed system: HA, load balancing, scalability and so on. And HoraeDB can be still configured to use a local storage with HoraeMeta, which certainly leads to a static cluster topology.\nThe relevant storage configurations include two parts:\nObject Storage WAL Storage Note: If you are deploying HoraeDB over multiple nodes in a production environment, please set the environment variable for the server address as follows:\n1 export HORAEDB_SERVER_ADDR=\"{server_address}:8831\" This address is used for communication between HoraeMeta and HoraeDB, please ensure it is valid.\nObject Storage Local Storage Similarly, we can configure HoraeDB to use a local disk as the underlying storage:\n1 2 3 [analytic.storage.object_store] type = \"Local\" data_dir = \"/home/admin/data/horaedb\" OSS Aliyun OSS can be also used as the underlying storage for HoraeDB, with which the data is replicated for disaster recovery. Here is a example config, and you have to replace the templates with the real OSS parameters:\n1 2 3 4 5 6 7 [analytic.storage.object_store] type = \"Aliyun\" key_id = \"{key_id}\" key_secret = \"{key_secret}\" endpoint = \"{endpoint}\" bucket = \"{bucket}\" prefix = \"{data_dir}\" S3 Amazon S3 can be also used as the underlying storage for HoraeDB. Here is a example config, and you have to replace the templates with the real S3 parameters:\n1 2 3 4 5 6 7 8 [analytic.storage.object_store] type = \"S3\" region = \"{region}\" key_id = \"{key_id}\" key_secret = \"{key_secret}\" endpoint = \"{endpoint}\" bucket = \"{bucket}\" prefix = \"{prefix}\" WAL Storage RocksDB The WAL based on RocksDB is also a kind of local storage for HoraeDB, which is easy for a quick start:\n1 2 3 [analytic.wal] type = \"RocksDB\" data_dir = \"/home/admin/data/horaedb\" OceanBase If you have deployed a OceanBase cluster, HoraeDB can use it as the WAL storage for data disaster recovery. Here is a example config for such WAL, and you have to replace the templates with real OceanBase parameters:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 [analytic.wal] type = \"Obkv\" [analytic.wal.data_namespace] ttl = \"365d\" [analytic.wal.obkv] full_user_name = \"{full_user_name}\" param_url = \"{param_url}\" password = \"{password}\" [analytic.wal.obkv.client] sys_user_name = \"{sys_user_name}\" sys_password = \"{sys_password}\" Kafka If you have deployed a Kafka cluster, HoraeDB can also use it as the WAL storage. Here is example config for it, and you have to replace the templates with real parameters of the Kafka cluster:\n1 2 3 4 5 [analytic.wal] type = \"Kafka\" [analytic.wal.kafka.client] boost_brokers = [ \"{boost_broker1}\", \"{boost_broker2}\" ] Meta Client Config Besides the storage configurations, HoraeDB must be configured to start in WithMeta mode and connect to the deployed HoraeMeta:\n1 2 3 4 5 6 7 8 9 10 11 [cluster_deployment] mode = \"WithMeta\" [cluster_deployment.meta_client] cluster_name = 'defaultCluster' meta_addr = 'http://{HoraeMetaAddr}:2379' lease = \"10s\" timeout = \"5s\" [cluster_deployment.etcd_client] server_addrs = ['http://{HoraeMetaAddr}:2379'] Complete Config of HoraeDB With all the parts of the configurations mentioned above, a runnable complete config for HoraeDB can be made. In order to make the HoraeDB cluster runnable, we can decide to adopt RocksDB-based WAL and local-disk-based Object Storage:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 [server] bind_addr = \"0.0.0.0\" http_port = 5440 grpc_port = 8831 [logger] level = \"info\" [runtime] read_thread_num = 20 write_thread_num = 16 background_thread_num = 12 [cluster_deployment] mode = \"WithMeta\" [cluster_deployment.meta_client] cluster_name = 'defaultCluster' meta_addr = 'http://127.0.0.1:2379' lease = \"10s\" timeout = \"5s\" [cluster_deployment.etcd_client] server_addrs = ['127.0.0.1:2379'] [analytic] write_group_worker_num = 16 replay_batch_size = 100 max_replay_tables_per_batch = 128 write_group_command_channel_cap = 1024 sst_background_read_parallelism = 8 [analytic.manifest] scan_batch_size = 100 snapshot_every_n_updates = 10000 scan_timeout = \"5s\" store_timeout = \"5s\" [analytic.wal] type = \"RocksDB\" data_dir = \"/home/admin/data/horaedb\" [analytic.storage] mem_cache_capacity = \"20GB\" # 1\u003c\u003c8=256 mem_cache_partition_bits = 8 [analytic.storage.object_store] type = \"Local\" data_dir = \"/home/admin/data/horaedb/\" [analytic.table_opts] arena_block_size = 2097152 write_buffer_size = 33554432 [analytic.compaction] schedule_channel_len = 16 schedule_interval = \"30m\" max_ongoing_tasks = 8 memory_limit = \"4G\" Let’s name this config file as config.toml. And the example configs, in which the templates must be replaced with real parameters before use, for remote storages are also provided:\nRocksDB WAL + OSS OceanBase WAL + OSS Kafka WAL + OSS Run HoraeDB cluster with HoraeMeta Firstly, let’s start the HoraeMeta:\n1 2 3 docker run -d --name horaemeta-server \\ -p 2379:2379 \\ ghcr.io/apache/horaemeta-server:nightly-20231225-ab067bf0 With the started HoraeMeta cluster, let’s start the HoraeDB instance: TODO: complete it later\n","categories":"","description":"","excerpt":"This guide shows how to deploy a HoraeDB cluster with HoraeMeta. And …","ref":"/docs/user-guide/cluster_deployment/with_meta/","tags":"","title":"WithMeta"},{"body":"","categories":"","description":"","excerpt":"","ref":"/cn/docs/dev/","tags":"","title":"开发手册"},{"body":"CPU 剖析 HoraeDB 提供 CPU 剖析 http 接口 debug/profile/cpu.\n例子:\n// 60s CPU 采样数据 curl 0:5000/debug/profile/cpu/60 // 产出文件 /tmp/flamegraph_cpu.svg 内存剖析 HoraeDB 提供内存剖析 http 接口 debug/profile/heap.\n安装依赖 sudo yum install -y jemalloc-devel ghostscript graphviz 例子:\n// 开启 malloc prof export MALLOC_CONF=prof:true // 运行 horaedb-server ./horaedb-server .... // 60s 内存采样数据 curl -L '0:5000/debug/profile/heap/60' \u003e /tmp/heap_profile jeprof --show_bytes --pdf /usr/bin/horaedb-server /tmp/heap_profile \u003e profile_heap.pdf jeprof --show_bytes --svg /usr/bin/horaedb-server /tmp/heap_profile \u003e profile_heap.svg ","categories":"","description":"","excerpt":"CPU 剖析 HoraeDB 提供 CPU 剖析 http 接口 debug/profile/cpu.\n例子:\n// 60s CPU 采样数 …","ref":"/cn/docs/dev/profiling/","tags":"","title":"性能诊断"},{"body":"HoraeDB 中表名、列名等标识符不能是保留关键字或以数字和标点符号开始，不过 HoraeDB 允许用反引号引用标识符（`）。在这种情况下，它可以是任何字符串，如 00_table 或 select。\n","categories":"","description":"","excerpt":"HoraeDB 中表名、列名等标识符不能是保留关键字或以数字和标点符号开始，不过 HoraeDB 允许用反引号引用标识符（`）。在这种情况 …","ref":"/cn/docs/user-guide/sql/identifier/","tags":"","title":"标识符"},{"body":"注意：文章中提到的部分特性暂时还未实现。\n整体架构 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 ┌───────────────────────────────────────────────────────────────────────┐ │ │ │ HoraeMeta Cluster │ │ │ └───────────────────────────────────────────────────────────────────────┘ ▲ ▲ ▲ │ │ │ │ │ │ ▼ ▼ ▼ ┌───────┐Route Info ┌HoraeDB─────┬┬─┐ ┌HoraeDB─────┬┬─┐ ┌HoraeDB─────┬┬─┐ │client │◀────────▶ │ │ │TableN││ │ │ │ │TableN││ │ │ │ │TableN││ │ └───────┘Write/Query└──Shard(L)──┴┴─┘ └──Shard(F)──┴┴─┘ └──Shard(F)──┴┴─┘ │ │ ▲ ▲ │ │ │ │ Write─────────┐ ├────Sync───────┘ │ │ │ ┌────────┬▼───┴────┬──────────────────┐ Upload SST │ │ │ │ │ │WAL │Region N │ │ │Service │ │ │ │ └────────┴─────────┴──────────────────┘ ▼ ┌───────────────────────────────────────────────────────────────────────┐ │ │ │ Object Storage │ │ │ └───────────────────────────────────────────────────────────────────────┘ 上面给出来 HoraeDB 集群化方案的整体架构图，对于其中的一些名词做出解释：\nHoraeMeta Cluster：集群的元数据中心，负责集群的整体调度； Shard(L)/Shard(F): Leader Shard 和 Follower Shard，每一个 Shard 由多张 Table 组成; HoraeDB：一个 HoraeDB 实例, 包含多个 Shard； WAL Service：WAL 服务，在集群方案中，用于存储实时写入的数据； Object Storage：对象存储服务，用于存储从 memtable 生成的 SST 文件； 根据架构图，读者应该对于集群化方案有一个初步的认知 —— 存储计算分离。正因如此，HoraeDB 实例本身不存储任何数据，从而使得计算存储弹性扩缩容、服务高可用和负载均衡等等有用的分布式特性比较容易实现。\nShard Shard 是一个重要的概念，是集群调度的基本单元，一个 Shard 包含多张表。\n这里值得说明的是，集群调度的基本单元并非是 Table，而是采用了 Shard，主要原因在于考虑到 HoraeDB 集群如果需要支持至少百万级别的 Table 的话，而部分组件，直接处理起这个量级的 Table 可能会存在一些性能问题，例如对于 WAL 服务，如果按照 Table 去单独管理数据的话，一个重要的性能问题就在于重启后恢复新写入数据的时候，按照 Table 级别去做数据恢复的话，将会非常耗时，但按照 Shard 去管理数据的话，这个情况会改善很多，因为具体的 WAL 服务实现可以按照 Shard 这个信息，将 Table 的数据进行合理组织，从而获得比较好的数据局部性，在重启恢复的时候，就可以通过 Shard 来进行快速的数据恢复。\n此外 Shard 具有 Leader 和 Follower 两种 Role，也就是图中的 Shard(L) 和 Shard(F)，Leader 负责读写，Follower 只有读权限，Follower 的引入其实是为了解决 HA 的问题，保证 Leader 在 crash 了之后，能快速的恢复相关表的读写服务，为了做到这一点，Follower 需要从 WAL 不停地同步最新的数据（由 Leader 写入）。\n一个 HoraeDB 实例会拥有多个 Shard，这些 Shard 可以是 Leader 也是 Follower，以交织的方式存在:\n1 2 3 4 5 6 7 8 9 10 11 12 13 ┌─HoraeDB Instance0──────┐ ┌─HoraeDB Instance1──────┐ │ ┌─Shard0(L)────────┐ │ │ ┌─Shard0(F)────────┐ │ │ │ ┌────┬────┬────┐ │ │ │ │ ┌────┬────┬────┐ │ │ │ │ │ T0 │ T1 │ T2 │ │ │ │ │ │ T0 │ T1 │ T2 │ │ │ │ │ └────┴────┴────┘ │ │ │ │ └────┴────┴────┘ │ │ │ └──────────────────┘ │ │ └──────────────────┘ │ │ │ │ │ │ ┌─Shard1(F)────────┐ │ │ ┌─Shard1(L)────────┐ │ │ │ ┌────┬────┬────┐ │ │ │ │ ┌────┬────┬────┐ │ │ │ │ │ T0 │ T1 │ T2 │ │ │ │ │ │ T0 │ T1 │ T2 │ │ │ │ │ └────┴────┴────┘ │ │ │ │ └────┴────┴────┘ │ │ │ └──────────────────┘ │ │ └──────────────────┘ │ └────────────────────────┘ └────────────────────────┘ 上面提到集群的基本调度单元是 Shard，因此对于 Shard 存在这一些基本操作，通过这些操作可以完成一些负责的调度逻辑：\n向 Shard 新增/删除 Table：在建表/删表的时候使用； Shard 的打开/关闭：可以用来将一个 Shard 迁移到另外一个 HoraeDB 实例上面； Shard 分裂/合并：可以用来完成扩容和缩容； Shard 角色切换：将一个 Shard 从 Leader 切换成 Follower，或者从 Follower 切换成 Leader； HoraeMeta HoraeMeta 主要负责集群的元数据管理和集群调度，其实现是通过内置一个 ETCD 来保证分布式一致性的。\n元数据主要都是围绕 Table 来构建的，包括一些建表信息（如 Table ID），Table 所属集群（HoraeMeta 是可以支持多个 HoraeDB 集群的），Table 与 Shard 的映射关系等等。\nHoraeMeta 的主要工作还是负责集群的调度，需要完成：\n接收来自 HoraeDB 实例的心跳，根据心跳来检测 HoraeDB 实例的存活状态； 负责 Shard 的具体分配以及调度，会将 Shard 按照一定的算法（负载均衡）分配到具体的实例上面去； 参与 Table 的创建，将 Table 分配到合适的 Shard（也就会分配到具体的实例）； 如果有新的 HoraeDB 实例加入到集群中，可以进行扩容操作； 如果有检测到 HoraeDB 实例的下线状况，可以完成自动的故障恢复； 路由 为了避免转发请求的开销，客户端与 HoraeDB 实例之间的通信是点对点的，也就是说，客户端在发送任何特定的写入/查询请求之前，应该从服务器获取路由信息。\n实际上，路由信息是由 HoraeMeta 决定的，但是客户端只允许通过 HoraeDB 实例而不是 HoraeMeta 来访问它，以避免由 HoraeMeta 引起的潜在性能瓶颈。\nWAL Service \u0026 Object Storage 在集群方案中，WAL Service 和 Object Storage 都是作为独立的服务而存在的，并且是具备容灾能力的分布式系统。目前 WAL Service 的实现，在集群化方案中主要包括两种实现：Kafka 和 OBKV（通过其提供的 Table API 访问 OceanBase）；而 Object Storage 的实现比较多，基本覆盖了主流的对象存储服务。\n这两个系统的相似之处在于，它们是作为计算存储分离架构中的的存储层；而它们的区别也很明显，WAL Service 具备较高的实时写入性能（高吞吐、低延时），负责实时的新增数据写入，而 Object Storage 具备低成本的存储代价和较为高效的查询吞吐，负责后台整理好的、长期存储的数据读写。\n这两个服务组件的引入，使得 HoraeDB 集群的水平扩容、服务高可用、负载均衡提供了实现基础。\n水平扩容 集群的扩展能力是评价一个集群方案的重要指标，下面从存储和计算两方面来介绍一下 HoraeDB 集群的水平扩容能力是如何实现的。\n存储 很明显，在选择 WAL Service 和 Object Storage 两个底层服务实现的时候，水平扩容能力是必须的，存储容量出现问题的时候，可以单独地进行存储服务的扩容。\n计算 计算能力的水平扩容可能会复杂一下，考虑如下几个容量问题：\n大量查询大量的表； 大量查询同一张数据量大的表； 大量查询同一张正常的表； 对于第一种情况，通过 Shard 分裂，将分裂出来的新 Shard 迁移到新扩容的 HoraeDB 实例，这样表就会分散到新的实例上。\n对于第二种情况，可以通过表分区的功能，将该表分成多个子表，从而达到水平扩容的效果；\n第三种情况是最重要也是最难处理的。事实上，Follower Shard 可以为 Leader Shard 分担部分查询请求，但是由于 Follower Shard 需要同步数据，它的数量是受到 WAL 同步能力的限制的。实际上，从下图可以看到，对于这样的场景（当 Follower Shard 不足以承担住访问量的时候），可以考虑在集群中添加一种纯计算节点，其中的 Shard 不参与实时的数据同步，当这个实例被访问的时候，新写入的实时数据可以从 Leader Shard 或者 Follower Shard 恢复出来，历史的数据可以直接从 Object Storage 获取并缓存，这样的话，就可以在一定程度上（实际上还是存在单表的实时数据拉取的瓶颈，但是考虑到这部分数据并非特别多，理论上这个瓶颈应该比较难以达到），达到水平扩容的效果。\n1 2 3 4 5 6 7 8 9 ┌HoraeDB─────┬┬─┐ ┌──newly written─│ │ │TableN││ │ ▼ └──Shard(L/F)┴┴─┘ ┌───────┐ Query ┌HoraeDB─────┬┬─┐ │client │────────▶│ │ │TableN││ │ └───────┘ └──Shard─────┴┴─┘ ┌───────────────┐ ▲ │ Object │ └───old SST──────│ Storage │ └───────────────┘ High Availability 通过上面的介绍，HoraeDB 的高可用方案，其实比较自然了。考虑某台 HoraeDB 实例 Crash 了，后续的服务恢复步骤整体如下：\nHoraeMeta 通过发现心跳断了，判断该 HoraeDB 实例下线； HoraeMeta 选择将该实例上的所有 Leader Shard 所对应的 Follower Shard，切换成 Leader Shard，从而完成快速恢复； 如果没有对应的 Follower Shard 存在，那么需要执行 Open Shard 操作，不过这样的恢复是较慢的； 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 ┌─────────────────────────────────────────────────────────┐ │ │ │ HoraeMeta Cluster │ │ │ └─────────────────────────────────────────────────────────┘ ▲ ┌ ─ ─Broken ─ ─ ┤ │ │ │ ┌ HoraeDB Instance0 ─ ─ ─ │ ┌─HoraeDB Instance1──────┐ ┌─HoraeDB Instance1──────┐ ┌─Shard0(L)────────┐ │ │ │ ┌─Shard0(F)────────┐ │ │ ┌─Shard0(L)────────┐ │ │ │ ┌────┬────┬────┐ │ │ │ │ ┌────┬────┬────┐ │ │ │ │ ┌────┬────┬────┐ │ │ │ │ T0 │ T1 │ T2 │ │ │ ├───│ │ │ T0 │ T1 │ T2 │ │ │ │ │ │ T0 │ T1 │ T2 │ │ │ │ │ └────┴────┴────┘ │ │ │ │ └────┴────┴────┘ │ │ │ │ └────┴────┴────┘ │ │ └──────────────────┘ │ │ │ └──────────────────┘ │ Failover │ └──────────────────┘ │ │ │ ├─HoraeDB Instance2──────┤ ───────────▶ ├─HoraeDB Instance2──────┤ ┌─Shard1(L)────────┐ │ │ │ ┌─Shard1(F)────────┐ │ │ ┌─Shard1(L)────────┐ │ │ │ ┌────┬────┬────┐ │ │ │ │ ┌────┬────┬────┐ │ │ │ │ ┌────┬────┬────┐ │ │ │ │ T0 │ T1 │ T2 │ │ │ └───│ │ │ T0 │ T1 │ T2 │ │ │ │ │ │ T0 │ T1 │ T2 │ │ │ │ │ └────┴────┴────┘ │ │ │ └────┴────┴────┘ │ │ │ │ └────┴────┴────┘ │ │ └──────────────────┘ │ │ └──────────────────┘ │ │ └──────────────────┘ │ └ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ └────────────────────────┘ └────────────────────────┘ Load Balancing HoraeDB 上传的心跳信息会带上机器的负载信息，HoraeMeta 会根据这些信息，完成自动的负载均衡的调度工作，主要包括：\n新建表的时候，为该表挑选一个负载低的实例上面的 Shard； 将负载高的实例上的 Shard 迁移到负载低的实例； 更好地，可以根据 Shard 的负载，通过 Shard 的分裂和合并，来完成更细粒度的负载均衡； ","categories":"","description":"","excerpt":"注意：文章中提到的部分特性暂时还未实现。\n整体架构 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 …","ref":"/cn/docs/design/clustering/","tags":"","title":"集群模式"},{"body":"在快速开始部分我们已经介绍过单机版本 HoraeDB 的部署。\n除此之外，HoraeDB 作为一个分布式时序数据库，多个 HoraeDB 实例能够以集群的方式提供可伸缩和高可用的数据服务。\n由于目前 HoraeDB 对于 Kubernetes 的支持还在开发之中，目前 HoraeDB 集群部署只能通过手动完成，集群部署的模式主要有两种，两者的区别在于是否需要部署 HoraeMeta，对于 NoMeta 的模式，我们仅建议在测试场景下使用。\n","categories":"","description":"","excerpt":"在快速开始部分我们已经介绍过单机版本 HoraeDB 的部署。\n除此之外，HoraeDB 作为一个分布式时序数据库，多个 HoraeDB 实 …","ref":"/cn/docs/user-guide/cluster_deployment/","tags":"","title":"集群部署"},{"body":"In order to compile HoraeDB, some relevant dependencies(including the Rust toolchain) should be installed.\nDependencies Ubuntu Assuming the development environment is Ubuntu20.04, execute the following command to install the required dependencies:\n1 sudo apt install git curl gcc g++ libssl-dev pkg-config cmake protobuf-compiler It should be noted that the compilation of the project requires a higher version of CMake; if your development environment is an older Linux distribution, you will need to manually install the dependencies for a higher version.\nmacOS If the development environment is MacOS, execute the following command to install the required dependencies.\nInstall command line tools: 1 xcode-select --install Install cmake: 1 brew install cmake Install protobuf: 1 brew install protobuf Rust Rust can be installed by rustup. After installing rustup, when entering the HoraeDB project, the specified Rust version will be automatically downloaded according to the rust-toolchain file.\nAfter execution, you need to add environment variables to use the Rust toolchain. Basically, just put the following commands into your ~/.bashrc or ~/.bash_profile:\n1 source $HOME/.cargo/env Compile and Run Compile HoraeDB by the following command:\ncargo build Then you can run HoraeDB using the default configuration file provided in the codebase.\n1 ./target/debug/horaedb-server --config ./docs/minimal.toml ","categories":"","description":"","excerpt":"In order to compile HoraeDB, some relevant dependencies(including the …","ref":"/docs/dev/compile_run/","tags":"","title":"Compile"},{"body":"Thank you for thinking of contributing! We very much welcome contributions from the community. To make the process easier and more valuable for everyone involved we have a few rules and guidelines to follow.\nSubmitting Issues and Feature Requests Before you file an issue, please search existing issues in case the same or similar issues have already been filed. If you find an existing open ticket covering your issue then please avoid adding “👍” or “me too” comments; GitHub notifications can cause a lot of noise for the project maintainers who triage the back-log. However, if you have a new piece of information for an existing ticket, and you think it may help the investigation or resolution, then please do add it as a comment! You can signal to the team that you’re experiencing an existing issue with one of GitHub’s emoji reactions (these are a good way to add “weight” to an issue from a prioritisation perspective).\nSubmitting an Issue The New Issue page has templates for both bug reports and feature requests. Please fill one of them out! The issue templates provide details on what information we will find useful to help us fix an issue. In short though, the more information you can provide us about your environment and what behaviour you’re seeing, the easier we can fix the issue. If you can push a PR with test cases that trigger a defect or bug, even better!\nAs well as bug reports we also welcome feature requests (there is a dedicated issue template for these). Typically, the maintainers will periodically review community feature requests and make decisions about if we want to add them. For features, we don’t plan to support we will close the feature request ticket (so, again, please check closed tickets for feature requests before submitting them).\nContributing Changes HoraeDB is written mostly in idiomatic Rust—please see the Style Guide for more details. All code must adhere to the rustfmt format, and pass all of the clippy checks we run in CI (there are more details further down this README).\nMaking a PR To open a PR you will need to have a GitHub account. Fork the horaedb repo and work on a branch on your fork. When you have completed your changes, or you want some incremental feedback make a Pull Request to HoraeDB here.\nIf you want to discuss some work in progress then please prefix [WIP] to the PR title.\nFor PRs that you consider ready for review, verify the following locally before you submit it:\nyou have a coherent set of logical commits, with messages conforming to the Conventional Commits specification; all the tests and/or benchmarks pass, including documentation tests; the code is correctly formatted and all clippy checks pass; and you haven’t left any “code cruft” (commented out code blocks etc). There are some tips on verifying the above in the next section.\nAfter submitting a PR, you should:\nverify that all CI status checks pass and the PR is 💚; ask for help on the PR if any of the status checks are 🔴, and you don’t know why; wait patiently for one of the team to review your PR, which could take a few days. Running Tests The cargo build tool runs tests as well. Run:\n1 cargo test --workspace Enabling logging in tests To enable logging to stderr during a run of cargo test set the Rust RUST_LOG environment variable. For example, to see all INFO messages:\n1 RUST_LOG=info cargo test --workspace Integration tests We have integration test suits in the SQL level so any change that may have influence on the user-facing query execution should be covered by the integration test. Refer to this document for more information.\nRunning rustfmt and clippy CI will check the code formatting with rustfmt and Rust best practices with clippy.\nTo automatically format your code according to rustfmt style, first make sure rustfmt is installed using rustup:\n1 rustup component add rustfmt Then, whenever you make a change and want to reformat, run:\n1 cargo fmt --all Similarly, with clippy, install with:\n1 rustup component add clippy And run with:\n1 cargo clippy --all-targets --workspace -- -D warnings ","categories":"","description":"","excerpt":"Thank you for thinking of contributing! We very much welcome …","ref":"/docs/contribution-guidelines/","tags":"","title":"Contribution Guidelines"},{"body":"This chapter introduces the data definition statements.\n","categories":"","description":"","excerpt":"This chapter introduces the data definition statements.\n","ref":"/docs/user-guide/sql/ddl/","tags":"","title":"Data Definition Statements"},{"body":"v0.1.0 支持基于本地磁盘的 Standalone 版本 支持分析存储格式 支持 SQL v0.2.0 静态路由的分布式版本 远端存储支持阿里云 OSS 支持基于 OBKV的 WAL v0.3.0 发布多语言客户端，包括 Java, Rust 和 Python 支持使用 HoraeMeta 的静态集群 混合存储格式基本实现 v0.4.0 实现更复杂的集群方案，增强 HoraeDB 的可靠性和可扩展性 构建日常运行的、基于 TSBS 的压测任务 v1.0.0-alpha (Released) 基于 Apache Kafka 实现分布式 WAL 发布 Golang 客户端 优化时序场景下的查询性能 支持集群模式下表的动态转移 v1.0.0 正式发布 HoraeDB 和相关 SDK，并完成所有的 breaking changes 完成分区表的主要工作 优化查询性能，特别是云原生集群模式下，包括： 多级缓存 多种方式减少从远端获取的数据量(提高 SST 数据过滤精度) 提高获取远程对象存储数据的并发度 通过控制合并时的资源消耗，提高数据写入性能 Afterwards 随着对时序数据库及其各种使用情况的深入了解，我们的大部分工作将聚焦在性能、可靠性、可扩展性、易用性以及与开源社区的合作方面\n增加支持 PromQL, InfluxQL, OpenTSDB 协议 提供基础的运维工具。特别包括如下： 适配云基础设施的部署工具，如 Kubernetes 加强自监控能力，特别是关键的日志和指标 开发多种工具，方便使用 HoraeDB，例如，数据导入和导出工具 探索新的存储格式，提高混合负载（分析和时序负载）的性能 ","categories":"","description":"","excerpt":"v0.1.0 支持基于本地磁盘的 Standalone 版本 支持分析存储格式 支持 SQL v0.2.0 静态路由的分布式版本 远端存储支 …","ref":"/cn/docs/dev/roadmap/","tags":"","title":"RoadMap"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/user-guide/sdk/","tags":"","title":"SDK"},{"body":"","categories":"","description":"","excerpt":"","ref":"/cn/docs/user-guide/sdk/","tags":"","title":"SDK 文档"},{"body":"存储引擎主要提供以下两个功能：\n数据的持久化 在保证数据正确性的前提下，用最合理的方式来组织数据，来满足不同场景的查询需求 本篇文档就来介绍 HoraeDB 中存储引擎的内部实现，读者可以参考这里面的内容，来探索如何高效使用 HoraeDB。\n整体架构 HoraeDB 是一种基于 share-nothing 架构的分布式存储系统，不同服务器之间的数据相互隔离，互不影响。每一个单机中的存储引擎是 LSM（Log-structured merge-tree）的一个变种，针对时序场景做了优化，下图展示了其主要组件的运作方式：\nWrite Ahead Log (WAL) 一次写入请求的数据会写到两个部分：\n内存中的 memtable 可持久化的 WAL 由于 memtable 不是实时持久化到底层存储系统，因此需要用 WAL 来保证 memtable 中数据的可靠性。\n另一方面，由于分布式架构的设计，要求 WAL 本身是高可用的，现在 HoraeDB 中，主要有以下几种实现：\n本地磁盘（基于 RocksDB，无分布式高可用） Oceanbase Kafka Memtable Memtable 是一个内存的数据结构，用来保存最近写入的数据。一个表对应一个 memtable。\nMemtable 默认是可读写的（称为 active），当写入达到一起阈值时，会变成只读的并且被一个新的 memtable 替换掉。只读的 memtable 会被后台线程以 SST 的形式写入到底层存储系统中，写入完成后，只读的 memtable 就可以被销毁，同时 WAL 中也可以删除对应部分的数据。\nSorted String Table（SST） SST 是数据的持久化格式，按照表主键的顺序存放，目前 HoraeDB 采用 parquet 格式来存储。\n对于 HoraeDB 来说，SST 有一个重要特性： segment_duration，只有同一个 segment 内的 SST 才有可能进行合并操作。而且有了 segment，也方便淘汰过期的数据。\n除了存放原始数据外，SST 内也会存储数据的统计信息来加速查询，比如：最大值、最小值等。\nCompactor Compactor 可以把多个小 SST 文件合并成一个，用于解决小文件数过多的问题。此外，Compactor 也会在合并时进行过期数据的删除，重复数据的去重。目前 HoraeDB 中的合并策略参考自 Cassandra，主要有两个：\nSizeTieredCompactionStrategy TimeWindowCompactionStrategy Manifest Manifest 记录表、SST 文件元信息，比如：一个 SST 内数据的最小、最大时间戳。\n由于分布式架构的设计，要求 Manifest 本身是高可用的，现在 HoraeDB 中，主要有以下几种实现：\nWAL ObjectStore ObjectStore ObjectStore 是数据（即 SST）持久化的地方，一般来说各大云厂商均有对应服务，像阿里云的 OSS，AWS 的 S3。\n","categories":"","description":"","excerpt":"存储引擎主要提供以下两个功能：\n数据的持久化 在保证数据正确性的前提下，用最合理的方式来组织数据， …","ref":"/cn/docs/design/storage/","tags":"","title":"存储引擎"},{"body":"本章介绍表结构相关 SQL 语句：\n","categories":"","description":"","excerpt":"本章介绍表结构相关 SQL 语句：\n","ref":"/cn/docs/user-guide/sql/ddl/","tags":"","title":"表结构操作"},{"body":"This chapter introduces the data manipulation statements.\n","categories":"","description":"","excerpt":"This chapter introduces the data manipulation statements.\n","ref":"/docs/user-guide/sql/dml/","tags":"","title":"Data Manipulation Statements"},{"body":"This guide introduces the operation and maintenance of HoraeDB, including cluster installation, database\u0026table operations, fault tolerance, disaster recovery, data import and export, etc.\n","categories":"","description":"","excerpt":"This guide introduces the operation and maintenance of HoraeDB, …","ref":"/docs/user-guide/operation/","tags":"","title":"Operation and Maintenance"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/design/","tags":"","title":"Technical and Design"},{"body":"本章介绍数据操作相关的 SQL.\n","categories":"","description":"","excerpt":"本章介绍数据操作相关的 SQL.\n","ref":"/cn/docs/user-guide/sql/dml/","tags":"","title":"数据操作"},{"body":"","categories":"","description":"","excerpt":"","ref":"/cn/docs/design/","tags":"","title":"设计文档"},{"body":"本章介绍 HoraeDB 的运维相关的操作，包括表相关操作，设置访问黑名单，已经如何监控 HoraeDB。未来还会介绍集群扩容，容灾相关：\n","categories":"","description":"","excerpt":"本章介绍 HoraeDB 的运维相关的操作，包括表相关操作，设置访问黑名单，已经如何监控 HoraeDB。未来还会介绍集群扩容，容灾相关：\n","ref":"/cn/docs/user-guide/operation/","tags":"","title":"运维文档"},{"body":"HoraeDB has an open ecosystem that encourages collaboration and innovation, allowing developers to use what suits them best.\n","categories":"","description":"","excerpt":"HoraeDB has an open ecosystem that encourages collaboration and …","ref":"/docs/user-guide/ecosystem/","tags":"","title":"Ecosystem"},{"body":"There are serval utilities SQL in HoraeDB that can help in table manipulation or query inspection.\nSHOW CREATE TABLE 1 SHOW CREATE TABLE table_name; SHOW CREATE TABLE returns a CREATE TABLE DDL that will create a same table with the given one. Including columns, table engine and options. The schema and options shows in CREATE TABLE will based on the current version of the table. An example:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 -- create one table CREATE TABLE `t` (a bigint, b int default 3, c string default 'x', d smallint null, t timestamp NOT NULL, TIMESTAMP KEY(t)) ENGINE = Analytic; -- Result: affected_rows: 0 -- show how one table should be created. SHOW CREATE TABLE `t`; -- Result DDL: CREATE TABLE `t` ( `t` timestamp NOT NULL, `tsid` uint64 NOT NULL, `a` bigint, `b` int, `c` string, `d` smallint, PRIMARY KEY(t,tsid), TIMESTAMP KEY(t) ) ENGINE=Analytic WITH ( arena_block_size='2097152', compaction_strategy='default', compression='ZSTD', enable_ttl='true', num_rows_per_row_group='8192', segment_duration='', ttl='7d', update_mode='OVERWRITE', write_buffer_size='33554432' ) DESCRIBE 1 DESCRIBE table_name; DESCRIBE will show a detailed schema of one table. The attributes include column name and type, whether it is tag and primary key (todo: ref) and whether it’s nullable. The auto created column tsid will also be included (todo: ref).\nExample:\n1 2 3 CREATE TABLE `t`(a int, b string, t timestamp NOT NULL, TIMESTAMP KEY(t)) ENGINE = Analytic; DESCRIBE TABLE `t`; The result is:\nname type is_primary is_nullable is_tag t timestamp true false false tsid uint64 true false false a int false true false b string false true false EXPLAIN 1 EXPLAIN query; EXPLAIN shows how a query will be executed. Add it to the beginning of a query like\n1 EXPLAIN SELECT max(value) AS c1, avg(value) AS c2 FROM `t` GROUP BY name; will give\nlogical_plan Projection: #MAX(07_optimizer_t.value) AS c1, #AVG(07_optimizer_t.value) AS c2 Aggregate: groupBy=[[#07_optimizer_t.name]], aggr=[[MAX(#07_optimizer_t.value), AVG(#07_optimizer_t.value)]] TableScan: 07_optimizer_t projection=Some([name, value]) physical_plan ProjectionExec: expr=[MAX(07_optimizer_t.value)@1 as c1, AVG(07_optimizer_t.value)@2 as c2] AggregateExec: mode=FinalPartitioned, gby=[name@0 as name], aggr=[MAX(07_optimizer_t.value), AVG(07_optimizer_t.value)] CoalesceBatchesExec: target_batch_size=4096 RepartitionExec: partitioning=Hash([Column { name: \\\"name\\\", index: 0 }], 6) AggregateExec: mode=Partial, gby=[name@0 as name], aggr=[MAX(07_optimizer_t.value), AVG(07_optimizer_t.value)] ScanTable: table=07_optimizer_t, parallelism=8, order=None ","categories":"","description":"","excerpt":"There are serval utilities SQL in HoraeDB that can help in table …","ref":"/docs/user-guide/sql/utility/","tags":"","title":"Utility Statements"},{"body":"HoraeDB 是一个开放的系统，鼓励合作和创新，允许开发者使用最适合其自身需求的系统。目前 HoraeDB 支持以下系统：\n","categories":"","description":"","excerpt":"HoraeDB 是一个开放的系统，鼓励合作和创新，允许开发者使用最适合其自身需求的系统。目前 HoraeDB 支持以下系统：\n","ref":"/cn/docs/user-guide/ecosystem/","tags":"","title":"周边生态"},{"body":"HoraeDB 中有许多实用的 SQL 工具，可以辅助表操作或查询检查。\n查看建表语句 1 SHOW CREATE TABLE table_name; SHOW CREATE TABLE 返回指定表的当前版本的创建语句，包括列定义、表引擎和参数选项等。例如：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 -- create one table CREATE TABLE `t` (a bigint, b int default 3, c string default 'x', d smallint null, t timestamp NOT NULL, TIMESTAMP KEY(t)) ENGINE = Analytic; -- Result: affected_rows: 0 -- show how one table should be created. SHOW CREATE TABLE `t`; -- Result DDL: CREATE TABLE `t` ( `t` timestamp NOT NULL, `tsid` uint64 NOT NULL, `a` bigint, `b` int, `c` string, `d` smallint, PRIMARY KEY(t,tsid), TIMESTAMP KEY(t) ) ENGINE=Analytic WITH ( arena_block_size='2097152', compaction_strategy='default', compression='ZSTD', enable_ttl='true', num_rows_per_row_group='8192', segment_duration='', ttl='7d', update_mode='OVERWRITE', write_buffer_size='33554432' ) 查看表信息 1 DESCRIBE table_name; DESCRIBE 语句返回一个表的详细结构信息，包括每个字段的名称和类型，字段是否为 Tag 或主键，字段是否可空等。 此外，自动生成的字段 tsid 也会展示在结果里。\n例如：\n1 2 3 CREATE TABLE `t`(a int, b string, t timestamp NOT NULL, TIMESTAMP KEY(t)) ENGINE = Analytic; DESCRIBE TABLE `t`; 返回结果如下：\nname type is_primary is_nullable is_tag t timestamp true false false tsid uint64 true false false a int false true false b string false true false 解释执行计划 1 EXPLAIN query; EXPLAIN 语句结果展示一个查询如何被执行。例如：\n1 EXPLAIN SELECT max(value) AS c1, avg(value) AS c2 FROM `t` GROUP BY name; 结果如下：\nlogical_plan Projection: #MAX(07_optimizer_t.value) AS c1, #AVG(07_optimizer_t.value) AS c2 Aggregate: groupBy=[[#07_optimizer_t.name]], aggr=[[MAX(#07_optimizer_t.value), AVG(#07_optimizer_t.value)]] TableScan: 07_optimizer_t projection=Some([name, value]) physical_plan ProjectionExec: expr=[MAX(07_optimizer_t.value)@1 as c1, AVG(07_optimizer_t.value)@2 as c2] AggregateExec: mode=FinalPartitioned, gby=[name@0 as name], aggr=[MAX(07_optimizer_t.value), AVG(07_optimizer_t.value)] CoalesceBatchesExec: target_batch_size=4096 RepartitionExec: partitioning=Hash([Column { name: \\\"name\\\", index: 0 }], 6) AggregateExec: mode=Partial, gby=[name@0 as name], aggr=[MAX(07_optimizer_t.value), AVG(07_optimizer_t.value)] ScanTable: table=07_optimizer_t, parallelism=8, order=None ","categories":"","description":"","excerpt":"HoraeDB 中有许多实用的 SQL 工具，可以辅助表操作或查询检查。\n查看建表语句 1 SHOW CREATE TABLE …","ref":"/cn/docs/user-guide/sql/utility/","tags":"","title":"常用 SQL"},{"body":"Options below can be used when create table for analytic engine\nenable_ttl, bool. When enable TTL on a table, rows older than ttl will be deleted and can’t be querid, default true\nttl, duration, lifetime of a row, only used when enable_ttl is true. default 7d.\nstorage_format, string. The underlying column’s format. Availiable values:\ncolumnar, default hybrid, Note: This feature is still in development, and it may change in the future. The meaning of those two values are in Storage format section.\nStorage Format There are mainly two formats supported in analytic engine. One is columnar, which is the traditional columnar format, with one table column in one physical column:\n1 2 3 4 5 6 7 8 9 | Timestamp | Device ID | Status Code | Tag 1 | Tag 2 | | --------- |---------- | ----------- | ----- | ----- | | 12:01 | A | 0 | v1 | v1 | | 12:01 | B | 0 | v2 | v2 | | 12:02 | A | 0 | v1 | v1 | | 12:02 | B | 1 | v2 | v2 | | 12:03 | A | 0 | v1 | v1 | | 12:03 | B | 0 | v2 | v2 | | ..... | | | | | The other one is hybrid, an experimental format used to simulate row-oriented storage in columnar storage to accelerate classic time-series query.\nIn classic time-series user cases like IoT or DevOps, queries will typically first group their result by series id(or device id), then by timestamp. In order to achieve good performance in those scenarios, the data physical layout should match this style, so the hybrid format is proposed like this:\n1 2 3 4 5 | Device ID | Timestamp | Status Code | Tag 1 | Tag 2 | minTime | maxTime | |-----------|---------------------|-------------|-------|-------|---------|---------| | A | [12:01,12:02,12:03] | [0,0,0] | v1 | v1 | 12:01 | 12:03 | | B | [12:01,12:02,12:03] | [0,1,0] | v2 | v2 | 12:01 | 12:03 | | ... | | | | | | | Within one file, rows belonging to the same primary key(eg: series/device id) are collapsed into one row The columns besides primary key are divided into two categories: collapsible, those columns will be collapsed into a list. Used to encode fields in time-series table Note: only fixed-length type is supported now non-collapsible, those columns should only contain one distinct value. Used to encode tags in time-series table Note: only string type is supported now Two more columns are added, minTime and maxTime. Those are used to cut unnecessary rows out in query. Note: Not implemented yet. Example 1 2 3 4 5 6 7 8 9 10 11 CREATE TABLE `device` ( `ts` timestamp NOT NULL, `tag1` string tag, `tag2` string tag, `value1` double, `value2` int, timestamp KEY (ts)) ENGINE=Analytic with ( enable_ttl = 'false', storage_format = 'hybrid' ); This will create a table with hybrid format, users can inspect data format with parquet-tools. The table above should have following parquet schema:\nmessage arrow_schema { optional group ts (LIST) { repeated group list { optional int64 item (TIMESTAMP(MILLIS,false)); } } required int64 tsid (INTEGER(64,false)); optional binary tag1 (STRING); optional binary tag2 (STRING); optional group value1 (LIST) { repeated group list { optional double item; } } optional group value2 (LIST) { repeated group list { optional int32 item; } } } ","categories":"","description":"","excerpt":"Options below can be used when create table for analytic engine …","ref":"/docs/user-guide/sql/engine_options/","tags":"","title":"Options"},{"body":"建表时可以使用下列的选项配置引擎：\nenable_ttl：布尔类型，默认为 true，当一个表配置 TTL 时，早于 ttl 的数据不会被查询到并且会被删除。\nttl：duration 类型，默认值为7d，此项定义数据的生命周期，只在 enable_ttl 为 true 的情况下使用。\nstorage_format： string 类型，数据存储的格式，有两种可选:\ncolumnar, 默认值 hybrid, 注意：此功能仍在开发中，将来可能会发生变化。 上述两种存储格式详见 存储格式 部分。\n存储格式 HoraeDB 支持两种存储格式，一个是 columnar, 这是传统的列式格式，一个物理列中存储表的一个列。\n1 2 3 4 5 6 7 8 9 | Timestamp | Device ID | Status Code | Tag 1 | Tag 2 | | --------- |---------- | ----------- | ----- | ----- | | 12:01 | A | 0 | v1 | v1 | | 12:01 | B | 0 | v2 | v2 | | 12:02 | A | 0 | v1 | v1 | | 12:02 | B | 1 | v2 | v2 | | 12:03 | A | 0 | v1 | v1 | | 12:03 | B | 0 | v2 | v2 | | ..... | | | | | 另一个是 hybrid, 当前还在实验阶段的存储格式，用于在列式存储中模拟面向行的存储，以加速经典的时序查询。\n在经典的时序场景中，如 IoT 或 DevOps，查询通常会先按系列 ID（或设备 ID）分组，然后再按时间戳分组。 为了在这些场景中实现良好的性能，数据的物理布局应该与这种风格相匹配， hybrid 格式就是这样提出的。\n1 2 3 4 5 | Device ID | Timestamp | Status Code | Tag 1 | Tag 2 | minTime | maxTime | |-----------|---------------------|-------------|-------|-------|---------|---------| | A | [12:01,12:02,12:03] | [0,0,0] | v1 | v1 | 12:01 | 12:03 | | B | [12:01,12:02,12:03] | [0,1,0] | v2 | v2 | 12:01 | 12:03 | | ... | | | | | | | 在一个文件中，同一个主键（例如设备 ID）的数据会被压缩到一行。 除了主键之外的列被分成两类： collapsible, 这些列会被压缩成一个 list，常用于时序表中的field字段。 注意: 当前仅支持定长的字段 non-collapsible, 这些列只能包含一个去重值，常用于时序表中的tag字段。 注意: 当前仅支持字符串类型 另外多加了两个字段，minTime 和 maxTime， 用于查询中过滤不必要的数据。 注意: 暂未实现此能力 示例 1 2 3 4 5 6 7 8 9 10 11 CREATE TABLE `device` ( `ts` timestamp NOT NULL, `tag1` string tag, `tag2` string tag, `value1` double, `value2` int, timestamp KEY (ts)) ENGINE=Analytic with ( enable_ttl = 'false', storage_format = 'hybrid' ); 这段语句会创建一个混合存储格式的表, 这种情况下用户可以通过 parquet-tools查看数据格式. 上面定义的表的 parquet 结构如下所示：\nmessage arrow_schema { optional group ts (LIST) { repeated group list { optional int64 item (TIMESTAMP(MILLIS,false)); } } required int64 tsid (INTEGER(64,false)); optional binary tag1 (STRING); optional binary tag2 (STRING); optional group value1 (LIST) { repeated group list { optional double item; } } optional group value2 (LIST) { repeated group list { optional int32 item; } } } ","categories":"","description":"","excerpt":"建表时可以使用下列的选项配置引擎：\nenable_ttl：布尔类型，默认为 true，当一个表配置 TTL 时，早于 ttl 的数据不会被查 …","ref":"/cn/docs/user-guide/sql/engine_options/","tags":"","title":"配置项"},{"body":"HoraeDB SQL is implemented with DataFusion, Here is the list of scalar functions. See more detail, Refer to Datafusion\nMath Functions Function Description abs(x) absolute value acos(x) inverse cosine asin(x) inverse sine atan(x) inverse tangent atan2(y, x) inverse tangent of y / x ceil(x) nearest integer greater than or equal to argument cos(x) cosine exp(x) exponential floor(x) nearest integer less than or equal to argument ln(x) natural logarithm log10(x) base 10 logarithm log2(x) base 2 logarithm power(base, exponent) base raised to the power of exponent round(x) round to nearest integer signum(x) sign of the argument (-1, 0, +1) sin(x) sine sqrt(x) square root tan(x) tangent trunc(x) truncate toward zero Conditional Functions Function Description coalesce Returns the first of its arguments that is not null. Null is returned only if all arguments are null. It is often used to substitute a default value for null values when data is retrieved for display. nullif Returns a null value if value1 equals value2; otherwise it returns value1. This can be used to perform the inverse operation of the coalesce expression. String Functions Function Description ascii Returns the numeric code of the first character of the argument. In UTF8 encoding, returns the Unicode code point of the character. In other multibyte encodings, the argument must be an ASCII character. bit_length Returns the number of bits in a character string expression. btrim Removes the longest string containing any of the specified characters from the start and end of string. char_length Equivalent to length. character_length Equivalent to length. concat Concatenates two or more strings into one string. concat_ws Combines two values with a given separator. chr Returns the character based on the number code. initcap Capitalizes the first letter of each word in a string. left Returns the specified leftmost characters of a string. length Returns the number of characters in a string. lower Converts all characters in a string to their lower case equivalent. lpad Left-pads a string to a given length with a specific set of characters. ltrim Removes the longest string containing any of the characters in characters from the start of string. md5 Calculates the MD5 hash of a given string. octet_length Equivalent to length. repeat Returns a string consisting of the input string repeated a specified number of times. replace Replaces all occurrences in a string of a substring with a new substring. reverse Reverses a string. right Returns the specified rightmost characters of a string. rpad Right-pads a string to a given length with a specific set of characters. rtrim Removes the longest string containing any of the characters in characters from the end of string. digest Calculates the hash of a given string. split_part Splits a string on a specified delimiter and returns the specified field from the resulting array. starts_with Checks whether a string starts with a particular substring. strpos Searches a string for a specific substring and returns its position. substr Extracts a substring of a string. translate Translates one set of characters into another. trim Removes the longest string containing any of the characters in characters from either the start or end of string. upper Converts all characters in a string to their upper case equivalent. Regular Expression Functions Function Description regexp_match Determines whether a string matches a regular expression pattern. regexp_replace Replaces all occurrences in a string of a substring that matches a regular expression pattern with a new substring. Temporal Functions Function Description to_timestamp Converts a string to type Timestamp(Nanoseconds, None). to_timestamp_millis Converts a string to type Timestamp(Milliseconds, None). to_timestamp_micros Converts a string to type Timestamp(Microseconds, None). to_timestamp_seconds Converts a string to type Timestamp(Seconds, None). extract Retrieves subfields such as year or hour from date/time values. date_part Retrieves subfield from date/time values. date_trunc Truncates date/time values to specified precision. date_bin Bin date/time values to specified precision. from_unixtime Converts Unix epoch to type Timestamp(Nanoseconds, None). now Returns current time as Timestamp(Nanoseconds, UTC). Other Functions Function Description array Create an array. arrow_typeof Returns underlying type. in_list Check if value in list. random Generate random value. sha224 sha224 sha256 sha256 sha384 sha384 sha512 sha512 to_hex Convert to hex. ","categories":"","description":"","excerpt":"HoraeDB SQL is implemented with DataFusion, Here is the list of scalar …","ref":"/docs/user-guide/sql/scalar_functions/","tags":"","title":"Scalar Functions"},{"body":"HoraeDB SQL is implemented with DataFusion, Here is the list of aggregate functions. See more detail, Refer to Datafusion\nGeneral Function Description min Returns the minimum value in a numerical column max Returns the maximum value in a numerical column count Returns the number of rows avg Returns the average of a numerical column sum Sums a numerical column array_agg Puts values into an array Statistical Function Description var / var_samp Returns the variance of a given column var_pop Returns the population variance of a given column stddev / stddev_samp Returns the standard deviation of a given column stddev_pop Returns the population standard deviation of a given column covar / covar_samp Returns the covariance of a given column covar_pop Returns the population covariance of a given column corr Returns the correlation coefficient of a given column Approximate Function Description approx_distinct Returns the approximate number (HyperLogLog) of distinct input values approx_median Returns the approximate median of input values. It is an alias of approx_percentile_cont(x, 0.5). approx_percentile_cont Returns the approximate percentile (TDigest) of input values, where p is a float64 between 0 and 1 (inclusive). It supports raw data as input and build Tdigest sketches during query time, and is approximately equal to approx_percentile_cont_with_weight(x, 1, p). approx_percentile_cont_with_weight Returns the approximate percentile (TDigest) of input values with weight, where w is weight column expression and p is a float64 between 0 and 1 (inclusive). It supports raw data as input or pre-aggregated TDigest sketches, then builds or merges Tdigest sketches during query time. TDigest sketches are a list of centroid (x, w), where x stands for mean and w stands for weight. ","categories":"","description":"","excerpt":"HoraeDB SQL is implemented with DataFusion, Here is the list of …","ref":"/docs/user-guide/sql/aggregate_functions/","tags":"","title":"Aggregate Functions"},{"body":"在数字化浪潮席卷全球的今天，开源软件已成为推动科技创新的核心驱动力。ApacheCon 作为全球最具影响力的开源盛会之一，每年都吸引着来自世界各地的开发者、技术专家和企业代表。\n这次 Community Over Code 北美大会将在丹佛君悦大酒店举行为期四天的现场会议，重点讨论搜索、大数据、物联网、社区、地理空间、金融技术和许多其他主题。\nApache HoraeDB PPMC 成员 jiacai2050 将带来 Optimizing Apache HoraeDB for High-Cardinality Metrics at AntGroup 为题的分享，欢迎在场的朋友前来观看、交流 🤝\n更多议程内容，可以参考官网的 Sessions Schedule 。\n","categories":"","description":"","excerpt":"在数字化浪潮席卷全球的今天，开源软件已成为推动科技创新的核心驱动力。ApacheCon 作为全球最具影响力的开源盛会之一，每年都吸引着来自世 …","ref":"/cn/blog/2024/coc-na/","tags":["community"],"title":"Apache HoraeDB 与你相约 Community Over Code 北美大会"},{"body":"","categories":"","description":"","excerpt":"","ref":"/cn/tags/community/","tags":"","title":"Community"},{"body":"","categories":"","description":"","excerpt":"","ref":"/cn/tags/","tags":"","title":"Tags"},{"body":"各位开发者们，\n今天很高兴宣布，HoraeDB 社区新增了一个 committer：鲍金日，他之前的贡献得到 HoraeDB PPMC 的一致认可，并于 2024-09-10 投票成为 committer，感谢他一直以来的贡献。下面是他的贡献记录：\n33 PRs to main repo 4 PRs to docs repo 13 PRs to meta repo 需要明确一点，成为 committer 并不需要额外做什么，主要是方便开发者后续可以更便利（有仓库写权限）地参与社区贡献。\n最后，再次感谢鲍金日一直以来对社区的付出，衷心希望他能继续贡献，共同见证社区的成长。\n欢迎更多感兴趣的朋友加入我们的社区，与我们近距离交流。\n","categories":"","description":"","excerpt":"各位开发者们，\n今天很高兴宣布，HoraeDB 社区新增了一个 committer：鲍金日，他之前的贡献得到 HoraeDB PPMC 的一 …","ref":"/cn/blog/2024/new-committer-baojinri/","tags":["community"],"title":"欢迎 Apache HoraeDB 新晋 Committer： 鲍金日"},{"body":"各位开发者们，\n欢迎大家参加我们的第一次线上会议！这次会议标志着我们团队在项目开发过程中的一个重要里程碑，也是我们合作共赢的开始。\n会议目的 这次会议的主要目的是让大家相互认识，了解项目的总体目标，讨论开发的初步计划，并明确各自的角色和责任。我们希望通过此次会议，能够为接下来的工作奠定坚实的基础，并建立起一个高效、透明的沟通机制。\n主要议题 Apache HoraeDB 项目现状介绍 新 Metric Engine 设计方案介绍 参会方式 入会链接：https://meeting.dingtalk.com/j/011mRkbIdqL\n可通过浏览器直接入会，无需下载钉钉。\n也欢迎感兴趣的朋友可以加入我们的社区（钉钉群、微信公众号等），获取社区最新动态。\n会议时间 2024 年 08 月 27 日 周二，21:00-22:00 (GMT+8)\n","categories":"","description":"","excerpt":"各位开发者们，\n欢迎大家参加我们的第一次线上会议！这次会议标志着我们团队在项目开发过程中的一个重要里程碑，也是我们合作共赢的开始。 …","ref":"/cn/blog/2024/first-online-meeting/","tags":["community"],"title":"预告：第一次线上会议"},{"body":"Upgrade from 1.x.x to 2.0.0 The transition from CeresDB to Apache HoraeDB introduces several breaking changes. To facilitate upgrading from older versions to v2.0.0, specific alterations are necessary.\nUpgrade Steps Setup required envs export HORAEDB_DEFAULT_CATALOG=ceresdb Update config Etcd’s root should be configured both in horaedb and horaemeta\nFor horaedb\n[cluster_deployment.etcd_client] server_addrs = ['127.0.0.1:2379'] root_path = \"/rootPath\" For horaemeta\nstorage-root-path = \"/rootPath\" Upgrade horaemeta Horaedb will throw following errors, which is expected\n2024-01-23 14:37:57.726 ERRO [src/cluster/src/cluster_impl.rs:136] Send heartbeat to meta failed, err:Failed to send heartbeat, cluster:defaultCluster, err:status: Unimplemented, message: \"unknown service meta_service.MetaRpcService\", details: [], metadata: MetadataMap { headers: {\"content-type\": \"application/grpc\"} } Upgrade horaedb After all server upgraded, the cluster should be ready for read/write, and old data could be queried like before.\nWhat’s Changed Breaking Changes refactor!: refactor shard version logic by @ZuLiangWang in https://github.com/apache/horaedb/pull/1286 Features feat: support re-acquire shard lock in a fast way by @ShiKaiWi in https://github.com/apache/horaedb/pull/1251 feat: support alter partition table by @chunshao90 in https://github.com/apache/horaedb/pull/1244 feat: support access etcd with tls by @ShiKaiWi in https://github.com/apache/horaedb/pull/1254 feat: support schema validate in remote write by @ShiKaiWi in https://github.com/apache/horaedb/pull/1256 feat: avoid flush when drop table by @jiacai2050 in https://github.com/apache/horaedb/pull/1257 feat: opentsdb api support gzip body by @tanruixiang in https://github.com/apache/horaedb/pull/1261 feat: infer timestamp constraint for single-timestamp column by @Dennis40816 in https://github.com/apache/horaedb/pull/1266 feat: primary keys support sample by @jiacai2050 in https://github.com/apache/horaedb/pull/1243 feat: cache space total memory by @jiacai2050 in https://github.com/apache/horaedb/pull/1278 feat: skip record column values for level0 sst by @jiacai2050 in https://github.com/apache/horaedb/pull/1282 feat: support write wal logs in columnar format by @ShiKaiWi in https://github.com/apache/horaedb/pull/1179 feat: support stack size of read threads configurable by @ShiKaiWi in https://github.com/apache/horaedb/pull/1305 feat: impl DoNothing wal by @jiacai2050 in https://github.com/apache/horaedb/pull/1311 feat: slow log include remote query by @jiacai2050 in https://github.com/apache/horaedb/pull/1316 feat: use string for request id by @jiacai2050 in https://github.com/apache/horaedb/pull/1349 feat: support metrics for number of bytes fetched from object storage by @ShiKaiWi in https://github.com/apache/horaedb/pull/1363 feat: avoid building dictionary for massive unique column values by @ShiKaiWi in https://github.com/apache/horaedb/pull/1365 feat: utilize the column cardinality for deciding whether to do dict by @ShiKaiWi in https://github.com/apache/horaedb/pull/1372 feat: avoid pulling unnecessary columns when querying append mode table by @Rachelint in https://github.com/apache/horaedb/pull/1307 feat: dist sql analyze by @baojinri in https://github.com/apache/horaedb/pull/1260 feat: impl priority runtime for read by @jiacai2050 in https://github.com/apache/horaedb/pull/1303 feat: upgrade horaedbproto by @chunshao90 in https://github.com/apache/horaedb/pull/1408 feat: block rules support query by @jiacai2050 in https://github.com/apache/horaedb/pull/1420 feat: try load page indexes by @jiacai2050 in https://github.com/apache/horaedb/pull/1425 feat: support setting meta_addr\u0026etcd_addrs by env by @chunshao90 in https://github.com/apache/horaedb/pull/1427 feat: add table status check by @ZuLiangWang in https://github.com/apache/horaedb/pull/1418 feat: support docker-compose and update README by @chunshao90 in https://github.com/apache/horaedb/pull/1429 feat: impl layered memtable to reduce duplicated encode during scan by @Rachelint in https://github.com/apache/horaedb/pull/1271 feat: update disk cache in another thread to avoid blocking normal query process by @jiacai2050 in https://github.com/apache/horaedb/pull/1431 feat: update pgwire to 0.19 by @sunng87 in https://github.com/apache/horaedb/pull/1436 feat: filter out MySQL federated components’ emitted statements by @chunshao90 in https://github.com/apache/horaedb/pull/1439 feat: add system_stats lib to collect system stats by @ShiKaiWi in https://github.com/apache/horaedb/pull/1442 feat(horaectl): initial commit by @chunshao90 in https://github.com/apache/horaedb/pull/1450 feat: support collect statistics about the engine by @ShiKaiWi in https://github.com/apache/horaedb/pull/1451 feat: persist sst meta size by @jiacai2050 in https://github.com/apache/horaedb/pull/1440 feat: add sst level config for benchmark by @zealchen in https://github.com/apache/horaedb/pull/1482 feat: add exponential backoff when retry by @zealchen in https://github.com/apache/horaedb/pull/1486 Refactor refactor: move wal structs and traits to wal crate by @tisonkun in https://github.com/apache/horaedb/pull/1263 refactor: improve error readability by @jiacai2050 in https://github.com/apache/horaedb/pull/1265 refactor: move wal crate to under src folder by @tisonkun in https://github.com/apache/horaedb/pull/1270 refactor: use notifier::RequestNotifiers instead of dedup_requests::RequestNotifiers by @baojinri in https://github.com/apache/horaedb/pull/1249 refactor: conditionally compile wal impls by @tisonkun in https://github.com/apache/horaedb/pull/1272 refactor: remove unused min/max timestamp in the RowGroup by @ShiKaiWi in https://github.com/apache/horaedb/pull/1297 refactor: avoid duplicate codes by @ShiKaiWi in https://github.com/apache/horaedb/pull/1371 refactor: avoid returning metrics in non-analyze sql by @baojinri in https://github.com/apache/horaedb/pull/1410 refactor: move sub crates to the src directory by @chunshao90 in https://github.com/apache/horaedb/pull/1443 refactor: adjust cpu’s stats by @ShiKaiWi in https://github.com/apache/horaedb/pull/1457 refactor: refactor compaction process for remote compaction by @Rachelint in https://github.com/apache/horaedb/pull/1476 Fixed fix: dist query dedup by @Rachelint in https://github.com/apache/horaedb/pull/1269 fix: log third party crates by @jiacai2050 in https://github.com/apache/horaedb/pull/1289 fix: ensure primary key order by @jiacai2050 in https://github.com/apache/horaedb/pull/1292 fix: use flag in preflush to indicate whether reorder is required by @jiacai2050 in https://github.com/apache/horaedb/pull/1298 fix: alter partition table tag column by @chunshao90 in https://github.com/apache/horaedb/pull/1304 fix: increase wait duration for flush by @jiacai2050 in https://github.com/apache/horaedb/pull/1315 fix: add license to workspace members by @jiacai2050 in https://github.com/apache/horaedb/pull/1317 fix: ensure channel size non zero by @jiacai2050 in https://github.com/apache/horaedb/pull/1345 fix: fix create table result by @ZuLiangWang in https://github.com/apache/horaedb/pull/1354 Revert “fix: fix create table result” by @ZuLiangWang in https://github.com/apache/horaedb/pull/1355 fix: fix test create table result by @ZuLiangWang in https://github.com/apache/horaedb/pull/1357 fix: no write stall by @ShiKaiWi in https://github.com/apache/horaedb/pull/1388 fix: collect metrics for get_ranges by @ShiKaiWi in https://github.com/apache/horaedb/pull/1364 fix: ignore collecting fetched bytes stats when sst file is read only once by @ShiKaiWi in https://github.com/apache/horaedb/pull/1369 fix: publich nightly image by @chunshao90 in https://github.com/apache/horaedb/pull/1396 fix: missing and verbose logs by @ShiKaiWi in https://github.com/apache/horaedb/pull/1398 fix: fix broken link by @caicancai in https://github.com/apache/horaedb/pull/1399 fix: the broken link about the issue status by @ShiKaiWi in https://github.com/apache/horaedb/pull/1402 fix: skip wal encoding when data wal is disabled by @jiacai2050 in https://github.com/apache/horaedb/pull/1401 fix: disable percentile for distributed tables by @jiacai2050 in https://github.com/apache/horaedb/pull/1406 fix: compatible for old table options by @Rachelint in https://github.com/apache/horaedb/pull/1432 fix: get_ranges is not spawned in io-runtime by @ShiKaiWi in https://github.com/apache/horaedb/pull/1426 fix: table name is normalized when find timestamp column by @jiacai2050 in https://github.com/apache/horaedb/pull/1446 fix: changes required for migrate dev to main by @jiacai2050 in https://github.com/apache/horaedb/pull/1455 fix: missing filter index over the primary keys by @ShiKaiWi in https://github.com/apache/horaedb/pull/1456 fix: random failure of test_collect_system_stats by @ShiKaiWi in https://github.com/apache/horaedb/pull/1459 fix(ci): refactor ci trigger conditions by @jiacai2050 in https://github.com/apache/horaedb/pull/1474 Docs chore(docs): rename CeresDB to HoraeDB by @caicancai in https://github.com/apache/horaedb/pull/1337 chore/docs: remove broken link by @caicancai in https://github.com/apache/horaedb/pull/1341 docs: update CONTRIBUTING.md by @suyanhanx in https://github.com/apache/horaedb/pull/1382 doc: fix broken link by @caicancai in https://github.com/apache/horaedb/pull/1358 chore/doc: rename ceresdb to horaedb by @caicancai in https://github.com/apache/horaedb/pull/1332 doc: add MySQL-Client in README by @jackwener in https://github.com/apache/horaedb/pull/1331 doc: fix link in illegal markdown format by @jackwener in https://github.com/apache/horaedb/pull/1334 style: normalize comments/doc in rustfmt by @jackwener in https://github.com/apache/horaedb/pull/1335 docs: add sudo for install commands by @caicancai in https://github.com/apache/horaedb/pull/1347 docs: sync GH activities to commits only by @tisonkun in https://github.com/apache/horaedb/pull/1385 chore(docs): fix invalid repo links by @SYaoJun in https://github.com/apache/horaedb/pull/1452 chore(docs): fix invalid repo links by @Apricity001 in https://github.com/apache/horaedb/pull/1472 Chore chore(deps): bump golang.org/x/net from 0.5.0 to 0.17.0 in /integration_tests/sdk/go by @dependabot in https://github.com/apache/horaedb/pull/1258 chore: delete the configuration related to github cache by @tanruixiang in https://github.com/apache/horaedb/pull/1259 chore: remove backtrace of blocked table by @chunshao90 in https://github.com/apache/horaedb/pull/1267 ci: setup golang in CI by @tisonkun in https://github.com/apache/horaedb/pull/1275 chore: remove default features in analytic_engine by @jiacai2050 in https://github.com/apache/horaedb/pull/1277 chore(deps): bump google.golang.org/grpc from 1.53.0 to 1.56.3 in /integration_tests/sdk/go by @dependabot in https://github.com/apache/horaedb/pull/1280 test: simplify ceresmeta-server installation by @tisonkun in https://github.com/apache/horaedb/pull/1287 chore: enable blank issue by @ShiKaiWi in https://github.com/apache/horaedb/pull/1290 chore: add metrics to inspect write path by @Rachelint in https://github.com/apache/horaedb/pull/1264 chore: refactor build_meta.sh in integration-test by @chunshao90 in https://github.com/apache/horaedb/pull/1306 chore: rename ceresdb to horaedb by @chunshao90 in https://github.com/apache/horaedb/pull/1310 edit: add schema id, schema name, catalog name in TableData by @dust1 in https://github.com/apache/horaedb/pull/1294 chore: ignore seq check for DoNothing wal by @jiacai2050 in https://github.com/apache/horaedb/pull/1314 chore: remove community by @jiacai2050 in https://github.com/apache/horaedb/pull/1318 chore: try to clear ceresdb stuff by @tisonkun in https://github.com/apache/horaedb/pull/1320 chore: change copyright owner by @tisonkun in https://github.com/apache/horaedb/pull/1321 ci: stop release docker image before we finish the rename and transfer by @tisonkun in https://github.com/apache/horaedb/pull/1323 chore: bump deps by @jiacai2050 in https://github.com/apache/horaedb/pull/1325 chore: rename ceresmeta to horaemeta by @chunshao90 in https://github.com/apache/horaedb/pull/1327 chore: rename binary to horaedb-server and more by @tisonkun in https://github.com/apache/horaedb/pull/1330 chore(license): rename license-header.txt’s CeresDB to HoraeDB by @caicancai in https://github.com/apache/horaedb/pull/1336 chore: replace ceresdb with horaedb by @jackwener in https://github.com/apache/horaedb/pull/1338 chore: more rename to horaedb by @tisonkun in https://github.com/apache/horaedb/pull/1340 chore: update create table integration test result by @ZuLiangWang in https://github.com/apache/horaedb/pull/1344 chore: disable frequently failed tests by @jiacai2050 in https://github.com/apache/horaedb/pull/1352 test: add integration test for alter table options by @caicancai in https://github.com/apache/horaedb/pull/1346 chore: ignore flush failure when flush by @ShiKaiWi in https://github.com/apache/horaedb/pull/1362 chore: disable timeout for http api by @jiacai2050 in https://github.com/apache/horaedb/pull/1367 chore: disable block for http api by @jiacai2050 in https://github.com/apache/horaedb/pull/1368 config: add .asf.yaml by @chunshao90 in https://github.com/apache/horaedb/pull/1377 ci: remove missing Required status by @tisonkun in https://github.com/apache/horaedb/pull/1383 chore: git repo link type fix by @fengmk2 in https://github.com/apache/horaedb/pull/1378 chore: apply ASF license header by @tanruixiang in https://github.com/apache/horaedb/pull/1384 chore: add dev mail list and rename ceresdb to horaedb by @tanruixiang in https://github.com/apache/horaedb/pull/1375 chore: more rename to horaedb by @chunshao90 in https://github.com/apache/horaedb/pull/1387 chore: add push-nightly-image in workflow by @chunshao90 in https://github.com/apache/horaedb/pull/1389 chore: update README by @chunshao90 in https://github.com/apache/horaedb/pull/1390 chore: refactor for better readability by @jiacai2050 in https://github.com/apache/horaedb/pull/1400 chore: add error log for remote server by @jiacai2050 in https://github.com/apache/horaedb/pull/1407 chore: update website url by @chunshao90 in https://github.com/apache/horaedb/pull/1404 chore: upload horaedb logo by @chunshao90 in https://github.com/apache/horaedb/pull/1409 chore: add slack link by @tanruixiang in https://github.com/apache/horaedb/pull/1395 chore: update logo by @chunshao90 in https://github.com/apache/horaedb/pull/1414 chore: update horaedb logo by @chunshao90 in https://github.com/apache/horaedb/pull/1415 chore: rename ceresformat to logformat by @ZuLiangWang in https://github.com/apache/horaedb/pull/1417 chore: fix logo link in readme by @chunshao90 in https://github.com/apache/horaedb/pull/1416 chore: update github pages by @chunshao90 in https://github.com/apache/horaedb/pull/1421 chore: more rename to horaedb by @chunshao90 in https://github.com/apache/horaedb/pull/1419 chore: fix error message by @jiacai2050 in https://github.com/apache/horaedb/pull/1412 chore: remove github pages in asf.yaml by @chunshao90 in https://github.com/apache/horaedb/pull/1428 chore: skip wal seq check when wal is disabled by @jiacai2050 in https://github.com/apache/horaedb/pull/1430 chore: enable merge on github by @ShiKaiWi in https://github.com/apache/horaedb/pull/1435 chore: merge change sets on the dev branch by @ShiKaiWi in https://github.com/apache/horaedb/pull/1423 chore: fix issue status of README-CN.md by @ShiKaiWi in https://github.com/apache/horaedb/pull/1437 chore(deps): bump h2 from 0.3.17 to 0.3.24 by @dependabot in https://github.com/apache/horaedb/pull/1448 chore(deps): bump shlex from 1.1.0 to 1.3.0 by @dependabot in https://github.com/apache/horaedb/pull/1458 chore: update create tables result by @ZuLiangWang in https://github.com/apache/horaedb/pull/1454 chore: merge HoraeMeta code into HoreaDB repository by @ZuLiangWang in https://github.com/apache/horaedb/pull/1460 chore(deps): bump google.golang.org/grpc from 1.47.0 to 1.56.3 in /horaemeta by @dependabot in https://github.com/apache/horaedb/pull/1464 chore(deps): bump golang.org/x/net from 0.16.0 to 0.17.0 in /horaemeta by @dependabot in https://github.com/apache/horaedb/pull/1465 chore(deps): bump golang.org/x/crypto from 0.14.0 to 0.17.0 in /horaemeta by @dependabot in https://github.com/apache/horaedb/pull/1462 chore: rename ci’s prefix name by @tanruixiang in https://github.com/apache/horaedb/pull/1467 chore: fix github issue template by @ZuLiangWang in https://github.com/apache/horaedb/pull/1470 chore(horaemeta\u0026horaectl): refactor clusters/diagnose response body by @chunshao90 in https://github.com/apache/horaedb/pull/1475 chore: free disk for ci by @jiacai2050 in https://github.com/apache/horaedb/pull/1484 deps: bump datafusion by @tanruixiang in https://github.com/apache/horaedb/pull/1445 horaectl: remove go implementation of horaectl by @chunshao90 in https://github.com/apache/horaedb/pull/1490 chore: update version to 2.0.0, prepare for releasing v2.0.0 by @chunshao90 in https://github.com/apache/horaedb/pull/1487 New Contributors @Dennis40816 made their first contribution in https://github.com/apache/horaedb/pull/1266 @caicancai made their first contribution in https://github.com/apache/horaedb/pull/1332 @jackwener made their first contribution in https://github.com/apache/horaedb/pull/1331 @suyanhanx made their first contribution in https://github.com/apache/horaedb/pull/1382 @fengmk2 made their first contribution in https://github.com/apache/horaedb/pull/1378 @sunng87 made their first contribution in https://github.com/apache/horaedb/pull/1436 @SYaoJun made their first contribution in https://github.com/apache/horaedb/pull/1452 @Apricity001 made their first contribution in https://github.com/apache/horaedb/pull/1472 Full Changelog: https://github.com/apache/horaedb/compare/v1.2.7...v2.0.0\n","categories":"","description":"This is the first version after enter ASF incubator, thanks everyone for making it happen!","excerpt":"This is the first version after enter ASF incubator, thanks everyone …","ref":"/blog/2024/release-2.0.0/","tags":"","title":"Release 2.0.0"},{"body":"使用 ALTER TABLE 可以改变表的结构和参数 .\n变更表结构 例如可以使用 ADD COLUMN 增加表的列 :\n1 2 3 -- create a table and add a column to it CREATE TABLE `t`(a int, t timestamp NOT NULL, TIMESTAMP KEY(t)) ENGINE = Analytic; ALTER TABLE `t` ADD COLUMN (b string); 变更后的表结构如下：\n-- DESCRIBE TABLE `t`; name type is_primary is_nullable is_tag t timestamp true false false tsid uint64 true false false a int false true false b string false true false 变更表参数 例如可以使用 MODIFY SETTING 修改表的参数 :\n1 2 3 -- create a table and add a column to it CREATE TABLE `t`(a int, t timestamp NOT NULL, TIMESTAMP KEY(t)) ENGINE = Analytic; ALTER TABLE `t` MODIFY SETTING write_buffer_size='300M'; 上面的 SQL 用来更改 writer_buffer 大小，变更后的建表如下：\n1 CREATE TABLE `t` (`tsid` uint64 NOT NULL, `t` timestamp NOT NULL, `a` int, PRIMARY KEY(tsid,t), TIMESTAMP KEY(t)) ENGINE=Analytic WITH(arena_block_size='2097152', compaction_strategy='default', compression='ZSTD', enable_ttl='true', num_rows_per_row_group='8192', segment_duration='', storage_format='AUTO', ttl='7d', update_mode='OVERWRITE', write_buffer_size='314572800') 除此之外，我们可以修改其 ttl 为 10 天：\n1 ALTER TABLE `t` MODIFY SETTING ttl='10d'; ","categories":"","description":"","excerpt":"使用 ALTER TABLE 可以改变表的结构和参数 .\n变更表结构 例如可以使用 ADD COLUMN 增加表的列 :\n1 2 3 -- …","ref":"/cn/docs/user-guide/sql/ddl/alter_table/","tags":"","title":"ALTER TABLE"},{"body":"ALTER TABLE can change the schema or options of a table.\nALTER TABLE SCHEMA HoraeDB current supports ADD COLUMN to alter table schema.\n1 2 3 -- create a table and add a column to it CREATE TABLE `t`(a int, t timestamp NOT NULL, TIMESTAMP KEY(t)) ENGINE = Analytic; ALTER TABLE `t` ADD COLUMN (b string); It now becomes:\n-- DESCRIBE TABLE `t`; name type is_primary is_nullable is_tag t timestamp true false false tsid uint64 true false false a int false true false b string false true false ALTER TABLE OPTIONS HoraeDB current supports MODIFY SETTING to alter table schema.\n1 2 3 -- create a table and add a column to it CREATE TABLE `t`(a int, t timestamp NOT NULL, TIMESTAMP KEY(t)) ENGINE = Analytic; ALTER TABLE `t` MODIFY SETTING write_buffer_size='300M'; The SQL above tries to modify the write_buffer_size of the table, and the table’s option becomes:\n1 CREATE TABLE `t` (`tsid` uint64 NOT NULL, `t` timestamp NOT NULL, `a` int, PRIMARY KEY(tsid,t), TIMESTAMP KEY(t)) ENGINE=Analytic WITH(arena_block_size='2097152', compaction_strategy='default', compression='ZSTD', enable_ttl='true', num_rows_per_row_group='8192', segment_duration='', storage_format='AUTO', ttl='7d', update_mode='OVERWRITE', write_buffer_size='314572800') Besides, the ttl can be altered from 7 days to 10 days by such SQL:\n1 ALTER TABLE `t` MODIFY SETTING ttl='10d'; ","categories":"","description":"","excerpt":"ALTER TABLE can change the schema or options of a table.\nALTER TABLE …","ref":"/docs/user-guide/sql/ddl/alter_table/","tags":"","title":"ALTER TABLE"},{"body":"Add block list If you want to reject query for a table, you can add table name to ‘read_block_list’.\nExample 1 2 3 4 5 6 7 curl --location --request POST 'http://localhost:5000/admin/block' \\ --header 'Content-Type: application/json' \\ -d '{ \"operation\":\"Add\", \"write_block_list\":[], \"read_block_list\":[\"my_table\"] }' Response 1 2 3 4 { \"write_block_list\": [], \"read_block_list\": [\"my_table\"] } Set block list You can use set operation to clear exist tables and set new tables to ‘read_block_list’ like following example.\nExample 1 2 3 4 5 6 7 curl --location --request POST 'http://localhost:5000/admin/block' \\ --header 'Content-Type: application/json' \\ -d '{ \"operation\":\"Set\", \"write_block_list\":[], \"read_block_list\":[\"my_table1\",\"my_table2\"] }' Response 1 2 3 4 { \"write_block_list\": [], \"read_block_list\": [\"my_table1\", \"my_table2\"] } Remove block list You can remove tables from ‘read_block_list’ like following example.\nExample 1 2 3 4 5 6 7 curl --location --request POST 'http://localhost:5000/admin/block' \\ --header 'Content-Type: application/json' \\ -d '{ \"operation\":\"Remove\", \"write_block_list\":[], \"read_block_list\":[\"my_table1\"] }' Response 1 2 3 4 { \"write_block_list\": [], \"read_block_list\": [\"my_table2\"] } ","categories":"","description":"","excerpt":"Add block list If you want to reject query for a table, you can add …","ref":"/docs/user-guide/operation/block_list/","tags":"","title":"Block List"},{"body":"","categories":"","description":"","excerpt":"","ref":"/cn/categories/","tags":"","title":"Categories"},{"body":"","categories":"","description":"","excerpt":"","ref":"/categories/","tags":"","title":"Categories"},{"body":"The Operations for HoraeDB cluster mode, it can only be used when HoraeMeta is deployed.\nOperation Interface You need to replace 127.0.0.1 with the actual project path.\nQuery table When tableNames is not empty, use tableNames for query. When tableNames is empty, ids are used for query. When querying with ids, schemaName is useless. curl --location 'http://127.0.0.1:8080/api/v1/table/query' \\ --header 'Content-Type: application/json' \\ -d '{ \"clusterName\":\"defaultCluster\", \"schemaName\":\"public\", \"names\":[\"demo1\", \"__demo1_0\"], }' curl --location 'http://127.0.0.1:8080/api/v1/table/query' \\ --header 'Content-Type: application/json' \\ -d '{ \"clusterName\":\"defaultCluster\", \"ids\":[0, 1] }' Query the route of table curl --location --request POST 'http://127.0.0.1:8080/api/v1/route' \\ --header 'Content-Type: application/json' \\ -d '{ \"clusterName\":\"defaultCluster\", \"schemaName\":\"public\", \"table\":[\"demo\"] }' Query the mapping of shard and node curl --location --request POST 'http://127.0.0.1:8080/api/v1/getNodeShards' \\ --header 'Content-Type: application/json' \\ -d '{ \"ClusterName\":\"defaultCluster\" }' Query the mapping of table and shard If ShardIDs in the request is empty, query with all shardIDs in the cluster. curl --location --request POST 'http://127.0.0.1:8080/api/v1/getShardTables' \\ --header 'Content-Type: application/json' \\ -d '{ \"clusterName\":\"defaultCluster\", \"shardIDs\": [1,2] }' Drop table curl --location --request POST 'http://127.0.0.1:8080/api/v1/dropTable' \\ --header 'Content-Type: application/json' \\ -d '{ \"clusterName\": \"defaultCluster\", \"schemaName\": \"public\", \"table\": \"demo\" }' Transfer leader shard curl --location --request POST 'http://127.0.0.1:8080/api/v1/transferLeader' \\ --header 'Content-Type: application/json' \\ -d '{ \"clusterName\":\"defaultCluster\", \"shardID\": 1, \"oldLeaderNodeName\": \"127.0.0.1:8831\", \"newLeaderNodeName\": \"127.0.0.1:18831\" }' Split shard curl --location --request POST 'http://127.0.0.1:8080/api/v1/split' \\ --header 'Content-Type: application/json' \\ -d '{ \"clusterName\" : \"defaultCluster\", \"schemaName\" :\"public\", \"nodeName\" :\"127.0.0.1:8831\", \"shardID\" : 0, \"splitTables\":[\"demo\"] }' Create cluster curl --location 'http://127.0.0.1:8080/api/v1/clusters' \\ --header 'Content-Type: application/json' \\ --data '{ \"name\":\"testCluster\", \"nodeCount\":3, \"shardTotal\":9, \"topologyType\":\"static\" }' Update cluster curl --location --request PUT 'http://127.0.0.1:8080/api/v1/clusters/{NewClusterName}' \\ --header 'Content-Type: application/json' \\ --data '{ \"nodeCount\":28, \"shardTotal\":128, \"topologyType\":\"dynamic\" }' List clusters curl --location 'http://127.0.0.1:8080/api/v1/clusters' Update enableSchedule curl --location --request PUT 'http://127.0.0.1:8080/api/v1/clusters/{ClusterName}/enableSchedule' \\ --header 'Content-Type: application/json' \\ --data '{ \"enable\":true }' Query enableSchedule curl --location 'http://127.0.0.1:8080/api/v1/clusters/{ClusterName}/enableSchedule' Update flow limiter curl --location --request PUT 'http://127.0.0.1:8080/api/v1/flowLimiter' \\ --header 'Content-Type: application/json' \\ --data '{ \"limit\":1000, \"burst\":10000, \"enable\":true }' Query information of flow limiter curl --location 'http://127.0.0.1:8080/api/v1/flowLimiter' List nodes of HoraeMeta cluster curl --location 'http://127.0.0.1:8080/api/v1/etcd/member' Move leader of HoraeMeta cluster curl --location 'http://127.0.0.1:8080/api/v1/etcd/moveLeader' \\ --header 'Content-Type: application/json' \\ --data '{ \"memberName\":\"meta1\" }' Add node of HoraeMeta cluster curl --location --request PUT 'http://127.0.0.1:8080/api/v1/etcd/member' \\ --header 'Content-Type: application/json' \\ --data '{ \"memberAddrs\":[\"http://127.0.0.1:42380\"] }' Replace node of HoraeMeta cluster curl --location 'http://127.0.0.1:8080/api/v1/etcd/member' \\ --header 'Content-Type: application/json' \\ --data '{ \"oldMemberName\":\"meta0\", \"newMemberAddr\":[\"http://127.0.0.1:42380\"] }' ","categories":"","description":"","excerpt":"The Operations for HoraeDB cluster mode, it can only be used when …","ref":"/docs/user-guide/operation/horaemeta/","tags":"","title":"Cluster Operation"},{"body":"","categories":"","description":"","excerpt":"","ref":"/community/","tags":"","title":"Community"},{"body":"Basic syntax Basic syntax:\n1 2 3 4 5 CREATE TABLE [IF NOT EXISTS] table_name ( column_definitions ) [partition_options] ENGINE = engine_type [WITH ( table_options )]; Column definition syntax:\n1 column_name column_type [[NOT] NULL] [TAG | TIMESTAMP KEY | PRIMARY KEY] [DICTIONARY] [COMMENT ''] Partition options syntax:\n1 PARTITION BY KEY (column_list) [PARTITIONS num] Table options syntax are key-value pairs. Value should be quoted with quotation marks ('). E.g.:\n1 ... WITH ( enable_ttl='false' ) IF NOT EXISTS Add IF NOT EXISTS to tell HoraeDB to ignore errors if the table name already exists.\nDefine Column A column’s definition should at least contains the name and type parts. All supported types are listed here.\nColumn is default be nullable. i.e. NULL keyword is implied. Adding NOT NULL constrains to make it required.\n1 2 3 4 5 6 7 -- this definition a_nullable int -- equals to a_nullable int NULL -- add NOT NULL to make it required b_not_null NOT NULL A column can be marked as special column with related keyword.\nFor string tag column, we recommend to define it as dictionary to reduce memory consumption:\n1 `tag1` string TAG DICTIONARY Engine Specifies which engine this table belongs to. HoraeDB current support Analytic engine type. This attribute is immutable.\nPartition Options Note: This feature is only supported in distributed version.\n1 CREATE TABLE ... PARTITION BY KEY Example below creates a table with 8 partitions, and partitioned by name:\n1 2 3 4 5 6 7 8 9 10 11 CREATE TABLE `demo` ( `name` string TAG COMMENT 'client username', `value` double NOT NULL, `t` timestamp NOT NULL, timestamp KEY (t) ) PARTITION BY KEY(name) PARTITIONS 8 ENGINE=Analytic with ( enable_ttl='false' ) ","categories":"","description":"","excerpt":"Basic syntax Basic syntax:\n1 2 3 4 5 CREATE TABLE [IF NOT EXISTS] …","ref":"/docs/user-guide/sql/ddl/create_table/","tags":"","title":"CREATE TABLE"},{"body":"HoraeDB implements table model, and the supported data types are similar to MySQL. The following table lists the mapping relationship between MySQL and HoraeDB.\nSupport Data Type(case-insensitive) SQL HoraeDB null Null timestamp Timestamp double Double float Float string String Varbinary Varbinary uint64 UInt64 uint32 UInt32 uint16 UInt16 uint8 UInt8 int64/bigint Int64 int32/int Int32 int16/smallint Int16 int8/tinyint Int8 boolean Boolean date Date time Time ","categories":"","description":"","excerpt":"HoraeDB implements table model, and the supported data types are …","ref":"/docs/user-guide/sql/model/data_types/","tags":"","title":"Data Types"},{"body":"Apache HoraeDB is released as source code tarballs with corresponding docker images for convenience.\nThe latest release The latest release is 2.0.0(2024-05-23), the source code can be downloaded here.\nVerify this release using the signatures, checksums by following guides below.\nDocker images Pre-built binaries are not provided yet, users can compile from source or using docker images:\nhttps://hub.docker.com/r/apache/horaemeta-server https://hub.docker.com/r/apache/horaedb-server All archived releases For older releases, please check the archive.\nVerify signatures and checksums It’s highly recommended to verify the files that you download.\nHoraeDB provides SHA digest and PGP signature files for all the files that we host on the download site. These files are named after the files they relate to but have sha512, asc extensions.\nVerify Checksums To verify the SHA digests, you need the tar.gz and its associated tar.gz.sha512 files. An example command:\n1 sha512sum -c apache-horaedb-incubating-v2.0.0-src.tar.gz.sha512 It should output something like:\napache-horaedb-incubating-v2.0.0-src.tar.gz: OK Verify Signatures To verify the PGP signatures, you will need to download the release KEYS first.\nThen import the downloaded KEYS:\n1 gpg --import KEYS Then you can verify signature:\n1 gpg --verify apache-horaedb-incubating-v2.0.0-src.tar.gz.asc It should output something like:\ngpg: Signature made Wed 12 Jun 2024 11:05:04 AM CST using RSA key ID 08A0BAB4 gpg: Good signature from \"jiacai2050@apache.org\" gpg: aka \"Jiacai Liu \u003chello@liujiacai.net\u003e\" gpg: aka \"Jiacai Liu \u003cdev@liujiacai.net\u003e\" gpg: WARNING: This key is not certified with a trusted signature! gpg: There is no indication that the signature belongs to the owner. Primary key fingerprint: 6F73 4AE4 297C 7F62 B605 4F91 D302 6E5C 08A0 BAB4 ","categories":"","description":"","excerpt":"Apache HoraeDB is released as source code tarballs with corresponding …","ref":"/downloads/","tags":"","title":"Downloads"},{"body":"基础语法 删除表的基础语法如下:\n1 DROP TABLE [IF EXISTS] table_name Drop Table 用来删除一个表，请谨慎使用这个语句，因为会同时删除表的定义和表的数据，并且无法恢复。\n","categories":"","description":"","excerpt":"基础语法 删除表的基础语法如下:\n1 DROP TABLE [IF EXISTS] table_name Drop Table 用来删除一个 …","ref":"/cn/docs/user-guide/sql/ddl/drop_table/","tags":"","title":"DROP TABLE"},{"body":"Basic syntax Basic syntax:\n1 DROP TABLE [IF EXISTS] table_name Drop Table removes a specific table. This statement should be used with caution, because it removes both the table definition and table data, and this removal is not recoverable.\n","categories":"","description":"","excerpt":"Basic syntax Basic syntax:\n1 DROP TABLE [IF EXISTS] table_name Drop …","ref":"/docs/user-guide/sql/ddl/drop_table/","tags":"","title":"DROP TABLE"},{"body":"安装 go get github.com/apache/incubator-horaedb-client-go 你可以在这里找到最新的版本 here.\n如何使用 初始化客户端 1 2 3 client, err := horaedb.NewClient(endpoint, horaedb.Direct, horaedb.WithDefaultDatabase(\"public\"), // Client所使用的database ) 参数名称 说明 defaultDatabase 所使用的 database，可以被单个 Write 或者 SQLRequest 请求中的 database 覆盖 RPCMaxRecvMsgSize grpc MaxCallRecvMsgSize 配置, 默认是 1024 _ 1024 _ 1024 RouteMaxCacheSize 如果 router 客户端中的 路由缓存超过了这个值，将会淘汰最不活跃的直至降低这个阈值, 默认是 10000 注意： HoraeDB 当前仅支持预创建的 public database , 未来会支持多个 database。\n管理表 HoraeDB 使用 SQL 来管理表格，比如创建表、删除表或者新增列等等，这和你在使用 SQL 管理其他的数据库时没有太大的区别。\n为了方便使用，在使用 gRPC 的 write 接口进行写入时，如果某个表不存在，HoraeDB 会根据第一次的写入自动创建一个表。\n当然你也可以通过 create table 语句来更精细化的管理的表（比如添加索引等）。\n创建表的样例\n1 2 3 4 5 6 7 8 9 10 11 12 13 createTableSQL := ` CREATE TABLE IF NOT EXISTS demo ( name string TAG, value double, t timestamp NOT NULL, TIMESTAMP KEY(t) ) ENGINE=Analytic with (enable_ttl=false)` req := horaedb.SQLQueryRequest{ Tables: []string{\"demo\"}, SQL: createTableSQL, } resp, err := client.SQLQuery(context.Background(), req) 删除表的样例\n1 2 3 4 5 6 dropTableSQL := `DROP TABLE demo` req := horaedb.SQLQueryRequest{ Tables: []string{\"demo\"}, SQL: dropTableSQL, } resp, err := client.SQLQuery(context.Background(), req) 构建写入数据 1 2 3 4 5 6 7 8 9 10 11 12 points := make([]horaedb.Point, 0, 2) for i := 0; i \u003c 2; i++ { point, err := horaedb.NewPointBuilder(\"demo\"). SetTimestamp(now)). AddTag(\"name\", horaedb.NewStringValue(\"test_tag1\")). AddField(\"value\", horaedb.NewDoubleValue(0.4242)). Build() if err != nil { panic(err) } points = append(points, point) } 写入数据 1 2 3 4 req := horaedb.WriteRequest{ Points: points, } resp, err := client.Write(context.Background(), req) 查询数据 1 2 3 4 5 6 7 8 9 10 querySQL := `SELECT * FROM demo` req := horaedb.SQLQueryRequest{ Tables: []string{\"demo\"}, SQL: querySQL, } resp, err := client.SQLQuery(context.Background(), req) if err != nil { panic(err) } fmt.Printf(\"query table success, rows:%+v\\n\", resp.Rows) 示例 你可以在这里找到完整的示例。\n","categories":"","description":"","excerpt":"安装 go get github.com/apache/incubator-horaedb-client-go …","ref":"/cn/docs/user-guide/sdk/go/","tags":"","title":"Go"},{"body":"Installation go get github.com/apache/incubator-horaedb-client-go You can get latest version here.\nHow To Use Init HoraeDB Client 1 2 3 client, err := horaedb.NewClient(endpoint, horaedb.Direct, horaedb.WithDefaultDatabase(\"public\"), ) option name description defaultDatabase using database, database can be overwritten by ReqContext in single Write or SQLRequest RPCMaxRecvMsgSize configration for grpc MaxCallRecvMsgSize, default 1024 _ 1024 _ 1024 RouteMaxCacheSize If the maximum number of router cache size, router client whill evict oldest if exceeded, default is 10000 Notice:\nHoraeDB currently only supports the default database public now, multiple databases will be supported in the future Manage Table HoraeDB uses SQL to manage tables, such as creating tables, deleting tables, or adding columns, etc., which is not much different from when you use SQL to manage other databases.\nFor ease of use, when using gRPC’s write interface for writing, if a table does not exist, HoraeDB will automatically create a table based on the first write.\nOf course, you can also use create table statement to manage the table more finely (such as adding indexes).\nExample for creating table\n1 2 3 4 5 6 7 8 9 10 11 12 13 createTableSQL := ` CREATE TABLE IF NOT EXISTS demo ( name string TAG, value double, t timestamp NOT NULL, TIMESTAMP KEY(t) ) ENGINE=Analytic with (enable_ttl=false)` req := horaedb.SQLQueryRequest{ Tables: []string{\"demo\"}, SQL: createTableSQL, } resp, err := client.SQLQuery(context.Background(), req) Example for droping table\n1 2 3 4 5 6 dropTableSQL := `DROP TABLE demo` req := horaedb.SQLQueryRequest{ Tables: []string{\"demo\"}, SQL: dropTableSQL, } resp, err := client.SQLQuery(context.Background(), req) How To Build Write Data 1 2 3 4 5 6 7 8 9 10 11 12 points := make([]horaedb.Point, 0, 2) for i := 0; i \u003c 2; i++ { point, err := horaedb.NewPointBuilder(\"demo\"). SetTimestamp(now). AddTag(\"name\", horaedb.NewStringValue(\"test_tag1\")). AddField(\"value\", horaedb.NewDoubleValue(0.4242)). Build() if err != nil { panic(err) } points = append(points, point) } Write Example 1 2 3 4 req := horaedb.WriteRequest{ Points: points, } resp, err := client.Write(context.Background(), req) Query Example 1 2 3 4 5 6 7 8 9 10 querySQL := `SELECT * FROM demo` req := horaedb.SQLQueryRequest{ Tables: []string{\"demo\"}, SQL: querySQL, } resp, err := client.SQLQuery(context.Background(), req) if err != nil { panic(err) } fmt.Printf(\"query table success, rows:%+v\\n\", resp.Rows) Example You can find the complete example here.\n","categories":"","description":"","excerpt":"Installation go get github.com/apache/incubator-horaedb-client-go You …","ref":"/docs/user-guide/sdk/go/","tags":"","title":"Go"},{"body":" Apache HoraeDB™ (incubating) 一款高性能、分布式的云原生时序数据库\nGitHub 快速开始 特性 高性能低成本、简单易用、社区驱动\n生态 积极拥抱开源生态，同时能够无缝对接各种开源组件。\nRead more\n100% 开源 所有代码均在 GitHub 上开源，欢迎新用户的加入！\nRead more\n","categories":"","description":"","excerpt":" Apache HoraeDB™ (incubating) 一款高性能、分布式的云原生时序数据库\nGitHub 快速开始 特性 高性能低成 …","ref":"/cn/","tags":"","title":"Horaedb"},{"body":" Apache HoraeDB™ (incubating) A high-performance, distributed, cloud native time-series database.\nGitHub Get started Features High scalability, Cost-efficiency, Community-driven\nEcosystem HoraeDB has an open ecosystem that encourages collaboration and innovation, allowing developers to use what suits them best.\nRead more\nContributions welcome! We do a Pull Request contributions workflow on GitHub. New users are always welcome!\nRead more\n","categories":"","description":"","excerpt":" Apache HoraeDB™ (incubating) A high-performance, distributed, cloud …","ref":"/","tags":"","title":"Horaedb"},{"body":"InfluxDB 是一个时间序列数据库，旨在处理高写入和查询负载。它是 TICK 堆栈的一个组成部分。InfluxDB 旨在用作涉及大量时间戳数据的任何用例的后备存储，包括 DevOps 监控、应用程序指标、物联网传感器数据和实时分析。\nHoraeDB 支持 InfluxDB v1.8 写入和查询 API。\n注意：用户需要将以下配置添加到服务器的配置中才能尝试 InfluxDB 写入/查询。\n[server.default_schema_config] default_timestamp_column_name = \"time\" 写入 curl -i -XPOST \"http://localhost:5440/influxdb/v1/write\" --data-binary ' demo,tag1=t1,tag2=t2 field1=90,field2=100 1679994647000 demo,tag1=t1,tag2=t2 field1=91,field2=101 1679994648000 demo,tag1=t11,tag2=t22 field1=90,field2=100 1679994647000 demo,tag1=t11,tag2=t22 field1=91,field2=101 1679994648000 ' Post 的内容采用的是 InfluxDB line protocol 格式。\nmeasurement 将映射到 HoraeDB 中的一个表，在首次写入时 server 会自动进行建表(注意：创建表的 TTL 是 7d，写入超过当前周期数据会被丢弃)。\n例如，在上面插入数据时，HoraeDB 中将创建下表：\nCREATE TABLE `demo` ( `tsid` uint64 NOT NULL, `time` timestamp NOT NULL, `field1` double, `field2` double, `tag1` string TAG, `tag2` string TAG, PRIMARY KEY (tsid, time), timestamp KEY (time)) 注意事项 InfluxDB 在写入时，时间戳精度默认是纳秒，HoraeDB 只支持毫秒级时间戳，用户可以通过 precision 参数指定数据精度，HoraeDB 内部会自动转成毫秒精度。 暂时不支持诸如 db 等查询参数 查询 1 curl -G 'http://localhost:5440/influxdb/v1/query' --data-urlencode 'q=SELECT * FROM \"demo\"' 查询结果和 InfluxDB 查询接口一致：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 { \"results\": [ { \"statement_id\": 0, \"series\": [ { \"name\": \"demo\", \"columns\": [\"time\", \"field1\", \"field2\", \"tag1\", \"tag2\"], \"values\": [ [1679994647000, 90.0, 100.0, \"t1\", \"t2\"], [1679994647000, 90.0, 100.0, \"t11\", \"t22\"], [1679994648000, 91.0, 101.0, \"t1\", \"t2\"], [1679994648000, 91.0, 101.0, \"t11\", \"t22\"] ] } ] } ] } 如何在 Grafana 中使用 HoraeDB 可以用作 Grafana 中的 InfluxDB 数据源。具体方式如下：\n在新增数据源时，选择 InfluxDB 类型 在 HTTP URL 处，输入 http://{ip}:{5440}/influxdb/v1/ 。对于本地部署的场景，可以直接输入 http://localhost:5440/influxdb/v1/ Save \u0026 test 注意事项 暂时不支持诸如 epoch, db 等的查询参数\n","categories":"","description":"","excerpt":"InfluxDB 是一个时间序列数据库，旨在处理高写入和查询负载。它是 TICK 堆栈的一个组成部分。InfluxDB 旨在用作涉及大量时间 …","ref":"/cn/docs/user-guide/ecosystem/influxdb/","tags":"","title":"InfluxDB"},{"body":"InfluxDB is a time series database designed to handle high write and query loads. It is an integral component of the TICK stack. InfluxDB is meant to be used as a backing store for any use case involving large amounts of timestamped data, including DevOps monitoring, application metrics, IoT sensor data, and real-time analytics.\nHoraeDB support InfluxDB v1.8 write and query API.\nWarn: users need to add following config to server’s config in order to try InfluxDB write/query.\n1 2 [server.default_schema_config] default_timestamp_column_name = \"time\" Write 1 2 3 4 5 6 curl -i -XPOST \"http://localhost:5440/influxdb/v1/write\" --data-binary ' demo,tag1=t1,tag2=t2 field1=90,field2=100 1679994647000 demo,tag1=t1,tag2=t2 field1=91,field2=101 1679994648000 demo,tag1=t11,tag2=t22 field1=90,field2=100 1679994647000 demo,tag1=t11,tag2=t22 field1=91,field2=101 1679994648000 ' Post payload is in InfluxDB line protocol format.\nMeasurement will be mapped to table in HoraeDB, and it will be created automatically in first write(Note: The default TTL is 7d, and points written exceed TTL will be discarded directly).\nFor example, when inserting data above, table below will be automatically created in HoraeDB:\n1 2 3 4 5 6 7 8 9 CREATE TABLE `demo` ( `tsid` uint64 NOT NULL, `time` timestamp NOT NULL, `field1` double, `field2` double, `tag1` string TAG, `tag2` string TAG, PRIMARY KEY (tsid, time), timestamp KEY (time)) Note When InfluxDB writes data, the timestamp precision is nanosecond by default, HoraeDB only supports millisecond timestamp, user can specify the data precision by precision parameter, HoraeDB will automatically convert to millisecond precision internally. Query string parameters such as db aren’t supported. Query 1 curl -G 'http://localhost:5440/influxdb/v1/query' --data-urlencode 'q=SELECT * FROM \"demo\"' Query result is same with InfluxDB:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 { \"results\": [ { \"statement_id\": 0, \"series\": [ { \"name\": \"demo\", \"columns\": [\"time\", \"field1\", \"field2\", \"tag1\", \"tag2\"], \"values\": [ [1679994647000, 90.0, 100.0, \"t1\", \"t2\"], [1679994647000, 90.0, 100.0, \"t11\", \"t22\"], [1679994648000, 91.0, 101.0, \"t1\", \"t2\"], [1679994648000, 91.0, 101.0, \"t11\", \"t22\"] ] } ] } ] } Usage in Grafana HoraeDB can be used as InfluxDB data source in Grafana.\nSelect InfluxDB type when add data source Then input http://{ip}:{5440}/influxdb/v1/ in HTTP URL. For local deployment, URL is http://localhost:5440/influxdb/v1/ Save \u0026 test Note Query string parameters such as epoch, db, pretty aren’t supported.\n","categories":"","description":"","excerpt":"InfluxDB is a time series database designed to handle high write and …","ref":"/docs/user-guide/ecosystem/influxdb/","tags":"","title":"InfluxDB"},{"body":"基础语法 写入数据的基础语法如下：\n1 2 3 INSERT [INTO] tbl_name [(col_name [, col_name] ...)] { {VALUES | VALUE} (value_list) [, (value_list)] ... } 写入一行数据的示例如下：\n1 INSERT INTO demo(`timestamp`, tag1) VALUES(1667374200022, 'horaedb') ","categories":"","description":"","excerpt":"基础语法 写入数据的基础语法如下：\n1 2 3 INSERT [INTO] tbl_name [(col_name [, col_name] …","ref":"/cn/docs/user-guide/sql/dml/insert/","tags":"","title":"INSERT"},{"body":"Basic syntax Basic syntax:\n1 2 3 INSERT [INTO] tbl_name [(col_name [, col_name] ...)] { {VALUES | VALUE} (value_list) [, (value_list)] ... } INSERT inserts new rows into a HoraeDB table. Here is an example:\n1 INSERT INTO demo(`time_stammp`, tag1) VALUES(1667374200022, 'horaedb') ","categories":"","description":"","excerpt":"Basic syntax Basic syntax:\n1 2 3 INSERT [INTO] tbl_name [(col_name [, …","ref":"/docs/user-guide/sql/dml/insert/","tags":"","title":"INSERT"},{"body":"Note: Some of the features mentioned in the article have not yet been implemented.\nOverview 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 ┌───────────────────────────────────────────────────────────────────────┐ │ │ │ HoraeMeta Cluster │ │ │ └───────────────────────────────────────────────────────────────────────┘ ▲ ▲ ▲ │ │ │ │ │ │ ▼ ▼ ▼ ┌───────┐Route Info ┌HoraeDB─────┬┬─┐ ┌HoraeDB─────┬┬─┐ ┌HoraeDB─────┬┬─┐ │client │◀────────▶ │ │ │TableN││ │ │ │ │TableN││ │ │ │ │TableN││ │ └───────┘Write/Query└──Shard(L)──┴┴─┘ └──Shard(F)──┴┴─┘ └──Shard(F)──┴┴─┘ ▲ │ ▲ ▲ │ │ │ │ Write─────────┐ ├────Sync───────┘ │ │ │ ┌────────┬▼───┴────┬──────────────────┐ Upload/Download │ │ │ │ SST │ │WAL │Region N │ │ │Service │ │ │ │ └────────┴─────────┴──────────────────┘ ▼ ┌───────────────────────────────────────────────────────────────────────┐ │ │ │ Object Storage │ │ │ └───────────────────────────────────────────────────────────────────────┘ The diagram above describes the architecture of a HoraeDB cluster, where some key concepts need to be explained:\nHoraeMeta Cluster: Takes responsibilities for managing the metadata and resource scheduling of the cluster; Shard(L)/Shard(F): Leader shard and follower shard consisting of multiple tables; HoraeDB: One HoraeDB instance consisting of multiple shards; WAL Service: Write-ahead log service for storing new-written real-time data; Object Storage: Object storage service for storing SST converted from memtable; From the architecture diagram above, it can be concluded that the compute and storage are separated in the HoraeDB cluster, which makes it easy to implement useful distributed features, such as elastic autoscaling of compute/storage resources, high availability, load balancing, and so on.\nLet’s dive into some of the key components mentioned above before explaining how these features are implemented.\nShard Shard is the basic scheduling unit in the cluster, which consists of a group of tables. And the tables in a shard share the same region for better storage locality in the WAL Service, and because of this, it is efficient to recover the data of all tables in the shard by scanning the entire WAL region. For most of implementations of WAL Service, without the shard concept, it costs a lot to recover the table data one by one due to massive random IO, and this case will deteriorate sharply when the number of tables grows to a certain level.\nA specific role, Leader or Follower, should be assigned to a shard. A pair of leader-follower shards share the same set of tables, and the leader shard can serve the write and query requests from the client while the follower shard can only serve the read-only requests, and must synchronize the newly written data from the WAL service in order to provide the latest snapshot for data retrieval. Actually, the follower is not needed if the high availability is not required, while with at least one follower, it takes only a short time to resume service by simply switching the Follower to Leader when the HoraeDB instance on which the leader shard exists crashes.\nThe diagram below concludes the relationship between HoraeDB instance, Shard, Table. As shown in the diagram, the leader and follower shards are interleaved on the HoraeDB instance.\n1 2 3 4 5 6 7 8 9 10 11 12 13 ┌─HoraeDB Instance0──────┐ ┌─HoraeDB Instance1──────┐ │ ┌─Shard0(L)────────┐ │ │ ┌─Shard0(F)────────┐ │ │ │ ┌────┬────┬────┐ │ │ │ │ ┌────┬────┬────┐ │ │ │ │ │ T0 │ T1 │ T2 │ │ │ │ │ │ T0 │ T1 │ T2 │ │ │ │ │ └────┴────┴────┘ │ │ │ │ └────┴────┴────┘ │ │ │ └──────────────────┘ │ │ └──────────────────┘ │ │ │ │ │ │ ┌─Shard1(F)────────┐ │ │ ┌─Shard1(L)────────┐ │ │ │ ┌────┬────┬────┐ │ │ │ │ ┌────┬────┬────┐ │ │ │ │ │ T0 │ T1 │ T2 │ │ │ │ │ │ T0 │ T1 │ T2 │ │ │ │ │ └────┴────┴────┘ │ │ │ │ └────┴────┴────┘ │ │ │ └──────────────────┘ │ │ └──────────────────┘ │ └────────────────────────┘ └────────────────────────┘ Since Shard is the basic scheduling unit, it is natural to introduce some basic shard operations:\nCreate/Drop table to/from a shard; Open/Close a shard; Split one shard into two shards; Merge two shards into one shard; Switch the role of a shard; With these basic shard operations, some complex scheduling logic can be implemented, e.g. perform an expansion by splitting one shard into two shards and migrating one of them to the new HoraeDB instance.\nHoraeMeta HoraeMeta is implemented by embedding an ETCD inside to ensure consistency and takes responsibilities for cluster metadata management and scheduling.\nThe cluster metadata includes:\nTable information, such as table name, table ID, and which cluster the table belongs to; The mapping between table and shard and between shard and HoraeDB instance; … As for the cluster scheduling work, it mainly includes:\nReceiving the heartbeats from the HoraeDB instances and determining the online status of these registered instances; Assigning specific role shards to the registered HoraeDB instances; Participating in table creation by assigning a unique table ID and the most appropriate shard to the table; Performing load balancing through shard operations according to the load information sent with the heartbeats; Performing expansion through shard operations when new instances are registered; Initiating failover through shard operations when old instances go offline; Route In order to avoid the overhead of forwarding requests, the communication between clients and the HoraeDB instances is peer-to-peer, that is to say, the client should retrieve routing information from the server before sending any specific write/query requests.\nActually, the routing information is decided by the HoraeMeta, but clients are only allowed the access to it through the HoraeDB instances rather than HoraeMeta, to avoid potential performance issues on the HoraeMeta.\nWAL Service \u0026 Object Storage In the HoraeDB cluster, WAL Service and Object Storage exist as separate distributed systems featured with HA, data replication and scalability. Current distributed implementations for WAL Service includes Kafka and OBKV (access OceanBase by its table api), and the implementations for Object Storage include popular object storage services, such as AWS S3, Azure object storage and Aliyun OSS.\nThe two components are similar in that they are introduced to serve as the underlying storage layer for separating compute and storage, while the difference between two components is obvious that WAL Service is used to store the newly written data from the real-time write requests whose individual size is small but quantity is large, and Object Storage is used to store the read-friendly data files (SST) organized in the background, whose individual size is large and aggregate size is much larger.\nThe two components make it much easier to implement the horaedb cluster, which features horizontal scalability, high availability and load balancing.\nScalability Scalability is an important feature for a distributed system. Let’s take a look at to how the horizontal scalability of the HoraeDB cluster is achieved.\nFirst, the two storage components (WAL Service and Object Storage) should be horizontally scalable when deciding on the actual implementations for them, so the two storage services can be expanded separately if the storage capacity is not sufficient.\nIt will be a little bit complex when discussing the scalability of the compute service. Basically, these cases will bring the capacity problem:\nMassive queries on massive tables; Massive queries on a single large table; Massive queries on a normal table; For the first case, it is easy to achieve horizontal scalability just by assigning shards that are created or split from old shards to expanded HoraeDB instances.\nFor the second case, the table partitioning is proposed and after partitioning, massive queries are distributed across multiple HoraeDB instances.\nAnd the last case is the most important and the most difficult. Actually, the follower shard can handle part of the queries, but the number of follower shards is limited by the throughput threshold of the synchronization from the WAL regions. As shown in the diagram below, a pure compute shard can be introduced if the followers are not enough to handle the massive queries. Such a shard is not required to synchronize data with the leader shard, and retrieves the newly written data from the leader/follower shard only when the query comes. As for the SSTs required by the query, they will be downloaded from Object Storage and cached afterwards. With the two parts of the data, the compute resources are fully utilized to execute the CPU-intensive query plan. As we can see, such a shard can be added with only a little overhead (retrieving some data from the leader/follower shard when it needs), so to some extent, the horizontal scalability is achieved.\n1 2 3 4 5 6 7 8 9 ┌HoraeDB─────┬┬─┐ ┌──newly written─│ │ │TableN││ │ ▼ └──Shard(L/F)┴┴─┘ ┌───────┐ Query ┌HoraeDB─────┬┬─┐ │client │────────▶│ │ │TableN││ │ └───────┘ └──Shard─────┴┴─┘ ┌───────────────┐ ▲ │ Object │ └───old SST──────│ Storage │ └───────────────┘ High Availability Assuming that WAL service and Object Storage are highly available, the high availability of the HoraeDB cluster can be achieved by such a procedure:\nWhen detecting that the heartbeat is broken, HoraeMeta determines that the HoraeDB instance is offline; The follower shards whose paired leader shards exist on the offline instance are switched to leader shards for fast failover; A slow failover can be achieved by opening the crashed shards on another instance if such follower shards don’t exist. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 ┌─────────────────────────────────────────────────────────┐ │ │ │ HoraeMeta Cluster │ │ │ └─────────────────────────────────────────────────────────┘ ▲ ┌ ─ Heartbeat ─ ┤ Broken │ │ │ ┌ HoraeDB Instance0 ─ ─ ─ │ ┌─HoraeDB Instance1──────┐ ┌─HoraeDB Instance1──────┐ ┌─Shard0(L)────────┐ │ │ │ ┌─Shard0(F)────────┐ │ │ ┌─Shard0(L)────────┐ │ │ │ ┌────┬────┬────┐ │ │ │ │ ┌────┬────┬────┐ │ │ │ │ ┌────┬────┬────┐ │ │ │ │ T0 │ T1 │ T2 │ │ │ ├───│ │ │ T0 │ T1 │ T2 │ │ │ │ │ │ T0 │ T1 │ T2 │ │ │ │ │ └────┴────┴────┘ │ │ │ │ └────┴────┴────┘ │ │ │ │ └────┴────┴────┘ │ │ └──────────────────┘ │ │ │ └──────────────────┘ │ Failover │ └──────────────────┘ │ │ │ ├─HoraeDB Instance2──────┤ ───────────▶ ├─HoraeDB Instance2──────┤ ┌─Shard1(L)────────┐ │ │ │ ┌─Shard1(F)────────┐ │ │ ┌─Shard1(L)────────┐ │ │ │ ┌────┬────┬────┐ │ │ │ │ ┌────┬────┬────┐ │ │ │ │ ┌────┬────┬────┐ │ │ │ │ T0 │ T1 │ T2 │ │ │ └───│ │ │ T0 │ T1 │ T2 │ │ │ │ │ │ T0 │ T1 │ T2 │ │ │ │ │ └────┴────┴────┘ │ │ │ └────┴────┴────┘ │ │ │ │ └────┴────┴────┘ │ │ └──────────────────┘ │ │ └──────────────────┘ │ │ └──────────────────┘ │ └ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ └────────────────────────┘ └────────────────────────┘ Load Balancing HoraeMeta collects the instance load information contained in the received heartbeats to create a load overview of the whole cluster, according to which the load balancing can be implemented as an automatic mechanism:\nPick a shard on a low-load instance for the newly created table; Migrate a shard from a high-load instance load to another low-load instance; Split the large shard on the high-load instance and migrate the split shards to other low-load instances; ","categories":"","description":"","excerpt":"Note: Some of the features mentioned in the article have not yet been …","ref":"/docs/design/clustering/","tags":"","title":"Introduction to Architecture of HoraeDB Cluster"},{"body":"Target Provide the overview of HoraeDB to the developers who want to know more about HoraeDB but have no idea where to start. Make a brief introduction to the important modules of HoraeDB and the connections between these modules but details about their implementations are not be involved. Motivation HoraeDB is a timeseries database (TSDB). However, HoraeDB’s goal is to handle both timeseries and analytic workloads compared with the classic TSDB, which usually have a poor performance in handling analytic workloads.\nIn the classic timeseries database, the Tag columns (InfluxDB calls them Tag and Prometheus calls them Label) are normally indexed by generating an inverted index. However, it is found that the cardinality of Tag varies in different scenarios. And in some scenarios the cardinality of Tag is very high (we name this case after analytic workload), and it takes a very high cost to store and retrieve the inverted index. On the other hand, it is observed that scanning+pruning often used by the analytical databases can do a good job to handle such analytic workload.\nThe basic design idea of HoraeDB is to adopt a hybrid storage format and the corresponding query method for a better performance in processing both timeseries and analytic workloads.\nArchitecture 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 ┌──────────────────────────────────────────┐ │ RPC Layer (HTTP/gRPC/MySQL) │ └──────────────────────────────────────────┘ ┌──────────────────────────────────────────┐ │ SQL Layer │ │ ┌─────────────────┐ ┌─────────────────┐ │ │ │ Parser │ │ Planner │ │ │ └─────────────────┘ └─────────────────┘ │ └──────────────────────────────────────────┘ ┌───────────────────┐ ┌───────────────────┐ │ Interpreter │ │ Catalog │ └───────────────────┘ └───────────────────┘ ┌──────────────────────────────────────────┐ │ Query Engine │ │ ┌─────────────────┐ ┌─────────────────┐ │ │ │ Optimizer │ │ Executor │ │ │ └─────────────────┘ └─────────────────┘ │ └──────────────────────────────────────────┘ ┌──────────────────────────────────────────┐ │ Pluggable Table Engine │ │ ┌────────────────────────────────────┐ │ │ │ Analytic │ │ │ │┌────────────────┐┌────────────────┐│ │ │ ││ Wal ││ Memtable ││ │ │ │└────────────────┘└────────────────┘│ │ │ │┌────────────────┐┌────────────────┐│ │ │ ││ Flush ││ Compaction ││ │ │ │└────────────────┘└────────────────┘│ │ │ │┌────────────────┐┌────────────────┐│ │ │ ││ Manifest ││ Object Store ││ │ │ │└────────────────┘└────────────────┘│ │ │ └────────────────────────────────────┘ │ │ ┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ │ │ Another Table Engine │ │ │ └ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ │ └──────────────────────────────────────────┘ The figure above shows the architecture of HoraeDB stand-alone service and the details of some important modules will be described in the following part.\nRPC Layer module path: https://github.com/apache/incubator-horaedb/tree/main/server\nThe current RPC supports multiple protocols including HTTP, gRPC, MySQL.\nBasically, HTTP and MySQL are used to debug HoraeDB, query manually and perform DDL operations (such as creating, deleting tables, etc.). And gRPC protocol can be regarded as a customized protocol for high-performance, which is suitable for massive reading and writing operations.\nSQL Layer module path: https://github.com/apache/incubator-horaedb/tree/main/query_frontend\nSQL layer takes responsibilities for parsing sql and generating the query plan.\nBased on sqlparser a sql dialect, which introduces some key concepts including Tag and Timestamp, is provided for processing timeseries data. And by utilizing DataFusion the planner is able to generate both regular logical plans and tailored ones which is used to implement the special operators defined by timeseries queries, e.g PromQL.\nInterpreter module path: https://github.com/apache/incubator-horaedb/tree/main/interpreters\nThe Interpreter module encapsulates the SQL CRUD operations. In the query procedure, a sql received by HoraeDB is parsed, converted into the query plan and then executed in some specific interpreter, such as SelectInterpreter, InsertInterpreter and etc.\nCatalog module path: https://github.com/apache/incubator-horaedb/tree/main/catalog_impls\nCatalog is actually the module managing metadata and the levels of metadata adopted by HoraeDB is similar to PostgreSQL: Catalog \u003e Schema \u003e Table, but they are only used as namespace.\nAt present, Catalog and Schema have two different kinds of implementation for standalone and distributed mode because some strategies to generate ids and ways to persist metadata differ in different mode.\nQuery Engine module path: https://github.com/apache/incubator-horaedb/tree/main/query_engine\nQuery Engine is responsible for optimizing and executing query plan given a basic SQL plan provided by SQL layer and now such work is mainly delegated to DataFusion.\nIn addition to the basic functions of SQL, HoraeDB also defines some customized query protocols and optimization rules for some specific query plans by utilizing the extensibility provided by DataFusion. For example, the implementation of PromQL is implemented in this way and read it if you are interested.\nPluggable Table Engine module path: https://github.com/apache/incubator-horaedb/tree/main/table_engine\nTable Engine is actually a storage engine for managing tables in HoraeDB and the pluggability of Table Engine is a core design of HoraeDB which matters in achieving our long-term target, e.g supporting handle log or tracing workload by implementing new storage engines. HoraeDB will have multiple kinds of Table Engine for different workloads and the most appropriate one should be chosen as the storage engine according to the workload pattern.\nNow the requirements for a Table Engine are:\nManage all the shared resources under the engine: Memory Storage CPU Manage metadata of tables such as table schema and table options; Provide Table instances which provides read and write methods; Take responsibilities for creating, opening, dropping and closing Table instance; …. Actually the things that a Table Engine needs to process are a little complicated. And now in HoraeDB only one Table Engine called Analytic is provided and does a good job in processing analytical workload, but it is not ready yet to handle the timeseries workload (we plan to enhance it for a better performance by adding some indexes which help handle timeseries workload).\nThe following part gives a description about details of Analytic Table Engine.\nWAL module path: https://github.com/apache/incubator-horaedb/tree/main/wal\nThe model of HoraeDB processing data is WAL + MemTable that the recent written data is written to WAL first and then to MemTable and after a certain amount of data in MemTable is accumulated, the data will be organized in a query-friendly form to persistent devices.\nNow three implementations of WAL are provided for standalone and distributed mode:\nFor standalone mode, WAL is based on RocksDB and data is persisted on the local disk. For distributed mode, WAL is required as a distributed component and to be responsible for durability of the newly written data, so now we provide an implementation based on OceanBase. For distributed mode, in addition to OceanBase, we also provide a more lightweight implementation based on Apache Kafka. MemTable module path: https://github.com/apache/incubator-horaedb/tree/main/analytic_engine/src/memtable\nFor WAL can’t provide efficient data retrieval, the newly written data is also stored in Memtable for efficient data retrieval, after a certain amount of data is reached, HoraeDB organizes the data in MemTable into a query-friendly storage format (SST) and stores it to the persistent device.\nThe current implementation of MemTable is based on agatedb’s skiplist. It allows concurrent reads and writes and can control memory usage based on Arena.\nFlush module path: https://github.com/apache/incubator-horaedb/blob/main/analytic_engine/src/instance/flush_compaction.rs\nWhat Flush does is that when the memory usage of MemTable reaches the threshold, some MemTables are selected for flushing into query-friendly SSTs saved on persistent device.\nDuring the flushing procedure, the data will be divided by a certain time range (which is configured by table option Segment Duration), and any SST is ensured that the timestamps of the data in it are in the same Segment. Actually this is also a common operation in most timeseries databases which organizes data in the time dimension to speed up subsequent time-related operations, such as querying data over a time range and assisting purge data outside the TTL.\nCompaction module path: https://github.com/apache/incubator-horaedb/tree/main/analytic_engine/src/compaction\nThe data of MemTable is flushed as SSTs, but the file size of recently flushed SST may be very small. And too small or too many SSTs lead to the poor query performance. Therefore, Compaction is then introduced to rearrange the SSTs so that the multiple smaller SST files can be compacted into a larger SST file.\nManifest module path: https://github.com/apache/incubator-horaedb/tree/main/analytic_engine/src/meta\nManifest takes responsibilities for managing tables’ metadata of Analytic Engine including:\nTable schema and table options; The sequence number where the newest flush finishes; The information of all the SSTs belonging to the table. Now the Manifest is based on WAL and Object Storage. The newly written updates on the Manifest are persisted as logs in WAL, and in order to avoid infinite expansion of Manifest (actually every Flush leads to an update), Snapshot is also introduced to clean up the history of metadata updates, and the generated Snapshot will be saved to Object Storage.\nObject Storage module path: https://github.com/apache/incubator-horaedb/tree/main/components/object_store\nThe SST generated by Flush needs to be persisted and the abstraction of the persistent storage device is ObjectStore including multiple implementations:\nBased on local file system; Based on Alibaba Cloud OSS. The distributed architecture of HoraeDB separates storage and computing, which requires Object Store needs to be a highly available and reliable service independent of HoraeDB. Therefore, storage systems like Amazon S3, Alibaba Cloud OSS is a good choice and in the future implementations on storage systems of some other cloud service providers is planned to provide.\nSST module path: https://github.com/apache/incubator-horaedb/tree/main/analytic_engine/src/sst\nSST is actually an abstraction that can have multiple specific implementations. The current implementation is based on Parquet, which is a column-oriented data file format designed for efficient data storage and retrieval.\nThe format of SST is very critical for retrieving data and is also the most important part to perform well in handling both timeseries and analytic workloads. At present, our Parquet-based implementation is good at processing analytic workload but is poor at processing timeseries workload. In our roadmap, we will explore more storage formats in order to achieve a good performance in both workloads.\nSpace module path: https://github.com/apache/incubator-horaedb/blob/main/analytic_engine/src/space.rs\nIn Analytic Engine, there is a concept called space and here is an explanation for it to resolve some ambiguities when read source code. Actually Analytic Engine does not have the concept of catalog and schema and only provides two levels of relationship: space and table. And in the implementation, the schema id (which should be unique across all catalogs) on the upper layer is actually mapped to space id.\nThe space in Analytic Engine serves mainly for isolation of resources for different tenants, such as the usage of memory.\nCritical Path After a brief introduction to some important modules of HoraeDB, we will give a description for some critical paths in code, hoping to provide interested developers with a guide for reading the code.\nQuery 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 ┌───────┐ ┌───────┐ ┌───────┐ │ │──1──▶│ │──2──▶│ │ │Server │ │ SQL │ │Catalog│ │ │◀─10──│ │◀─3───│ │ └───────┘ └───────┘ └───────┘ │ ▲ 4│ 9│ │ │ ▼ │ ┌─────────────────────────────────────┐ │ │ │ Interpreter │ │ │ └─────────────────────────────────────┘ │ ▲ 5│ 8│ │ │ ▼ │ ┌──────────────────┐ │ │ │ Query Engine │ │ │ └──────────────────┘ │ ▲ 6│ 7│ │ │ ▼ │ ┌─────────────────────────────────────┐ │ │ │ Table Engine │ │ │ └─────────────────────────────────────┘ Take SELECT SQL as an example. The figure above shows the query procedure and the numbers in it indicates the order of calling between the modules.\nHere are the details:\nServer module chooses a proper rpc module (it may be HTTP, gRPC or mysql) to process the requests according the protocol used by the requests; Parse SQL in the request by the parser; With the parsed sql and the information provided by catalog/schema module, DataFusion can generate the logical plan; With the logical plan, the corresponding Interpreter is created and logical plan will be executed by it; For the logical plan of normal Select SQL, it will be executed through SelectInterpreter; In the SelectInterpreter the specific query logic is executed by the Query Engine: Optimize the logical plan; Generate the physical plan; Optimize the physical plan; Execute the physical plan; The execution of physical plan involves Analytic Engine: Data is obtained by read method of Table instance provided by Analytic Engine; The source of the table data is SST and Memtable, and the data can be filtered by the pushed down predicates; After retrieving the table data, Query Engine will complete the specific computation and generate the final results; SelectInterpreter gets the results and feeds them to the protocol module; After the protocol layer converts the results, the server module responds to the client with them. The following is the flow of function calls in version v1.2.2:\n┌───────────────────────◀─────────────┐ ┌───────────────────────┐ │ handle_sql │────────┐ │ │ parse_sql │ └───────────────────────┘ │ │ └────────────────┬──────┘ │ ▲ │ │ ▲ │ │ │ │ │ │ │ │ │ │ └36───┐ │ 11 1│ │ │ │ │ │ │ 8│ │ │ │ │ │ │ │ │ 10 │ │ │ │ │ │ │ ▼ │ │ │ │ ▼ ┌─────────────────┴─────┐ 9│ ┌┴─────┴────────────────┐───────12─────────▶┌───────────────────────┐ │maybe_forward_sql_query│ └────────▶│fetch_sql_query_output │ │ statement_to_plan │ └───┬───────────────────┘ └────┬──────────────────┘◀───────19─────────└───────────────────────┘ │ ▲ │ ▲ │ ▲ │ │ │ │ │ │ │ │ │ │ │ │ │ │ │ 35 13 18 2│ 7│ 20 │ │ │ │ │ │ │ │ │ │ │ │ │ │ │ │ │ │ │ ▼ │ ▼ │ ▼ │ ┌───────────────────────┐ ┌───────────────────────┐───────────6───────▶┌─────────────────┴─────┐ ┌─────────────────┴─────┐ │Planner::statement_to_p│ │ forward_with_endpoint │ │ forward │ │execute_plan_involving_│ │ lan │ └───────────────────────┘◀────────5──────────└───┬───────────────────┘ ┌──│ partition_table │◀────────┐ └───┬───────────────────┘ │ ▲ │ └───────────────────────┘ │ │ ▲ │ │ │ │ ▲ │ │ │ │ │ │ │ │ │ 14 17 ┌───────────────────────┐ │ 4│ │ │ │ │ │ │ ┌─────│ PhysicalPlan::execute │ 3│ │ │ 21 │ │ │ │ │ └───────────────────────┘◀──┐ │ │ │ │ 22 │ │ │ │ │ │ │ │ │ │ │ ▼ │ │ │ │ │ │ │ │ │ ┌────────────────────────┐ │ │ ▼ │ │ ▼ │ 34 │sql_statement_to_datafus│ │ ┌───────────────────────┐ 30 ┌─────────────────┴─────┐ │ ┌─────────────────┴─────┐ │ │ ion_plan │ 31 │ build_df_session_ctx │ │ │ route │ │ │ build_interpreter │ │ └────────────────────────┘ │ └────┬──────────────────┘ │ └───────────────────────┘ │ └───────────────────────┘ │ │ ▲ │ │ ▲ │ │ │ │ │ │ 27 26 │ 23 │ 15 16 │ ▼ │ │ │ │ │ │ └────▶┌────────────────┴──────┐ │ ┌───────────────────────┐ │ │ │ │ │ execute_logical_plan ├───┴────32────────▶│ execute │──────────┐ │ ┌───────────────────────┐ │ ▼ │ └────┬──────────────────┘◀────────────25────┴───────────────────────┘ 33 │ │interpreter_execute_pla│ │ ┌────────────────────────┐ │ ▲ ▲ └──────┴──▶│ n │────────┘ │SqlToRel::sql_statement_│ 28 │ └──────────24────────────────┴───────────────────────┘ │ to_datafusion_plan │ │ 29 └────────────────────────┘ ▼ │ ┌────────────────┴──────┐ │ optimize_plan │ └───────────────────────┘ The received request will be forwarded to handle_sql after various protocol conversions, and since the request may not be processed by this node, it may need to be forwarded to maybe_forward_sql_query to handle the forwarding logic. After constructing the ForwardRequest in maybe_forward_sql_query, call forward After constructing the RouteRequest in forward, call route Use route to get the destination node endpoint and return to forward. Call forward_with_endpoint to forward the request return forward return maybe_forward_sql_query return handle_sql If this is a Local request, call fetch_sql_query_output to process it Call parse_sql to parse sql into Statment return fetch_sql_query_output Call statement_to_plan with Statment Construct Planner with ctx and Statment, and call the statement_to_plan method of Planner The planner will call the corresponding planner method for the requested category, at this point our sql is a query and will call sql_statement_to_plan Call sql_statement_to_datafusion_plan , which will generate the datafusion object, and then call SqlToRel::sql_statement_to_plan The generated logical plan is returned from SqlToRel::sql_statement_to_plan return return return Call execute_plan_involving_partition_table (in the default configuration) for subsequent optimization and execution of this logical plan Call build_interpreter to generate Interpreter return Call Interpreter's interpreter_execute_plan method for logical plan execution. The corresponding execute function is called, at this time the sql is a query, so the execute of the SelectInterpreter will be called call execute_logical_plan , which will call build_df_session_ctx to generate the optimizer build_df_session_ctx will use the config information to generate the corresponding context, first using datafusion and some custom optimization rules (in logical_optimize_rules()) to generate the logical plan optimizer, using apply_adapters_for_physical_optimize_rules to generate the physical plan optimizer return optimizer Call optimize_plan, using the optimizer just generated to first optimize the logical plan and then the physical plan Return to optimized physical plan execute physical plan returned after execution After collecting the results of all slices, return return return return Return to the upper layer for network protocol conversion and finally return to the request sender Write 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 ┌───────┐ ┌───────┐ ┌───────┐ │ │──1──▶│ │──2──▶│ │ │Server │ │ SQL │ │Catalog│ │ │◀─8───│ │◀─3───│ │ └───────┘ └───────┘ └───────┘ │ ▲ 4│ 7│ │ │ ▼ │ ┌─────────────────────────────────────┐ │ │ │ Interpreter │ │ │ └─────────────────────────────────────┘ │ ▲ │ │ │ │ │ │ │ │ ┌──────────────────┐ │ │ │ │ 5│ 6│ │ Query Engine │ │ │ │ │ │ │ └──────────────────┘ │ │ │ │ │ │ ▼ │ ┌─────────────────────────────────────┐ │ │ │ Table Engine │ │ │ └─────────────────────────────────────┘ Take INSERT SQL as an example. The figure above shows the query procedure and the numbers in it indicates the order of calling between the modules.\nHere are the details:\nServer module chooses a proper rpc module (it may be HTTP, gRPC or mysql) to process the requests according the protocol used by the requests; Parse SQL in the request by the parser; With the parsed sql and the catalog/schema module, DataFusion can generate the logical plan; With the logical plan, the corresponding Interpreter is created and logical plan will be executed by it; For the logical plan of normal INSERT SQL, it will be executed through InsertInterpreter; In the InsertInterpreter, write method of Table provided Analytic Engine is called: Write the data into WAL first; Write the data into MemTable then; Before writing to MemTable, the memory usage will be checked. If the memory usage is too high, the flush process will be triggered: Persist some old MemTables as SSTs; Store updates about the new SSTs and the flushed sequence number of WAL to Manifest; Delete the corresponding WAL entries; Server module responds to the client with the execution result. ","categories":"","description":"","excerpt":"Target Provide the overview of HoraeDB to the developers who want to …","ref":"/docs/design/architecture/","tags":"","title":"Introduction to HoraeDB's Architecture"},{"body":"介绍 HoraeDBClient 是 HoraeDB 的高性能 Java 版客户端。\n环境要求 Java 8 及以上\n依赖 1 2 3 4 5 \u003cdependency\u003e \u003cgroupId\u003eio.ceresdb\u003c/groupId\u003e \u003cartifactId\u003eceresdb-all\u003c/artifactId\u003e \u003cversion\u003e${CERESDB.VERSION}\u003c/version\u003e \u003c/dependency\u003e 最新的版本可以从这里获取。\n初始化客户端 1 2 3 4 5 6 7 8 9 10 11 // CeresDB options final CeresDBOptions opts = CeresDBOptions.newBuilder(\"127.0.0.1\", 8831, DIRECT) // 默认 gprc 端口号，DIRECT 模式 .database(\"public\") // Client所使用的database，可被RequestContext的database覆盖 .writeMaxRetries(1) // 写入失败重试次数上限（只有部分错误 code 才会重试，比如路由表失效） .readMaxRetries(1) // 查询失败重试次数上限（只有部分错误 code 才会重试，比如路由表失效） .build(); final CeresDBClient client = new CeresDBClient(); if (!client.init(opts)) { throw new IllegalStateException(\"Fail to start CeresDBClient\"); } 客户端初始化至少需要三个参数：\nEndPoint： 127.0.0.1 Port： 8831 RouteMode： DIRECT/PROXY 这里重点解释下 RouteMode 参数，PROXY 模式用在客户端和服务端存在网络隔离，请求需要经过转发的场景；DIRECT 模式用在客户端和服务端网络连通的场景，节省转发的开销，具有更高的性能。 更多的参数配置详情见 configuration。\n注意： HoraeDB 当前仅支持默认的 public database , 未来会支持多个 database。\n建表 为了方便使用，在使用 gRPC 的 write 接口进行写入时，如果某个表不存在，HoraeDB 会根据第一次的写入自动创建一个表。\n当然你也可以通过 create table 语句来更精细化的管理的表（比如添加索引等）。\n下面的建表语句（使用 SDK 的 SQL API）包含了 HoraeDB 支持的所有字段类型：\n1 2 3 4 5 6 7 8 9 10 11 12 13 String createTableSql = \"CREATE TABLE IF NOT EXISTS machine_table(\" + \"ts TIMESTAMP NOT NULL,\" + \"city STRING TAG NOT NULL,\" + \"ip STRING TAG NOT NULL,\" + \"cpu DOUBLE NULL,\" + \"mem DOUBLE NULL,\" + \"TIMESTAMP KEY(ts)\" + // 建表时必须指定时间戳序列 \") ENGINE=Analytic\"; Result\u003cSqlQueryOk, Err\u003e createResult = client.sqlQuery(new SqlQueryRequest(createTableSql)).get(); if (!createResult.isOk()) { throw new IllegalStateException(\"Fail to create table\"); } 删表 下面是一个删表的示例：\n1 2 3 4 5 6 String dropTableSql = \"DROP TABLE machine_table\"; Result\u003cSqlQueryOk, Err\u003e dropResult = client.sqlQuery(new SqlQueryRequest(dropTableSql)).get(); if (!dropResult.isOk()) { throw new IllegalStateException(\"Fail to drop table\"); } 数据写入 首先我们需要构建数据，示例如下：\n1 2 3 4 5 6 7 8 9 10 11 12 List\u003cPoint\u003e pointList = new LinkedList\u003c\u003e(); for (int i = 0; i \u003c 100; i++) { // 构建单个Point final Point point = Point.newPointBuilder(\"machine_table\") .setTimestamp(t0) .addTag(\"city\", \"Singapore\") .addTag(\"ip\", \"10.0.0.1\") .addField(\"cpu\", Value.withDouble(0.23)) .addField(\"mem\", Value.withDouble(0.55)) .build(); points.add(point); } 然后使用 write 接口写入数据，示例如下：\n1 2 3 4 5 6 7 8 9 10 final CompletableFuture\u003cResult\u003cWriteOk, Err\u003e\u003e wf = client.write(pointList); // 这里用 `future.get` 只是方便演示，推荐借助 CompletableFuture 强大的 API 实现异步编程 final Result\u003cWriteOk, Err\u003e writeResult = wf.get(); Assert.assertTrue(writeResult.isOk()); Assert.assertEquals(3, writeResult.getOk().getSuccess()); // `Result` 类参考了 Rust 语言，提供了丰富的 mapXXX、andThen 类 function 方便对结果值进行转换，提高编程效率，欢迎参考 API 文档使用 Assert.assertEquals(3, writeResult.mapOr(0, WriteOk::getSuccess).intValue()); Assert.assertEquals(0, writeResult.getOk().getFailed()); Assert.assertEquals(0, writeResult.mapOr(-1, WriteOk::getFailed).intValue()); 详情见 write\n数据查询 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 final SqlQueryRequest queryRequest = SqlQueryRequest.newBuilder() .forTables(\"machine_table\") // 这里表名是可选的，如果未提供，SDK将自动解析SQL填充表名并自动路由 .sql(\"select * from machine_table where ts = %d\", t0) // .build(); final CompletableFuture\u003cResult\u003cSqlQueryOk, Err\u003e\u003e qf = client.sqlQuery(queryRequest); // 这里用 `future.get` 只是方便演示，推荐借助 CompletableFuture 强大的 API 实现异步编程 final Result\u003cSqlQueryOk, Err\u003e queryResult = qf.get(); Assert.assertTrue(queryResult.isOk()); final SqlQueryOk queryOk = queryResult.getOk(); Assert.assertEquals(1, queryOk.getRowCount()); // 直接获取结果数组 final List\u003cRow\u003e rows = queryOk.getRowList(); Assert.assertEquals(t0, rows.get(0).getColumn(\"ts\").getValue().getTimestamp()); Assert.assertEquals(\"Singapore\", rows.get(0).getColumn(\"city\").getValue().getString()); Assert.assertEquals(\"10.0.0.1\", rows.get(0).getColumn(\"ip\").getValue().getString()); Assert.assertEquals(0.23, rows.get(0).getColumn(\"cpu\").getValue().getDouble(), 0.0000001); Assert.assertEquals(0.55, rows.get(0).getColumn(\"mem\").getValue().getDouble(), 0.0000001); // 获取结果流 final Stream\u003cRow\u003e rowStream = queryOk.stream(); rowStream.forEach(row -\u003e System.out.println(row.toString())); 详情见 read\n流式读写 HoraeDB 支持流式读写，适用于大规模数据读写。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 long start = System.currentTimeMillis(); long t = start; final StreamWriteBuf\u003cPoint, WriteOk\u003e writeBuf = client.streamWrite(\"machine_table\"); for (int i = 0; i \u003c 1000; i++) { final Point streamData = Point.newPointBuilder(\"machine_table\") .setTimestamp(t) .addTag(\"city\", \"Beijing\") .addTag(\"ip\", \"10.0.0.3\") .addField(\"cpu\", Value.withDouble(0.42)) .addField(\"mem\", Value.withDouble(0.67)) .build(); writeBuf.writeAndFlush(Collections.singletonList(streamData)); t = t+1; } final CompletableFuture\u003cWriteOk\u003e writeOk = writeBuf.completed(); Assert.assertEquals(1000, writeOk.join().getSuccess()); final SqlQueryRequest streamQuerySql = SqlQueryRequest.newBuilder() .sql(\"select * from %s where city = '%s' and ts \u003e= %d and ts \u003c %d\", \"machine_table\", \"Beijing\", start, t).build(); final Result\u003cSqlQueryOk, Err\u003e streamQueryResult = client.sqlQuery(streamQuerySql).get(); Assert.assertTrue(streamQueryResult.isOk()); Assert.assertEquals(1000, streamQueryResult.getOk().getRowCount()); 详情见 streaming\n","categories":"","description":"","excerpt":"介绍 HoraeDBClient 是 HoraeDB 的高性能 Java 版客户端。\n环境要求 Java 8 及以上\n依赖 1 2 3 4 …","ref":"/cn/docs/user-guide/sdk/java/","tags":"","title":"Java"},{"body":"Introduction HoraeDB Client is a high-performance Java client for HoraeDB.\nRequirements Java 8 or later is required for compilation Dependency 1 2 3 4 5 \u003cdependency\u003e \u003cgroupId\u003eio.ceresdb\u003c/groupId\u003e \u003cartifactId\u003eceresdb-all\u003c/artifactId\u003e \u003cversion\u003e${CERESDB.VERSION}\u003c/version\u003e \u003c/dependency\u003e You can get latest version here.\nInit HoraeDB Client 1 2 3 4 5 6 7 8 9 10 11 12 13 final CeresDBOptions opts = CeresDBOptions.newBuilder(\"127.0.0.1\", 8831, DIRECT) // CeresDB default grpc port 8831，use DIRECT RouteMode .database(\"public\") // use database for client, can be overridden by the RequestContext in request // maximum retry times when write fails // (only some error codes will be retried, such as the routing table failure) .writeMaxRetries(1) // maximum retry times when read fails // (only some error codes will be retried, such as the routing table failure) .readMaxRetries(1).build(); final CeresDBClient client = new CeresDBClient(); if (!client.init(opts)) { throw new IllegalStateException(\"Fail to start CeresDBClient\"); } The initialization requires at least three parameters:\nEndpoint: 127.0.0.1 Port: 8831 RouteMode: DIRECT/PROXY Here is the explanation of RouteMode. There are two kinds of RouteMode,The Direct mode should be adopted to avoid forwarding overhead if all the servers are accessible to the client. However, the Proxy mode is the only choice if the access to the servers from the client must go through a gateway. For more configuration options, see configuration\nNotice: HoraeDB currently only supports the default database public now, multiple databases will be supported in the future;\nCreate Table Example For ease of use, when using gRPC’s write interface for writing, if a table does not exist, HoraeDB will automatically create a table based on the first write.\nOf course, you can also use create table statement to manage the table more finely (such as adding indexes).\nThe following table creation statement（using the SQL API included in SDK ）shows all field types supported by HoraeDB：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 // Create table manually, creating table schema ahead of data ingestion is not required String createTableSql = \"CREATE TABLE IF NOT EXISTS machine_table(\" + \"ts TIMESTAMP NOT NULL,\" + \"city STRING TAG NOT NULL,\" + \"ip STRING TAG NOT NULL,\" + \"cpu DOUBLE NULL,\" + \"mem DOUBLE NULL,\" + \"TIMESTAMP KEY(ts)\" + // timestamp column must be specified \") ENGINE=Analytic\"; Result\u003cSqlQueryOk, Err\u003e createResult = client.sqlQuery(new SqlQueryRequest(createTableSql)).get(); if (!createResult.isOk()) { throw new IllegalStateException(\"Fail to create table\"); } Drop Table Example Here is an example of dropping table：\n1 2 3 4 5 6 String dropTableSql = \"DROP TABLE machine_table\"; Result\u003cSqlQueryOk, Err\u003e dropResult = client.sqlQuery(new SqlQueryRequest(dropTableSql)).get(); if (!dropResult.isOk()) { throw new IllegalStateException(\"Fail to drop table\"); } Write Data Example Firstly, you can use PointBuilder to build HoraeDB points:\n1 2 3 4 5 6 7 8 9 10 11 12 List\u003cPoint\u003e pointList = new LinkedList\u003c\u003e(); for (int i = 0; i \u003c 100; i++) { // build one point final Point point = Point.newPointBuilder(\"machine_table\") .setTimestamp(t0) .addTag(\"city\", \"Singapore\") .addTag(\"ip\", \"10.0.0.1\") .addField(\"cpu\", Value.withDouble(0.23)) .addField(\"mem\", Value.withDouble(0.55)) .build(); points.add(point); } Then, you can use write interface to write data:\n1 2 3 4 5 6 7 8 final CompletableFuture\u003cResult\u003cWriteOk, Err\u003e\u003e wf = client.write(new WriteRequest(pointList)); // here the `future.get` is just for demonstration, a better async programming practice would be using the CompletableFuture API final Result\u003cWriteOk, Err\u003e writeResult = wf.get(); Assert.assertTrue(writeResult.isOk()); // `Result` class referenced the Rust language practice, provides rich functions (such as mapXXX, andThen) transforming the result value to improve programming efficiency. You can refer to the API docs for detail usage. Assert.assertEquals(3, writeResult.getOk().getSuccess()); Assert.assertEquals(3, writeResult.mapOr(0, WriteOk::getSuccess).intValue()); Assert.assertEquals(0, writeResult.mapOr(-1, WriteOk::getFailed).intValue()); See write\nQuery Data Example 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 final SqlQueryRequest queryRequest = SqlQueryRequest.newBuilder() .forTables(\"machine_table\") // table name is optional. If not provided, SQL parser will parse the `ssql` to get the table name and do the routing automaticly .sql(\"select * from machine_table where ts = %d\", t0) // .build(); final CompletableFuture\u003cResult\u003cSqlQueryOk, Err\u003e\u003e qf = client.sqlQuery(queryRequest); // here the `future.get` is just for demonstration, a better async programming practice would be using the CompletableFuture API final Result\u003cSqlQueryOk, Err\u003e queryResult = qf.get(); Assert.assertTrue(queryResult.isOk()); final SqlQueryOk queryOk = queryResult.getOk(); Assert.assertEquals(1, queryOk.getRowCount()); // get rows as list final List\u003cRow\u003e rows = queryOk.getRowList(); Assert.assertEquals(t0, rows.get(0).getColumn(\"ts\").getValue().getTimestamp()); Assert.assertEquals(\"Singapore\", rows.get(0).getColumn(\"city\").getValue().getString()); Assert.assertEquals(\"10.0.0.1\", rows.get(0).getColumn(\"ip\").getValue().getString()); Assert.assertEquals(0.23, rows.get(0).getColumn(\"cpu\").getValue().getDouble(), 0.0000001); Assert.assertEquals(0.55, rows.get(0).getColumn(\"mem\").getValue().getDouble(), 0.0000001); // get rows as stream final Stream\u003cRow\u003e rowStream = queryOk.stream(); rowStream.forEach(row -\u003e System.out.println(row.toString())); See read\nStream Write/Read Example HoraeDB support streaming writing and reading，suitable for large-scale data reading and writing。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 long start = System.currentTimeMillis(); long t = start; final StreamWriteBuf\u003cPoint, WriteOk\u003e writeBuf = client.streamWrite(\"machine_table\"); for (int i = 0; i \u003c 1000; i++) { final Point streamData = Point.newPointBuilder(\"machine_table\") .setTimestamp(t) .addTag(\"city\", \"Beijing\") .addTag(\"ip\", \"10.0.0.3\") .addField(\"cpu\", Value.withDouble(0.42)) .addField(\"mem\", Value.withDouble(0.67)) .build(); writeBuf.writeAndFlush(Collections.singletonList(streamData)); t = t+1; } final CompletableFuture\u003cWriteOk\u003e writeOk = writeBuf.completed(); Assert.assertEquals(1000, writeOk.join().getSuccess()); final SqlQueryRequest streamQuerySql = SqlQueryRequest.newBuilder() .sql(\"select * from %s where city = '%s' and ts \u003e= %d and ts \u003c %d\", \"machine_table\", \"Beijing\", start, t).build(); final Result\u003cSqlQueryOk, Err\u003e streamQueryResult = client.sqlQuery(streamQuerySql).get(); Assert.assertTrue(streamQueryResult.isOk()); Assert.assertEquals(1000, streamQueryResult.getOk().getRowCount()); See streaming\n","categories":"","description":"","excerpt":"Introduction HoraeDB Client is a high-performance Java client for …","ref":"/docs/user-guide/sdk/java/","tags":"","title":"Java"},{"body":"注意：此功能仅供测试使用，不推荐生产使用，相关功能将来可能会发生变化。\n本章介绍如何部署一个静态（无 HoraeMeta）的 HoraeDB 集群。\n在没有 HoraeMeta 的情况下，利用 HoraeDB 服务端针对表名提供了可配置的路由功能即可实现集群化部署，为此我们需要提供一个包含路由规则的正确配置。根据这个配置，请求会被发送到集群中的每个 HoraeDB 实例。\n目标 本文的目标是：在同一台机器上部署一个集群，这个集群包含两个 HoraeDB 实例。\n如果想要部署一个更大规模的集群，参考此方案也可以进行部署。\n准备配置文件 基础配置 HoraeDB 的基础配置如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 [server] bind_addr = \"0.0.0.0\" http_port = 5440 grpc_port = 8831 [logger] level = \"info\" [tracing] dir = \"/tmp/horaedb\" [analytic.storage.object_store] type = \"Local\" data_dir = \"/tmp/horaedb\" [analytic.wal] type = \"RocksDB\" data_dir = \"/tmp/horaedb\" 为了在同一个机器上部署两个实例，我们需要为每个实例配置不同的服务端口和数据目录。\n实例 HoraeDB_0 的配置如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 [server] bind_addr = \"0.0.0.0\" http_port = 5440 grpc_port = 8831 [logger] level = \"info\" [tracing] dir = \"/tmp/horaedb_0\" [analytic.storage.object_store] type = \"Local\" data_dir = \"/tmp/horaedb_0\" [analytic.wal] type = \"RocksDB\" data_dir = \"/tmp/horaedb_0\" 实例 HoraeDB_1 的配置如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 [server] bind_addr = \"0.0.0.0\" http_port = 15440 grpc_port = 18831 [logger] level = \"info\" [tracing] dir = \"/tmp/horaedb_1\" [analytic.storage.object_store] type = \"Local\" data_dir = \"/tmp/horaedb_1\" [analytic.wal] type = \"RocksDB\" data_dir = \"/tmp/horaedb_1\" Schema 和 Shard 接下来我们需要定义 Schema 和分片以及路由规则。\n如下定义了 Schema 和分片：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 [cluster_deployment] mode = \"NoMeta\" [[cluster_deployment.topology.schema_shards]] schema = 'public_0' [[cluster_deployment.topology.schema_shards.shard_views]] shard_id = 0 [cluster_deployment.topology.schema_shards.shard_views.endpoint] addr = '127.0.0.1' port = 8831 [[cluster_deployment.topology.schema_shards.shard_views]] shard_id = 1 [cluster_deployment.topology.schema_shards.shard_views.endpoint] addr = '127.0.0.1' port = 8831 [[cluster_deployment.topology.schema_shards]] schema = 'public_1' [[cluster_deployment.topology.schema_shards.shard_views]] shard_id = 0 [cluster_deployment.topology.schema_shards.shard_views.endpoint] addr = '127.0.0.1' port = 8831 [[cluster_deployment.topology.schema_shards.shard_views]] shard_id = 1 [cluster_deployment.topology.schema_shards.shard_views.endpoint] addr = '127.0.0.1' port = 18831 上述的配置中，定义了两个 Schema：\npublic_0 有两个分片在 HoraeDB_0 实例上。 public_1 有两个分片同时在 HoraeDB_0 和 HoraeDB_1 实例上。 路由规则 定义 Schema 和分片后，需要定义路由规则，如下是一个前缀路由规则：\n1 2 3 4 [[cluster_deployment.route_rules.prefix_rules]] schema = 'public_0' prefix = 'prod_' shard = 0 在这个规则里，public_0 中表名以 prod_ 为前缀的所有表属于，相关操作会被路由到 shard_0 也就是 HoraeDB_0 实例。 public_0 中其他的表会以 hash 的方式路由到 shard_0 和 shard_1.\n在前缀规则之外，我们也可以定义一个 hash 规则：\n1 2 3 [[cluster_deployment.route_rules.hash_rules]] schema = 'public_1' shards = [0, 1] 这个规则告诉 HoraeDB, public_1 的所有表会被路由到 public_1 的 shard_0 and shard_1, 也就是 HoraeDB0 和 HoraeDB_1. 实际上如果没有定义 public_1 的路由规则，这是默认的路由行为。\nHoraeDB_0 和 HoraeDB_1 实例完整的配置文件如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 [server] bind_addr = \"0.0.0.0\" http_port = 5440 grpc_port = 8831 [logger] level = \"info\" [tracing] dir = \"/tmp/horaedb_0\" [analytic.storage.object_store] type = \"Local\" data_dir = \"/tmp/horaedb_0\" [analytic.wal] type = \"RocksDB\" data_dir = \"/tmp/horaedb_0\" [cluster_deployment] mode = \"NoMeta\" [[cluster_deployment.topology.schema_shards]] schema = 'public_0' [[cluster_deployment.topology.schema_shards.shard_views]] shard_id = 0 [cluster_deployment.topology.schema_shards.shard_views.endpoint] addr = '127.0.0.1' port = 8831 [[cluster_deployment.topology.schema_shards.shard_views]] shard_id = 1 [cluster_deployment.topology.schema_shards.shard_views.endpoint] addr = '127.0.0.1' port = 8831 [[cluster_deployment.topology.schema_shards]] schema = 'public_1' [[cluster_deployment.topology.schema_shards.shard_views]] shard_id = 0 [cluster_deployment.topology.schema_shards.shard_views.endpoint] addr = '127.0.0.1' port = 8831 [[cluster_deployment.topology.schema_shards.shard_views]] shard_id = 1 [cluster_deployment.topology.schema_shards.shard_views.endpoint] addr = '127.0.0.1' port = 18831 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 [server] bind_addr = \"0.0.0.0\" http_port = 15440 grpc_port = 18831 [logger] level = \"info\" [tracing] dir = \"/tmp/horaedb_1\" [analytic.storage.object_store] type = \"Local\" data_dir = \"/tmp/horaedb_1\" [analytic.wal] type = \"RocksDB\" data_dir = \"/tmp/horaedb_1\" [cluster_deployment] mode = \"NoMeta\" [[cluster_deployment.topology.schema_shards]] schema = 'public_0' [[cluster_deployment.topology.schema_shards.shard_views]] shard_id = 0 [cluster_deployment.topology.schema_shards.shard_views.endpoint] addr = '127.0.0.1' port = 8831 [[cluster_deployment.topology.schema_shards.shard_views]] shard_id = 1 [cluster_deployment.topology.schema_shards.shard_views.endpoint] addr = '127.0.0.1' port = 8831 [[cluster_deployment.topology.schema_shards]] schema = 'public_1' [[cluster_deployment.topology.schema_shards.shard_views]] shard_id = 0 [cluster_deployment.topology.schema_shards.shard_views.endpoint] addr = '127.0.0.1' port = 8831 [[cluster_deployment.topology.schema_shards.shard_views]] shard_id = 1 [cluster_deployment.topology.schema_shards.shard_views.endpoint] addr = '127.0.0.1' port = 18831 我们给这两份不同的配置文件分别命名为 config_0.toml 和 config_1.toml； 但是在实际环境中不同的实例可以部署在不同的服务器上，也就是说，不同的实例没有必要设置不同的服务端口和数据目录，这种情况下实例的配置可以使用同一份配置文件。\n启动 HoraeDB 配置准备好后，我们就可以开始启动 HoraeDB 容器了。\n启动命令如下：\n1 2 sudo docker run -d -t --name horaedb_0 -p 5440:5440 -p 8831:8831 -v $(pwd)/config_0.toml:/etc/horaedb/horaedb.toml horaedb/horaedb-server sudo docker run -d -t --name horaedb_1 -p 15440:15440 -p 18831:18831 -v $(pwd)/config_1.toml:/etc/horaedb/horaedb.toml horaedb/horaedb-server 容器启动成功后，两个实例的 HoraeDB 集群就搭建完成了，可以开始提供读写服务。\n","categories":"","description":"","excerpt":"注意：此功能仅供测试使用，不推荐生产使用，相关功能将来可能会发生变化。\n本章介绍如何部署一个静态（无 HoraeMeta）的 HoraeDB …","ref":"/cn/docs/user-guide/cluster_deployment/no_meta/","tags":"","title":"NoMeta 模式"},{"body":"HoraeDB is observable with Prometheus and Grafana.\nPrometheus Prometheus is a systems and service monitoring system.\nConfiguration Save the following configuration into the prometheus.yml file. For example, in the tmp directory, /tmp/prometheus.yml.\nTwo HoraeDB http service are started on localhost:5440 and localhost:5441.\n1 2 3 4 5 6 7 8 global: scrape_interval: 30s scrape_configs: - job_name: horaedb-server static_configs: - targets: [your_ip:5440, your_ip:5441] labels: env: horaedbcluster See details about configuration here.\nRun You can use docker to start Prometheus. The docker image information is here.\ndocker run \\ -d --name=prometheus \\ -p 9090:9090 \\ -v /tmp/prometheus.yml:/etc/prometheus/prometheus.yml \\ prom/prometheus:v2.41.0 For more detailed installation methods, refer to here.\nGrafana Grafana is an open and composable observability and data visualization platform.\nRun You can use docker to start grafana. The docker image information is here.\ndocker run -d --name=grafana -p 3000:3000 grafana/grafana:9.3.6 Default admin user credentials are admin/admin.\nGrafana is available on http://127.0.0.1:3000.\nFor more detailed installation methods, refer to here.\nConfigure data source Hover the cursor over the Configuration (gear) icon. Select Data Sources. Select the Prometheus data source. Note: The url of Prometheus is http://your_ip:9090.\nSee more details here.\nImport grafana dashboard dashboard json\nHoraeDB Metrics After importing the dashboard, you will see the following page:\nPanels tps: Number of cluster write requests. qps: Number of cluster query requests. 99th query/write duration: 99th quantile of write and query duration. table query: Query group by table. 99th write duration details by instance: 99th quantile of write duration group by instance. 99th query duration details by instance: 99th quantile of query duration group by instance. 99th write partition table duration: 99th quantile of write duration of partition table. table rows: The rows of data written. table rows by instance: The written rows by instance. total tables to write: Number of tables with data written. flush count: Number of HoraeDB flush. 99th flush duration details by instance: 99th quantile of flush duration group by instance. 99th write stall duration details by instance: 99th quantile of write stall duration group by instance. ","categories":"","description":"","excerpt":"HoraeDB is observable with Prometheus and Grafana.\nPrometheus …","ref":"/docs/user-guide/operation/observability/","tags":"","title":"Observability"},{"body":"OpenTSDB 是基于 HBase 的分布式、可伸缩的时间序列数据库。\n写入 HoraeDB 遵循 OpenTSDB put 写入接口。\nsummary 和 detailed 还未支持。\ncurl --location 'http://localhost:5440/opentsdb/api/put' \\ --header 'Content-Type: application/json' \\ -d '[{ \"metric\": \"sys.cpu.nice\", \"timestamp\": 1692588459000, \"value\": 18, \"tags\": { \"host\": \"web01\", \"dc\": \"lga\" } }, { \"metric\": \"sys.cpu.nice\", \"timestamp\": 1692588459000, \"value\": 18, \"tags\": { \"host\": \"web01\" } }]' metric 将映射到 HoraeDB 中的一个表，在首次写入时 server 会自动进行建表(注意：创建表的 TTL 是 7d，写入超过当前周期数据会被丢弃)。\n例如，在上面插入数据时，HoraeDB 中将创建下表：\nCREATE TABLE `sys.cpu.nice`( `tsid` uint64 NOT NULL, `timestamp` timestamp NOT NULL, `dc` string TAG, `host` string TAG, `value` bigint, PRIMARY KEY(tsid, timestamp), TIMESTAMP KEY(timestamp)) ENGINE = Analytic WITH(arena_block_size = '2097152', compaction_strategy = 'default', compression = 'ZSTD', enable_ttl = 'true', num_rows_per_row_group = '8192', segment_duration = '2h', storage_format = 'AUTO', ttl = '7d', update_mode = 'OVERWRITE', write_buffer_size = '33554432') 查询 暂不支持 OpenTSDB 查询，tracking issue。\n","categories":"","description":"","excerpt":"OpenTSDB 是基于 HBase 的分布式、可伸缩的时间序列数据库。\n写入 HoraeDB 遵循 OpenTSDB put 写入接口。 …","ref":"/cn/docs/user-guide/ecosystem/opentsdb/","tags":"","title":"OpenTSDB"},{"body":"OpenTSDB is a distributed and scalable time series database based on HBase.\nWrite HoraeDB follows the OpenTSDB put write protocol.\nsummary and detailed are not yet supported.\ncurl --location 'http://localhost:5440/opentsdb/api/put' \\ --header 'Content-Type: application/json' \\ -d '[{ \"metric\": \"sys.cpu.nice\", \"timestamp\": 1692588459000, \"value\": 18, \"tags\": { \"host\": \"web01\", \"dc\": \"lga\" } }, { \"metric\": \"sys.cpu.nice\", \"timestamp\": 1692588459000, \"value\": 18, \"tags\": { \"host\": \"web01\" } }]' ' Metric will be mapped to table in HoraeDB, and it will be created automatically in first write(Note: The default TTL is 7d, and points written exceed TTL will be discarded directly).\nFor example, when inserting data above, table below will be automatically created in HoraeDB:\nCREATE TABLE `sys.cpu.nice`( `tsid` uint64 NOT NULL, `timestamp` timestamp NOT NULL, `dc` string TAG, `host` string TAG, `value` bigint, PRIMARY KEY(tsid, timestamp), TIMESTAMP KEY(timestamp)) ENGINE = Analytic WITH(arena_block_size = '2097152', compaction_strategy = 'default', compression = 'ZSTD', enable_ttl = 'true', num_rows_per_row_group = '8192', segment_duration = '2h', storage_format = 'AUTO', ttl = '7d', update_mode = 'OVERWRITE', write_buffer_size = '33554432') Query OpenTSDB query protocol is not currently supported, tracking issue.\n","categories":"","description":"","excerpt":"OpenTSDB is a distributed and scalable time series database based on …","ref":"/docs/user-guide/ecosystem/opentsdb/","tags":"","title":"OpenTSDB"},{"body":"CPU profiling HoraeDB provides cpu profiling http api debug/profile/cpu.\nExample:\n// 60s cpu sampling data curl 0:5000/debug/profile/cpu/60 // Output file path. /tmp/flamegraph_cpu.svg Heap profiling HoraeDB provides heap profiling http api debug/profile/heap.\nInstall dependencies sudo yum install -y jemalloc-devel ghostscript graphviz Example:\n// enable malloc prof export MALLOC_CONF=prof:true // run horaedb-server ./horaedb-server .... // 60s cpu sampling data curl -L '0:5000/debug/profile/heap/60' \u003e /tmp/heap_profile jeprof --show_bytes --pdf /usr/bin/horaedb-server /tmp/heap_profile \u003e profile_heap.pdf jeprof --show_bytes --svg /usr/bin/horaedb-server /tmp/heap_profile \u003e profile_heap.svg ","categories":"","description":"","excerpt":"CPU profiling HoraeDB provides cpu profiling http api …","ref":"/docs/dev/profiling/","tags":"","title":"Profiling"},{"body":"Prometheus是一个流行的云原生监控工具，由于其可扩展性、可靠性和可伸缩性，被企业广泛采用。它用于从云原生服务（如 Kubernetes 和 OpenShift）中获取指标，并将其存储在时序数据库中。Prometheus 也很容易扩展，允许用户用其他数据库扩展其特性和功能。\nHoraeDB 可以作为 Prometheus 的长期存储解决方案，同时支持远程读取和远程写入 API。\n配置 你可以通过在prometheus.yml中添加以下几行来配置 Prometheus 使用 HoraeDB 作为一个远程存储：\n1 2 3 4 remote_write: - url: \"http://\u003caddress\u003e:\u003chttp_port\u003e/prom/v1/write\" remote_read: - url: \"http://\u003caddress\u003e:\u003chttp_port\u003e/prom/v1/read\" 每一个指标都会对应一个 HoraeDB 中的表：\n标签（labels）对应字符串类型的 tag 列 数据的时间戳对应一个 timestamp 类型的 timestmap 列 数据的值对应一个双精度浮点数类型的 value 列 比如有如下 Prometheus 指标：\nup{env=\"dev\", instance=\"127.0.0.1:9090\", job=\"prometheus-server\"} 对应 HoraeDB 中如下的表(注意：创建表的 TTL 是 7d，写入超过当前周期数据会被丢弃)：\nCREATE TABLE `up` ( `timestamp` timestamp NOT NULL, `tsid` uint64 NOT NULL, `env` string TAG, `instance` string TAG, `job` string TAG, `value` double, PRIMARY KEY (tsid, timestamp), timestamp KEY (timestamp) ); SELECT * FROM up; tsid timestamp env instance job value 12683162471309663278 1675824740880 dev 127.0.0.1:9090 prometheus-server 1 ","categories":"","description":"","excerpt":"Prometheus是一个流行的云原生监控工具，由于其可扩展性、可靠性和可伸缩性，被企业广泛采用。它用于从云原生服务（ …","ref":"/cn/docs/user-guide/ecosystem/prometheus/","tags":"","title":"Prometheus"},{"body":"Prometheus is a popular cloud-native monitoring tool that is widely adopted by organizations due to its scalability, reliability, and scalability. It is used to scrape metrics from cloud-native services, such as Kubernetes and OpenShift, and stores it in a time-series database. Prometheus is also easily extensible, allowing users to extend its features and capabilities with other databases.\nHoraeDB can be used as a long-term storage solution for Prometheus. Both remote read and remote write API are supported.\nConfig You can configure Prometheus to use HoraeDB as a remote storage by adding following lines to prometheus.yml:\n1 2 3 4 remote_write: - url: \"http://\u003caddress\u003e:\u003chttp_port\u003e/prom/v1/write\" remote_read: - url: \"http://\u003caddress\u003e:\u003chttp_port\u003e/prom/v1/read\" Each metric will be converted to one table in HoraeDB:\nlabels are mapped to corresponding string tag column timestamp of sample is mapped to a timestamp timestmap column value of sample is mapped to a double value column For example, up metric below will be mapped to up table:\nup{env=\"dev\", instance=\"127.0.0.1:9090\", job=\"prometheus-server\"} Its corresponding table in HoraeDB(Note: The TTL for creating a table is 7d, and points written exceed TTL will be discarded directly):\nCREATE TABLE `up` ( `timestamp` timestamp NOT NULL, `tsid` uint64 NOT NULL, `env` string TAG, `instance` string TAG, `job` string TAG, `value` double, PRIMARY KEY (tsid, timestamp), timestamp KEY (timestamp) ); SELECT * FROM up; tsid timestamp env instance job value 12683162471309663278 1675824740880 dev 127.0.0.1:9090 prometheus-server 1 ","categories":"","description":"","excerpt":"Prometheus is a popular cloud-native monitoring tool that is widely …","ref":"/docs/user-guide/ecosystem/prometheus/","tags":"","title":"Prometheus"},{"body":"介绍 horaedb-client 是 HoraeDB python 客户端.\n借助于 PyO3，python 客户端的实现实际上是基于 rust 客户端 的封装。\n本手册将会介绍 python client 的一些基本用法，其中涉及到的完整示例，可以查看该示例代码.\n环境要求 Python \u003e= 3.7 安装 1 pip install ceresdb-client 你可以在这里找到最新的版本 here.\n初始化客户端 首先介绍下如何初始化客户端，代码示例如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 import asyncio import datetime from ceresdb_client import Builder, RpcContext, PointBuilder, ValueBuilder, WriteRequest, SqlQueryRequest, Mode, RpcConfig rpc_config = RpcConfig() rpc_config = RpcConfig() rpc_config.thread_num = 1 rpc_config.default_write_timeout_ms = 1000 builder = Builder('127.0.0.1:8831', Mode.Direct) builder.set_rpc_config(rpc_config) builder.set_default_database('public') client = builder.build() 代码的最开始部分是依赖库的导入，在后面的示例中将省略这部分。\n客户端初始化需要至少两个参数：\nEndpoint： 服务端地址，由 ip 和端口组成，例如 127.0.0.1：8831; Mode: 客户端和服务端通信模式，有两种模式可供选择: Direct 和 Proxy。 这里重点介绍下通信模式 Mode， 当客户端可以访问所有的服务器的时候，建议采用 Direct 模式，以减少转发开销；但是如果客户端访问服务器必须要经过网关，那么只能选择 Proxy 模式。\n至于 default_database，会在执行 RPC 请求时未通过 RpcContext 设置 database 的情况下，将被作为目标 database 使用。\n最后，通过配置 RpcConfig, 可以管理客户端使用的资源和调整其性能，所有的配置参数可以参考这里.\n建表 为了方便使用，在使用 gRPC 的 write 接口进行写入时，如果某个表不存在，HoraeDB 会根据第一次的写入自动创建一个表。\n当然你也可以通过 create table 语句来更精细化的管理的表（比如添加索引等）。\n初始化客户端后，建表示例如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 def async_query(client, ctx, req): await client.sql_query(ctx, req) create_table_sql = 'CREATE TABLE IF NOT EXISTS demo ( \\ name string TAG, \\ value double, \\ t timestamp NOT NULL, \\ TIMESTAMP KEY(t)) ENGINE=Analytic with (enable_ttl=false)' req = SqlQueryRequest(['demo'], create_table_sql) rpc_ctx = RpcContext() rpc_ctx.database = 'public' rpc_ctx.timeout_ms = 100 event_loop = asyncio.get_event_loop() event_loop.run_until_complete(async_query(client, rpc_ctx, req)) RpcContext 可以用来指定目标 database （可以覆盖在初始化的时候设置的 default_space） 和超时参数。\n数据写入 可以使用 PointBuilder 来构建一个 point（实际上就是数据集的一行），多个 point 构成一个写入请求。\n示例如下:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 async def async_write(client, ctx, req): return await client.write(ctx, req) point_builder = PointBuilder('demo') point_builder.set_timestamp(1000 * int(round(datetime.datetime.now().timestamp()))) point_builder.set_tag(\"name\", ValueBuilder().string(\"test_tag1\")) point_builder.set_field(\"value\", ValueBuilder().double(0.4242)) point = point_builder.build() write_request = WriteRequest() write_request.add_point(point) event_loop = asyncio.get_event_loop() event_loop.run_until_complete(async_write(client, ctx, req)) 数据查询 通过 sql_query 接口, 可以方便地从服务端查询数据：\nreq = SqlQueryRequest(['demo'], 'select * from demo') event_loop = asyncio.get_event_loop() resp = event_loop.run_until_complete(async_query(client, ctx, req)) 如示例所展示, 构建 SqlQueryRequest 需要两个参数:\n查询 sql 中涉及到的表； 查询 sql. 当前为了查询的性能，第一个参数是必须的。\n查询到数据后，逐行逐列处理数据的示例如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # Access row by index in the resp. for row_idx in range(0, resp.num_rows()): row_tokens = [] row = resp.row_by_idx(row_idx) for col_idx in range(0, row.num_cols()): col = row.column_by_idx(col_idx) row_tokens.append(f\"{col.name()}:{col.value()}#{col.data_type()}\") print(f\"row#{row_idx}: {','.join(row_tokens)}\") # Access row by iter in the resp. for row in resp.iter_rows(): row_tokens = [] for col in row.iter_columns(): row_tokens.append(f\"{col.name()}:{col.value()}#{col.data_type()}\") print(f\"row: {','.join(row_tokens)}\") 删表 和创建表类似，我们可以使用 sql 来删除表：\n1 2 3 4 5 6 drop_table_sql = 'DROP TABLE demo' req = SqlQueryRequest(['demo'], drop_table_sql) event_loop = asyncio.get_event_loop() event_loop.run_until_complete(async_query(client, rpc_ctx, req)) ","categories":"","description":"","excerpt":"介绍 horaedb-client 是 HoraeDB python 客户端.\n借助于 PyO3，python …","ref":"/cn/docs/user-guide/sdk/python/","tags":"","title":"Python"},{"body":"Introduction horaedb-client is the python client for HoraeDB.\nThanks to PyO3, the python client is actually a wrapper on the rust client.\nThe guide will give a basic introduction to the python client by example.\nRequirements Python \u003e= 3.7 Installation 1 pip install ceresdb-client You can get latest version here.\nInit HoraeDB Client The client initialization comes first, here is a code snippet:\n1 2 3 4 5 6 7 8 9 10 11 12 13 import asyncio import datetime from ceresdb_client import Builder, RpcContext, PointBuilder, ValueBuilder, WriteRequest, SqlQueryRequest, Mode, RpcConfig rpc_config = RpcConfig() rpc_config = RpcConfig() rpc_config.thread_num = 1 rpc_config.default_write_timeout_ms = 1000 builder = Builder('127.0.0.1:8831', Mode.Direct) builder.set_rpc_config(rpc_config) builder.set_default_database('public') client = builder.build() Firstly, it’s worth noting that the imported packages are used across all the code snippets in this guide, and they will not be repeated in the following.\nThe initialization requires at least two parameters:\nEndpoint: the server endpoint consisting of ip address and serving port, e.g. 127.0.0.1:8831; Mode: The mode of the communication between client and server, and there are two kinds of Mode: Direct and Proxy. Endpoint is simple, while Mode deserves more explanation. The Direct mode should be adopted to avoid forwarding overhead if all the servers are accessible to the client. However, the Proxy mode is the only choice if the access to the servers from the client must go through a gateway.\nThe default_database can be set and will be used if following rpc calling without setting the database in the RpcContext.\nBy configuring the RpcConfig, resource and performance of the client can be manipulated, and all of the configurations can be referred at here.\nCreate Table For ease of use, when using gRPC’s write interface for writing, if a table does not exist, HoraeDB will automatically create a table based on the first write.\nOf course, you can also use create table statement to manage the table more finely (such as adding indexes).\nHere is a example for creating table by the initialized client:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 async def async_query(client, ctx, req): await client.sql_query(ctx, req) create_table_sql = 'CREATE TABLE IF NOT EXISTS demo ( \\ name string TAG, \\ value double, \\ t timestamp NOT NULL, \\ TIMESTAMP KEY(t)) ENGINE=Analytic with (enable_ttl=false)' req = SqlQueryRequest(['demo'], create_table_sql) rpc_ctx = RpcContext() rpc_ctx.database = 'public' rpc_ctx.timeout_ms = 100 event_loop = asyncio.get_event_loop() event_loop.run_until_complete(async_query(client, rpc_ctx, req)) RpcContext can be used to overwrite the default database and timeout defined in the initialization of the client.\nWrite Data PointBuilder can be used to construct a point, which is actually a row in data set. The write request consists of multiple points.\nThe example is simple:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 async def async_write(client, ctx, req): return await client.write(ctx, req) point_builder = PointBuilder('demo') point_builder.set_timestamp(1000 * int(round(datetime.datetime.now().timestamp()))) point_builder.set_tag(\"name\", ValueBuilder().string(\"test_tag1\")) point_builder.set_field(\"value\", ValueBuilder().double(0.4242)) point = point_builder.build() write_request = WriteRequest() write_request.add_point(point) event_loop = asyncio.get_event_loop() event_loop.run_until_complete(async_write(client, ctx, req)) Query Data By sql_query interface, it is easy to retrieve the data from the server:\nreq = SqlQueryRequest(['demo'], 'select * from demo') event_loop = asyncio.get_event_loop() resp = event_loop.run_until_complete(async_query(client, ctx, req)) As the example shows, two parameters are needed to construct the SqlQueryRequest:\nThe tables involved by this sql query; The query sql. Currently, the first parameter is necessary for performance on routing.\nWith retrieved data, we can process it row by row and column by column:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # Access row by index in the resp. for row_idx in range(0, resp.num_rows()): row_tokens = [] row = resp.row_by_idx(row_idx) for col_idx in range(0, row.num_cols()): col = row.column_by_idx(col_idx) row_tokens.append(f\"{col.name()}:{col.value()}#{col.data_type()}\") print(f\"row#{row_idx}: {','.join(row_tokens)}\") # Access row by iter in the resp. for row in resp.iter_rows(): row_tokens = [] for col in row.iter_columns(): row_tokens.append(f\"{col.name()}:{col.value()}#{col.data_type()}\") print(f\"row: {','.join(row_tokens)}\") Drop Table Finally, we can drop the table by the sql api, which is similar to the table creation:\n1 2 3 4 5 6 drop_table_sql = 'DROP TABLE demo' req = SqlQueryRequest(['demo'], drop_table_sql) event_loop = asyncio.get_event_loop() event_loop.run_until_complete(async_query(client, rpc_ctx, req)) ","categories":"","description":"","excerpt":"Introduction horaedb-client is the python client for HoraeDB.\nThanks …","ref":"/docs/user-guide/sdk/python/","tags":"","title":"Python"},{"body":"As every Rust programmer knows, the language has many powerful features, and there are often several patterns which can express the same idea. Also, as every professional programmer comes to discover, code is almost always read far more than it is written.\nThus, we choose to use a consistent set of idioms throughout our code so that it is easier to read and understand for both existing and new contributors.\nUnsafe and Platform-Dependent conditional compilation Avoid unsafe Rust One of the main reasons to use Rust as an implementation language is its strong memory safety guarantees; Almost all of these guarantees are voided by the use of unsafe. Thus, unless there is an excellent reason and the use is discussed beforehand, it is unlikely HoraeDB will accept patches with unsafe code.\nWe may consider taking unsafe code given:\nperformance benchmarks showing a very compelling improvement a compelling explanation of why the same performance can not be achieved using safe code tests showing how it works safely across threads Avoid platform-specific conditional compilation cfg We hope that HoraeDB is usable across many different platforms and Operating systems, which means we put a high value on standard Rust.\nWhile some performance critical code may require architecture specific instructions, (e.g. AVX512) most of the code should not.\nErrors All errors should follow the SNAFU crate philosophy and use SNAFU functionality Good:\nDerives Snafu and Debug functionality Has a useful, end-user-friendly display message 1 2 3 4 5 6 #[derive(Snafu, Debug)] pub enum Error { #[snafu(display(r#\"Conversion needs at least one line of data\"#))] NeedsAtLeastOneLine, // ... } Bad:\n1 2 3 pub enum Error { NeedsAtLeastOneLine, // ... Use the ensure! macro to check a condition and return an error Good:\nReads more like an assert! Is more concise 1 ensure!(!self.schema_sample.is_empty(), NeedsAtLeastOneLine); Bad:\n1 2 3 if self.schema_sample.is_empty() { return Err(Error::NeedsAtLeastOneLine {}); } Errors should be defined in the module they are instantiated Good:\nGroups related error conditions together most closely with the code that produces them Reduces the need to match on unrelated errors that would never happen 1 2 3 4 5 6 7 8 9 #[derive(Debug, Snafu)] pub enum Error { #[snafu(display(\"Not implemented: {}\", operation_name))] NotImplemented { operation_name: String } } // ... ensure!(foo.is_implemented(), NotImplemented { operation_name: \"foo\", } Bad:\n1 2 3 4 5 use crate::errors::NotImplemented; // ... ensure!(foo.is_implemented(), NotImplemented { operation_name: \"foo\", } The Result type alias should be defined in each module Good:\nReduces repetition 1 2 3 pub type Result\u003cT, E = Error\u003e = std::result::Result\u003cT, E\u003e; ... fn foo() -\u003e Result\u003cbool\u003e { true } Bad:\n1 2 ... fn foo() -\u003e Result\u003cbool, Error\u003e { true } Err variants should be returned with fail() Good:\n1 2 3 return NotImplemented { operation_name: \"Parquet format conversion\", }.fail(); Bad:\n1 2 3 return Err(Error::NotImplemented { operation_name: String::from(\"Parquet format conversion\"), }); Use context to wrap underlying errors into module specific errors Good:\nReduces boilerplate 1 2 3 4 5 input_reader .read_to_string(\u0026mut buf) .context(UnableToReadInput { input_filename, })?; Bad:\n1 2 3 4 5 6 input_reader .read_to_string(\u0026mut buf) .map_err(|e| Error::UnableToReadInput { name: String::from(input_filename), source: e, })?; Hint for Box\u003cdyn::std::error::Error\u003e in Snafu:\nIf your error contains a trait object (e.g. Box\u003cdyn std::error::Error + Send + Sync\u003e), in order to use context() you need to wrap the error in a Box, we provide a box_err function to help do this conversion:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 #[derive(Debug, Snafu)] pub enum Error { #[snafu(display(\"gRPC planner got error listing partition keys: {}\", source))] ListingPartitions { source: Box\u003cdyn std::error::Error + Send + Sync\u003e, }, } ... use use common_util::error::BoxError; // Wrap error in a box prior to calling context() database .partition_keys() .await .box_err() .context(ListingPartitions)?; Each error cause in a module should have a distinct Error enum variant Specific error types are preferred over a generic error with a message or kind field.\nGood:\nMakes it easier to track down the offending code based on a specific failure Reduces the size of the error enum (String is 3x 64-bit vs no space) Makes it easier to remove vestigial errors Is more concise 1 2 3 4 5 6 7 8 9 10 11 12 13 #[derive(Debug, Snafu)] pub enum Error { #[snafu(display(\"Error writing remaining lines {}\", source))] UnableToWriteGoodLines { source: IngestError }, #[snafu(display(\"Error while closing the table writer {}\", source))] UnableToCloseTableWriter { source: IngestError }, } // ... write_lines.context(UnableToWriteGoodLines)?; close_writer.context(UnableToCloseTableWriter))?; Bad:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 pub enum Error { #[snafu(display(\"Error {}: {}\", message, source))] WritingError { source: IngestError, message: String, }, } write_lines.context(WritingError { message: String::from(\"Error while writing remaining lines\"), })?; close_writer.context(WritingError { message: String::from(\"Error while closing the table writer\"), })?; Leaf error should contains backtrace In order to make debugging easier, leaf errors in error chain should contains a backtrace.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 // Error in module A pub enum Error { #[snafu(display(\"This is a leaf error, source:{}.\\nBacktrace:\\n{}\", source, backtrace))] LeafError { source: ErrorFromDependency, backtrace: Backtrace }, } // Error in module B pub enum Error { #[snafu(display(\"Another error, source:{}.\\nBacktrace:\\n{}\", source, backtrace))] AnotherError { /// This error wraps another error that already has a /// backtrace. Instead of capturing our own, we forward the /// request for the backtrace to the inner error. This gives a /// more accurate backtrace. #[snafu(backtrace)] source: crate::A::Error, }, } Tests Don’t return Result from test functions At the time of this writing, if you return Result from test functions to use ? in the test function body and an Err value is returned, the test failure message is not particularly helpful. Therefore, prefer not having a return type for test functions and instead using expect or unwrap in test function bodies.\nGood:\n1 2 3 4 5 6 7 8 9 10 11 #[test] fn google_cloud() { let config = Config::new(); let integration = ObjectStore::new_google_cloud_storage(GoogleCloudStorage::new( config.service_account, config.bucket, )); put_get_delete_list(\u0026integration).unwrap(); list_with_delimiter(\u0026integration).unwrap(); } Bad:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 type TestError = Box\u003cdyn std::error::Error + Send + Sync + 'static\u003e; type Result\u003cT, E = TestError\u003e = std::result::Result\u003cT, E\u003e; #[test] fn google_cloud() -\u003e Result\u003c()\u003e { let config = Config::new(); let integration = ObjectStore::new_google_cloud_storage(GoogleCloudStorage::new( config.service_account, config.bucket, )); put_get_delete_list(\u0026integration)?; list_with_delimiter(\u0026integration)?; Ok(()) } Thanks Initial version of this doc is forked from influxdb_iox, thanks for their hard work.\n","categories":"","description":"","excerpt":"As every Rust programmer knows, the language has many powerful …","ref":"/docs/dev/style_guide/","tags":"","title":"Rationale and Goals"},{"body":"v0.1.0 Standalone version, local storage Analytical storage format Support SQL v0.2.0 Distributed version supports static topology defined in config file. The underlying storage supports Aliyun OSS. WAL implementation based on OBKV. v0.3.0 Release multi-language clients, including Java, Rust and Python. Static cluster mode with HoraeMeta. Basic implementation of hybrid storage format. v0.4.0 Implement more sophisticated cluster solution that enhances reliability and scalability of HoraeDB. Set up nightly benchmark with TSBS. v1.0.0-alpha (Released) Implement Distributed WAL based on Apache Kafka. Release Golang client. Improve the query performance for classic time series workloads. Support dynamic migration of tables in cluster mode. v1.0.0 Formally release HoraeDB and its SDKs with all breaking changes finished. Finish the majority of work related to Table Partitioning. Various efforts to improve query performance, especially for cloud-native cluster mode. These works include: Multi-tier cache. Introduce various methods to reduce the data fetched from remote storage (improve the accuracy of SST data filtering). Increase the parallelism while fetching data from remote object-store. Improve data ingestion performance by introducing resource control over compaction. Afterwards With an in-depth understanding of the time-series database and its various use cases, the majority of our work will focus on performance, reliability, scalability, ease of use, and collaborations with open-source communities.\nAdd utilities that support PromQL, InfluxQL, OpenTSDB protocol, and so on. Provide basic utilities for operation and maintenance. Specifically, the following are included: Deployment tools that fit well for cloud infrastructures like Kubernetes. Enhance self-observability, especially critical logs and metrics should be supplemented. Develop various tools that ease the use of HoraeDB. For example, data import and export tools. Explore new storage formats that will improve performance on hybrid workloads (analytical and time-series workloads). ","categories":"","description":"","excerpt":"v0.1.0 Standalone version, local storage Analytical storage format …","ref":"/docs/dev/roadmap/","tags":"","title":"RoadMap"},{"body":"安装 1 cargo add ceresdb-client 你可以在这里找到最新的版本 here.\n初始化客户端 首先，我们需要初始化客户端。\n创建客户端的 builder，你必须设置 endpoint 和 mode： endpoint 是类似 “ip/domain_name:port” 形式的字符串。 mode 用于指定访问 HoraeDB 服务器的方式，关于 mode 的详细信息。 1 let mut builder = Builder::new(\"ip/domain_name:port\", Mode::Direct/Mode::Proxy); 创建和设置 rpc_config，可以按需进行定义或者直接使用默认值，更多详细参数请参考这里： 1 2 3 4 5 6 let rpc_config = RpcConfig { thread_num: Some(1), default_write_timeout: Duration::from_millis(1000), ..Default::default() }; let builder = builder.rpc_config(rpc_config); 设置 default_database，这会在执行 RPC 请求时未通过 RpcContext 设置 database 的情况下，将被作为目标 database 使用。 1 let builder = builder.default_database(\"public\"); 最后，我们从 builder 中创建客户端： 1 let client = builder.build(); 管理表 为了方便使用，在使用 gRPC 的 write 接口进行写入时，如果某个表不存在，HoraeDB 会根据第一次的写入自动创建一个表。\n当然你也可以通过 create table 语句来更精细化的管理的表（比如添加索引等）。\n建表: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 let create_table_sql = r#\"CREATE TABLE IF NOT EXISTS horaedb ( str_tag string TAG, int_tag int32 TAG, var_tag varbinary TAG, str_field string, int_field int32, bin_field varbinary, t timestamp NOT NULL, TIMESTAMP KEY(t)) ENGINE=Analytic with (enable_ttl='false')\"#; let req = SqlQueryRequest { tables: vec![\"horaedb\".to_string()], sql: create_table_sql.to_string(), }; let resp = client .sql_query(rpc_ctx, \u0026req) .await .expect(\"Should succeed to create table\"); 删表： 1 2 3 4 5 6 7 8 9 10 let drop_table_sql = \"DROP TABLE horaedb\"; let req = SqlQueryRequest { tables: vec![\"horaedb\".to_string()], sql: drop_table_sql.to_string(), }; let resp = client .sql_query(rpc_ctx, \u0026req) .await .expect(\"Should succeed to create table\"); 写入数据 我们支持使用类似 InfluxDB 的时序数据模型进行写入。\n利用 PointBuilder 创建 point，tag value 和 field value 的相关数据结构为 Value，[Value 的详细信息](detail about Value](https://github.com/apache/incubator-horaedb-client-rs/blob/main/src/model/value.rs： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 let test_table = \"horaedb\"; let ts = Local::now().timestamp_millis(); let point = PointBuilder::new(test_table.to_string()) .timestamp(ts) .tag(\"str_tag\".to_string(), Value::String(\"tag_val\".to_string())) .tag(\"int_tag\".to_string(), Value::Int32(42)) .tag( \"var_tag\".to_string(), Value::Varbinary(b\"tag_bin_val\".to_vec()), ) .field( \"str_field\".to_string(), Value::String(\"field_val\".to_string()), ) .field(\"int_field\".to_string(), Value::Int32(42)) .field( \"bin_field\".to_string(), Value::Varbinary(b\"field_bin_val\".to_vec()), ) .build() .unwrap(); 将 point 添加到 write request 中： 1 2 let mut write_req = WriteRequest::default(); write_req.add_point(point); 创建 rpc_ctx，同样地可以按需设置或者使用默认值，rpc_ctx 的详细信息请参考这里： 1 2 3 4 let rpc_ctx = RpcContext { database: Some(\"public\".to_string()), ..Default::default() }; 最后，利用客户端写入数据到服务器： 1 2 3 4 5 let rpc_ctx = RpcContext { database: Some(\"public\".to_string()), ..Default::default() }; let resp = client.write(rpc_ctx, \u0026write_req).await.expect(\"Should success to write\"); Sql query 我们支持使用 sql 进行数据查询。\n在 sql query request 中指定相关的表和 sql 语句： 1 2 3 4 let req = SqlQueryRequest { tables: vec![table name 1,...,table name n], sql: sql string (e.g. select * from xxx), }; 利用客户端进行查询： 1 let resp = client.sql_query(rpc_ctx, \u0026req).await.expect(\"Should success to write\"); 示例 你可以在本项目的仓库中找到完整的例子。\n","categories":"","description":"","excerpt":"安装 1 cargo add ceresdb-client 你可以在这里找到最新的版本 here.\n初始化客户端 首先，我们需要初始化客户 …","ref":"/cn/docs/user-guide/sdk/rust/","tags":"","title":"Rust"},{"body":"Install 1 cargo add ceresdb-client You can get latest version here.\nInit Client At first, we need to init the client.\nNew builder for the client, and you must set endpoint and mode: endpoint is a string which is usually like “ip/domain_name:port”. mode is used to define the way to access horaedb server, detail about mode. 1 let mut builder = Builder::new(\"ip/domain_name:port\", Mode::Direct/Mode::Proxy); New and set rpc_config, it can be defined on demand or just use the default value, detail about rpc config: 1 2 3 4 5 6 let rpc_config = RpcConfig { thread_num: Some(1), default_write_timeout: Duration::from_millis(1000), ..Default::default() }; let builder = builder.rpc_config(rpc_config); Set default_database, it will be used if following rpc calling without setting the database in the RpcContext(will be introduced in later): 1 let builder = builder.default_database(\"public\"); Finally, we build client from builder: 1 let client = builder.build(); Manage Table For ease of use, when using gRPC’s write interface for writing, if a table does not exist, HoraeDB will automatically create a table based on the first write.\nOf course, you can also use create table statement to manage the table more finely (such as adding indexes).\nYou can use the sql query interface to create or drop table, related setting will be introduced in sql query section.\nCreate table: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 let create_table_sql = r#\"CREATE TABLE IF NOT EXISTS horaedb ( str_tag string TAG, int_tag int32 TAG, var_tag varbinary TAG, str_field string, int_field int32, bin_field varbinary, t timestamp NOT NULL, TIMESTAMP KEY(t)) ENGINE=Analytic with (enable_ttl='false')\"#; let req = SqlQueryRequest { tables: vec![\"horaedb\".to_string()], sql: create_table_sql.to_string(), }; let resp = client .sql_query(rpc_ctx, \u0026req) .await .expect(\"Should succeed to create table\"); Drop table: 1 2 3 4 5 6 7 8 9 10 let drop_table_sql = \"DROP TABLE horaedb\"; let req = SqlQueryRequest { tables: vec![\"horaedb\".to_string()], sql: drop_table_sql.to_string(), }; let resp = client .sql_query(rpc_ctx, \u0026req) .await .expect(\"Should succeed to create table\"); Write We support to write with the time series data model like InfluxDB.\nBuild the point first by PointBuilder, the related data structure of tag value and field value in it is defined as Value, detail about Value: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 let test_table = \"horaedb\"; let ts = Local::now().timestamp_millis(); let point = PointBuilder::new(test_table.to_string()) .timestamp(ts) .tag(\"str_tag\".to_string(), Value::String(\"tag_val\".to_string())) .tag(\"int_tag\".to_string(), Value::Int32(42)) .tag( \"var_tag\".to_string(), Value::Varbinary(b\"tag_bin_val\".to_vec()), ) .field( \"str_field\".to_string(), Value::String(\"field_val\".to_string()), ) .field(\"int_field\".to_string(), Value::Int32(42)) .field( \"bin_field\".to_string(), Value::Varbinary(b\"field_bin_val\".to_vec()), ) .build() .unwrap(); Add the point to write request: 1 2 let mut write_req = WriteRequest::default(); write_req.add_point(point); New rpc_ctx, and it can also be defined on demand or just use the default value, detail about rpc ctx:\nFinally, write to server by client.\n1 2 3 4 5 let rpc_ctx = RpcContext { database: Some(\"public\".to_string()), ..Default::default() }; let resp = client.write(rpc_ctx, \u0026write_req).await.expect(\"Should success to write\"); Sql Query We support to query data with sql.\nDefine related tables and sql in sql query request: 1 2 3 4 let req = SqlQueryRequest { tables: vec![table name 1,...,table name n], sql: sql string (e.g. select * from xxx), }; Query by client: 1 let resp = client.sql_query(rpc_ctx, \u0026req).await.expect(\"Should success to write\"); Example You can find the complete example in the project.\n","categories":"","description":"","excerpt":"Install 1 cargo add ceresdb-client You can get latest version here. …","ref":"/docs/user-guide/sdk/rust/","tags":"","title":"Rust"},{"body":"Rust 1 2 3 git clone https://github.com/apache/horaedb-client-rs cargo build Python Requirements python 3.7+ The Python SDK rely on Rust SDK, so cargo is also required, then install build tool maturin:\n1 pip install maturin Then we can build Python SDK:\n1 2 3 git clone https://github.com/apache/horaedb-client-py maturin build Go 1 2 3 git clone https://github.com/apache/horaedb-client-go go build ./... Java Requirements java 1.8 maven 3.6.3+ 1 2 3 git clone https://github.com/apache/horaedb-client-java mvn clean install -DskipTests=true -Dmaven.javadoc.skip=true -B -V ","categories":"","description":"","excerpt":"Rust 1 2 3 git clone https://github.com/apache/horaedb-client-rs cargo …","ref":"/docs/dev/sdk_develop/","tags":"","title":"SDK Development"},{"body":"基础语法 数据查询的基础语法如下：\n1 2 3 4 5 6 7 SELECT select_expr [, select_expr] ... FROM table_name [WHERE where_condition] [GROUP BY {col_name | expr} ... ] [ORDER BY {col_name | expr} [ASC | DESC] [LIMIT [offset,] row_count ] 数据查询的语法和 mysql 类似，示例如下：\n1 SELECT * FROM `demo` WHERE time_stamp \u003e '2022-10-11 00:00:00' AND time_stamp \u003c '2022-10-12 00:00:00' LIMIT 10 ","categories":"","description":"","excerpt":"基础语法 数据查询的基础语法如下：\n1 2 3 4 5 6 7 SELECT select_expr [, select_expr] ... …","ref":"/cn/docs/user-guide/sql/dml/select/","tags":"","title":"SELECT"},{"body":"Basic syntax Basic syntax (parts between [] are optional):\n1 2 3 4 5 6 7 SELECT select_expr [, select_expr] ... FROM table_name [WHERE where_condition] [GROUP BY {col_name | expr} ... ] [ORDER BY {col_name | expr} [ASC | DESC] [LIMIT [offset,] row_count ] Select syntax in HoraeDB is similar to mysql, here is an example:\n1 SELECT * FROM `demo` WHERE time_stamp \u003e '2022-10-11 00:00:00' AND time_stamp \u003c '2022-10-12 00:00:00' LIMIT 10 ","categories":"","description":"","excerpt":"Basic syntax Basic syntax (parts between [] are optional):\n1 2 3 4 5 6 …","ref":"/docs/user-guide/sql/dml/select/","tags":"","title":"SELECT"},{"body":"背景 在 集群 文章中介绍了 HoraeDB 的集群方案，简单总结一下就是：\n计算存储分离； 由中心化的元数据中心，管理整个集群； 然而计算存储分离的架构下，有个重要的问题在于，在集群调度的过程中，如何保证在共享的存储层中的数据不会因为不同的计算节点访问导致数据损坏，一个简单的例子就是如果同一块数据块被多个计算节点同时更新，可能就会出现数据损坏。\n而 HoraeDB 的解决方案是通过特定的机制，在共享存储的情况下达到了类似 Shared-Nothing 架构 的效果，也就是说存储层的数据经过一定规则的划分，可以保证在任何时刻最多只有一个 HoraeDB 实例可以对其进行更新，本文中，将这个特性定义成集群拓扑的正确性，如果这个正确性得到保证的话，那么数据就不会因为集群的灵活调度而受到损坏。\n本文对于 Shared Nothing 架构的优劣不做赘述，主要分享一下，HoraeDB 集群方案是如何在计算存储分离的方案下，达到 Shared Nothing 的效果（即如何保证 集群拓扑的正确性）。\n数据划分 为了达到 Shared Nothing 的效果，首先需要将数据在共享的存储层上面进行好逻辑和物理的划分。在 此前的集群介绍文章 中介绍了 Shard 的基本作用，作为集群的基本调度单元，同时也是数据分布的基本划分单元，不同的 Shard 在存储层对应的数据是隔离的：\n在 WAL 中，写入的 Table 数据会按照 Shard 组织起来，按照 Shard 写入到 WAL 的不同区域中，不同的 Shard 在 WAL 中的数据是隔离开的； 在 Object Storage 中，数据的管理是按照 Table 来划分的，而 Shard 和 Table 之间的关系是一对多的关系，也就说，任何一个 Table 只属于一个 Shard，因此在 Object Storage 中，Shard 之间的数据也是隔离的； Shard Lock 在数据划分好之后，需要保证的就是在任何时刻，同一时刻最多只有一个 HoraeDB 实例能够更新 Shard 的数据。那么要如何保证这一点的呢？很自然地，通过锁可以达到互斥的效果，不过在分布式集群中，我们需要的是分布式锁。通过分布式锁，每一个 Shard 被分配给 HoraeDB 实例时，HoraeDB 必须先获取到相应的 Shard Lock，才能完成 Shard 的打开操作，对应地，当 Shard 关闭后，HoraeDB 实例也需要主动释放 Shard Lock。\nHoraeDB 集群的元数据服务 HoraeMeta 是基于 ETCD 构建的，而基于 ETCD 实现分布式的 Shard Lock 是非常方便的，因此我们选择基于现有的 ETCD 实现 Shard Lock，具体逻辑如下：\n以 Shard ID 作为 ETCD 的 Key，获取到 Shard Lock 等价于创建出这个 Key； 对应的 Value，可以把 HoraeDB 的地址编码进去（用于 HoraeMeta 调度）； Shard Lock 获取到了之后，HoraeDB 实例需要通过 ETCD 提供的接口对其进行续租，保证 Shard Lock 不会被释放； HoraeMeta 暴露了 ETCD 的服务提供给 HoraeDB 集群来构建 Shard Lock，下图展示了 Shard Lock 的工作流程，图中的两个 HoraeDB 实例都尝试打开 Shard 1，但是由于 Shard Lock 的存在，最终只有一个 HoraeDB 实例可以完成 Shard 1 的打开：\n┌────────────────────┐ │ │ │ │ ├───────┐ │ ┌─────┬──▶│ ETCD │ │ │ │ └───────┴────HoraeMeta │ │ ▲ │ │ └──────┬─────┐ │ │ Rejected │ │ │ │ │ ┌─────┬─────┐ ┌─────┬─────┐ │Shard│Shard│ │Shard│Shard│ │ 0 │ 1 │ │ 1 │ 2 │ ├─────┴─────┤ ├─────┴─────┤ └─────HoraeDB └─────HoraeDB 其他方案 Shard Lock 的方案本质上是 HoraeDB 通过 ETCD 来保证在集群中对于任何一个 Shard 在任何时刻最多只有一个 HoraeDB 实例可以对其进行更新操作，也就是保证了在任何时刻 集群拓扑的正确性，需要注意，这个保证实际上成为了 HoraeDB 实例提供的能力（虽然是利用 ETCD 来实现的），而 HoraeMeta 无需保证这一点，下面的对比中，这一点是一个非常大的优势。\n除此 Shard Lock 的方案，我们还考虑过这样的两种方案：\nHoraeMeta 状态同步 HoraeMeta 规划并存储集群的拓扑状态，保证其正确性，并将这个正确的拓扑状态同步到 HoraeDB，而 HoraeDB 本身无权决定 Shard 是否可以打开，只有在得到 HoraeMeta 的通知后，才能打开指定的 Shard。此外，HoraeDB 需要不停地向 HoraeMeta 发送心跳，一方面汇报自身的负载信息，另一方面让 HoraeMeta 知道该节点仍然在线，用以计算最新的正确的拓扑状态。\n该方案也是 HoraeDB 一开始采用的方案，该方案的思路简洁，但是在实现过程中却是很难做好的，其难点在于，HoraeMeta 在执行调度的时候，需要基于最新的拓扑状态，决策出一个新的变更，并且应用到 HoraeDB 集群，但是这个变更到达某个具体的 HoraeDB 实例时，即将产生效果的时候，该方案无法简单地保证此刻的集群状态仍然是和做出该变更决策时基于的那个集群状态是一致的。\n让我们用更精确的语言描述一下：\nt0: 集群状态是 S0，HoraeMeta 据此计算出变更 U； t1: HoraeMeta 将 U 发送到某个 HoraeDB 实例，让其执行变更； t2: 集群状态变成 S1； t3: HoraeDB 接收到 U，准备进行变更； 上述的例子的问题在于，t3 时刻，HoraeDB 执行变更 U 是否正确呢？执行这个变更 U，是否会让数据遭到损坏？这个正确性，需要 HoraeMeta 完成相当复杂的逻辑来保证即使在集群状态为 S1 的情况下，执行 U 变更也不会出现问题，除此之外，状态的回滚也是一个非常麻烦的过程。\n举一个例子，就可以发现这个方案的处理比较麻烦：\nt0: HoraeMeta 尝试在 HoraeDB0 打开 Shard0，然而 HoraeDB0 打开 Shard0 遇到一些问题，Hang 住了，HoraeMeta 只能在超时之后，认为打开失败； t1: HoraeMeta 计算出新的拓扑结构，尝试在 HoraeDB1 继续打开 Shard0； t2: HoraeDB0 和 HoraeDB1 可能会同时打开 Shard0； 自然是有一些办法来避免掉 t2 时刻的事情，比如在 t0 时刻失败了之后，需要等待一个心跳周期，来获知 HoraeDB0 是否仍然在尝试打开 Shard0，来避免 t1 时刻发出的命令，但是这样的逻辑比较繁琐，难以维护。\n对比 Shard Lock 的方案，可以发现，该方案尝试获得更强的一致性，即尝试保证集群的拓扑状态在任何时刻都需要和 HoraeMeta 中的集群状态保持一致，显然，这样的一致性肯定是能够保证集群拓扑的正确性的，但也正因为如此实现起来才会更加复杂，而基于 Shard Lock 的方案放弃了这样一个已知的、正确的集群拓扑状态，转而只需要保证集群状态是正确的即可，而不需要知道这个状态究竟是什么。更重要的是，从另外一个角度来看，保证集群拓扑的正确性这部分逻辑和集群的调度完成了解耦，因此 HoraeMeta 的逻辑大大简化，只需要专注于完成负载均衡的集群调度工作即可，而集群拓扑的正确性由 HoraeDB 本身来保证。\nHoraeDB 提供一致性协议 该方案是参考 TiDB 的元数据服务 PD 的，PD 管理着所有的 TiKV 数据节点，但是 PD 不需要维护一致性的集群状态，并且应用到 TiKV 节点上面去，因为 TiKV 集群中，每一个 Raft Group，都能够达到一致性，也就是说 TiKV 无需借助 PD，本身就具备了让整个集群拓扑正确的能力（一个 Raft Group 不会出现两个 Leader）。\n参考该方案，实际上我们也可以在 HoraeDB 实例之间实现一致性协议，让其本身也具备这样的能力，不过在 HoraeDB 之间引入一致性协议，似乎把事情变得更加复杂了，而且目前也没有更多的数据需要同步，通过外部的服务（ETCD）依然可以同样的效果，从 HoraeMeta 看，就等价于 HoraeDB 本身获得了让集群一致的能力。\n因此 Shard Lock 的方案，可以看作是该方案的一个变种，是一种取巧但是很实用的实现。\n总结 HoraeDB 分布式方案的最终目标自然不是保证集群拓扑正确就够了，但是保持正确性是后续特性的重要基石，一旦这部分的逻辑简洁明了，有充分的理论保证，就可以让后续的特性实现也同样的优雅、简洁。例如，为了使得 HoraeDB 集群中的各个节点达到负载均衡的效果，HoraeMeta 就必须根据 HoraeDB 实例上报的消息，对集群中的节点进行调度，而调度的单位必然是 Shard，然而任何一次 Shard 的变动都可能造成数据的损坏（一个 Shard 被两个实例同时打开），在有了 Shard Lock 的保证后，HoraeMeta 就可以放心地生成调度计划，根据上报的负载信息，计算出当前状态的最佳调度结果，然后发送到涉及的 HoraeDB 实例让其执行，即使计算的前提可能是错误的（即集群状态已经和计算出调度结果时的状态不一样了），也不用担心集群拓扑的正确性遭到破坏，因而 HoraeMeta 的调度逻辑，变得简洁而优雅（只需要生成、执行，不需要考虑失败处理）。\n","categories":"","description":"","excerpt":"背景 在 集群 文章中介绍了 HoraeDB 的集群方案，简单总结一下就是：\n计算存储分离； 由中心化的元数据中心，管理整个集群； 然而计算 …","ref":"/cn/docs/design/shared_nothing/","tags":"","title":"Shared Nothing 架构"},{"body":"Tables in HoraeDB have the following constraints:\nPrimary key is required The primary key must contain a time column, and can only contain one time column The primary key must be non-null, so all columns in primary key must be non-null. Timestamp Column Tables in HoraeDB must have one timestamp column maps to timestamp in timeseries data, such as timestamp in OpenTSDB/Prometheus. The timestamp column can be set with timestamp key keyword, like TIMESTAMP KEY(ts).\nTag Column Tag is use to defined column as tag column, similar to tag in timeseries data, such as tag in OpenTSDB and label in Prometheus.\nPrimary Key The primary key is used for data deduplication and sorting. The primary key is composed of some columns and one time column. The primary key can be set in the following some ways：\nuse primary key keyword use tag to auto generate TSID, HoraeDB will use (TSID,timestamp) as primary key only set Timestamp column, HoraeDB will use (timestamp) as primary key Notice: If the primary key and tag are specified at the same time, then the tag column is just an additional information identification and will not affect the logic.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 CREATE TABLE with_primary_key( ts TIMESTAMP NOT NULL, c1 STRING NOT NULL, c2 STRING NULL, c4 STRING NULL, c5 STRING NULL, TIMESTAMP KEY(ts), PRIMARY KEY(c1, ts) ) ENGINE=Analytic WITH (ttl='7d'); CREATE TABLE with_tag( ts TIMESTAMP NOT NULL, c1 STRING TAG NOT NULL, c2 STRING TAG NULL, c3 STRING TAG NULL, c4 DOUBLE NULL, c5 STRING NULL, c6 STRING NULL, TIMESTAMP KEY(ts) ) ENGINE=Analytic WITH (ttl='7d'); CREATE TABLE with_timestamp( ts TIMESTAMP NOT NULL, c1 STRING NOT NULL, c2 STRING NULL, c3 STRING NULL, c4 DOUBLE NULL, c5 STRING NULL, c6 STRING NULL, TIMESTAMP KEY(ts) ) ENGINE=Analytic WITH (ttl='7d'); TSID If primary keyis not set, and tag columns is provided, TSID will auto generated from hash of tag columns. In essence, this is also a mechanism for automatically generating id.\n","categories":"","description":"","excerpt":"Tables in HoraeDB have the following constraints:\nPrimary key is …","ref":"/docs/user-guide/sql/model/special_columns/","tags":"","title":"Special Columns"},{"body":"The storage engine mainly provides the following two functions：\nPersistence of data Under the premise of ensuring the correctness of the data, organize the data in the most reasonable way to meet the query needs of different scenarios. This document will introduce the internal implementation of the storage engine in HoraeDB. Readers can refer to the content here to explore how to use HoraeDB efficiently.\nOverall Structure HoraeDB is a distributed storage system based on the share-nothing architecture.\nData between different servers is isolated from each other and does not affect each other. The storage engine in each stand-alone machine is a variant of log-structured merge-tree, which is optimized for time-series scenarios. The following figure shows its core components:\nWrite Ahead Log (WAL) A write request will be written to\nmemtable in memory WAL in durable storage Since memtable is not persisted to the underlying storage system in real time, so WAL is required to ensure the reliability of the data in memtable.\nOn the other hand, due to the design of the distributed architecture, WAL itself is required to be highly available. Now there are following implementations in HoraeDB:\nLocal disk (based on RocksDB, no distributed high availability) OceanBase Kafka Memtable Memtable is a memory data structure used to hold recently written table data. Different tables have its corresponding memtable.\nMemtable is read-write by default (aka active), and when the write reaches some threshold, it will become read-only and be replaced by a new memtable.\nThe read-only memtable will be flushed to the underlying storage system in SST format by background thread. After flush is completed, the read-only memtable can be destroyed, and the corresponding data in WAL can also be deleted.\nSorted String Table（SST） SST is a persistent format for data, which is stored in the order of primary keys of table. Currently, HoraeDB uses parquet format for this.\nFor HoraeDB, SST has an important option: segment_duration, only SST within the same segment can be merged, which is benefical for time-series data. And it is also convenient to eliminate expired data.\nIn addition to storing the original data, the statistical information of the data will also be stored in the SST to speed up the query, such as the maximum value, the minimum value, etc.\nCompactor Compactor can merge multiple small SST files into one, which is used to solve the problem of too many small files. In addition, Compactor will also delete expired data and duplicate data during the compaction. In future, compaction maybe add more task, such as downsample.\nThe current compaction strategy in HoraeDB reference Cassandra:\nSizeTieredCompactionStrategy TimeWindowCompactionStrategy Manifest Manifest records metadata of table, SST file, such as: the minimum and maximum timestamps of the data in an SST.\nDue to the design of the distributed architecture, the manifest itself is required to be highly available. Now in HoraeDB, there are mainly the following implementations：\nWAL ObjectStore ObjectStore ObjectStore is place where data (i.e. SST) is persisted.\nGenerally speaking major cloud vendors should provide corresponding services, such as Alibaba Cloud’s OSS and AWS’s S3.\n","categories":"","description":"","excerpt":"The storage engine mainly provides the following two functions： …","ref":"/docs/design/storage/","tags":"","title":"Storage"},{"body":"Query Table Information Like Mysql’s information_schema.tables, HoraeDB provides system.public.tables to save tables information. Columns:\ntimestamp([TimeStamp]) catalog([String]) schema([String]) table_name([String]) table_id([Uint64]) engine([String]) Example Query table information via table_name like this:\n1 2 3 4 5 curl --location --request POST 'http://localhost:5000/sql' \\ --header 'Content-Type: application/json' \\ -d '{ \"query\": \"select * from system.public.tables where `table_name`=\\\"my_table\\\"\" }' Response 1 2 3 4 5 6 7 8 9 10 11 { \"rows\":[ { \"timestamp\":0, \"catalog\":\"horaedb\", \"schema\":\"public\", \"table_name\":\"my_table\", \"table_id\":3298534886446, \"engine\":\"Analytic\" } } ","categories":"","description":"","excerpt":"Query Table Information Like Mysql’s information_schema.tables, …","ref":"/docs/user-guide/operation/system_table/","tags":"","title":"Table Operation"},{"body":"HoraeDB supports standard SQL protocols and allows you to create tables and read/write data via http requests. More SQL\nCreate Table Example 1 2 3 4 5 curl --location --request POST 'http://127.0.0.1:5000/sql' \\ --header 'Content-Type: application/json' \\ -d '{ \"query\": \"CREATE TABLE `demo` (`name` string TAG, `value` double NOT NULL, `t` timestamp NOT NULL, TIMESTAMP KEY(t)) ENGINE=Analytic with (enable_ttl='\\''false'\\'')\" }' Write Data Example 1 2 3 4 5 curl --location --request POST 'http://127.0.0.1:5000/sql' \\ --header 'Content-Type: application/json' \\ -d '{ \"query\": \"INSERT INTO demo(t, name, value) VALUES(1651737067000, '\\''horaedb'\\'', 100)\" }' Read Data Example 1 2 3 4 5 curl --location --request POST 'http://127.0.0.1:5000/sql' \\ --header 'Content-Type: application/json' \\ -d '{ \"query\": \"select * from demo\" }' Query Table Info Example 1 2 3 4 5 curl --location --request POST 'http://127.0.0.1:5000/sql' \\ --header 'Content-Type: application/json' \\ -d '{ \"query\": \"show create table demo\" }' Drop Table Example 1 2 3 4 5 curl --location --request POST 'http://127.0.0.1:5000/sql' \\ --header 'Content-Type: application/json' \\ -d '{ \"query\": \"DROP TABLE demo\" }' Route Table Example 1 curl --location --request GET 'http://127.0.0.1:5000/route/{table_name}' ","categories":"","description":"","excerpt":"HoraeDB supports standard SQL protocols and allows you to create …","ref":"/docs/user-guide/operation/table/","tags":"","title":"Table Operation"},{"body":"Note: This feature is still in development, and the API may change in the future.\nThis chapter discusses PartitionTable.\nThe partition table syntax used by HoraeDB is similar to that of MySQL.\nGeneral partition tables include Range Partitioning, List Partitoning, Hash Partitioning, and Key Partititioning.\nHoraeDB currently only supports Key Partitioning.\nArchitecture Similar to MySQL, different portions of a partition table are stored as separate tables in different locations.\nCurrently designed, a partition table can be opened on multiple HoraeDB nodes, supports writing and querying at the same time, and can be expanded horizontally.\nAs shown in the figure below, PartitionTable is opened on node0 and node1, and the physical subtables where the actual data are stored on node2 and node3.\n┌───────────────────────┐ ┌───────────────────────┐ │Node0 │ │Node1 │ │ ┌────────────────┐ │ │ ┌────────────────┐ │ │ │ PartitionTable │ │ │ │ PartitionTable │ │ │ └────────────────┘ │ │ └────────────────┘ │ │ │ │ │ │ │ └────────────┼──────────┘ └───────────┼───────────┘ │ │ │ │ ┌───────────────────────┼─────────────────────────────┼───────────────────────┐ │ │ │ │ ┌────────────┼───────────────────────┼─────────────┐ ┌─────────────┼───────────────────────┼────────────┐ │Node2 │ │ │ │Node3 │ │ │ │ ▼ ▼ │ │ ▼ ▼ │ │ ┌─────────────────────┐ ┌─────────────────────┐ │ │ ┌─────────────────────┐ ┌─────────────────────┐ │ │ │ │ │ │ │ │ │ │ │ │ │ │ │ SubTable_0 │ │ SubTable_1 │ │ │ │ SubTable_2 │ │ SubTable_3 │ │ │ │ │ │ │ │ │ │ │ │ │ │ │ └─────────────────────┘ └─────────────────────┘ │ │ └─────────────────────┘ └─────────────────────┘ │ │ │ │ │ └──────────────────────────────────────────────────┘ └──────────────────────────────────────────────────┘ Key Partitioning Key Partitioning supports one or more column calculations, using the hash algorithm provided by HoraeDB for calculations.\nUse restrictions:\nOnly tag column is supported as partition key. LINEAR KEY is not supported yet. The table creation statement for the key partitioning is as follows:\n1 2 3 4 5 6 7 CREATE TABLE `demo`( `name`string TAG, `id` int TAG, `value` double NOT NULL, `t` timestamp NOT NULL, TIMESTAMP KEY(t) ) PARTITION BY KEY(name) PARTITIONS 2 ENGINE = Analytic Refer to MySQL key partitioning.\nQuery Since the partition table data is actually stored in different physical tables, it is necessary to calculate the actual requested physical table according to the query request when querying.\nThe query will calculate the physical table to be queried according to the query parameters, and then remotely request the node where the physical table is located to obtain data through the HoraeDB internal service remote engine (support predicate pushdown).\nThe implementation of the partition table is in PartitionTableImpl.\nStep 1: Parse query sql and calculate the physical table to be queried according to the query parameters. Step 2: Query data of physical table. Step 3: Compute with the raw data. │ 1 │ │ ▼ ┌───────────────┐ │Node0 │ │ │ │ │ └───────────────┘ ┬ 2 │ 2 ┌──────────────┴──────────────┐ │ ▲ │ │ 3 │ 3 │ ▼ ─────────────┴───────────── ▼ ┌───────────────┐ ┌───────────────┐ │Node1 │ │Node2 │ │ │ │ │ │ │ │ │ └───────────────┘ └───────────────┘ Key partitioning Filters like and, or, in, = will choose specific SubTables. Fuzzy matching filters like \u003c, \u003e are also supported, but may have poor performance since it will scan all physical tables. Key partitioning rule is implemented in KeyRule.\nWrite The write process is similar to the query process.\nFirst, according to the partition rules, the write request is split into different partitioned physical tables, and then sent to different physical nodes through the remote engine for actual data writing.\n","categories":"","description":"","excerpt":"Note: This feature is still in development, and the API may change in …","ref":"/docs/design/table_partitioning/","tags":"","title":"Table Partitioning"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/","tags":"","title":"Tags"},{"body":" WAL on RocksDB WAL on Kafka WAL on OceanBase will be introduced in later. ","categories":"","description":"","excerpt":" WAL on RocksDB WAL on Kafka WAL on OceanBase will be introduced in …","ref":"/docs/design/wal/","tags":"","title":"Wal"},{"body":"Architecture This section introduces the implementation of a standalone Write-Ahead Log (WAL, hereinafter referred to as “the log”) based on a local disk. In this implementation, the log is managed at the region level.\n┌────────────────────────────┐ │ HoraeDB │ │ │ │ ┌────────────────────────┐ │ │ │ WAL │ │ ┌────────────────────────┐ │ │ │ │ │ │ │ │ ...... │ │ │ File System │ │ │ │ │ │ │ │ │ ┌────────────────────┐ │ │ manage │ ┌────────────────────┐ │ Write ─────┼─┼─► Region ├─┼─┼─────────┼─► Region Dir │ │ │ │ │ │ │ │ │ │ │ │ Read ─────┼─┼─► ┌────────────┐ │ │ │ mmap │ │ ┌────────────────┐ │ │ │ │ │ │ Segment 0 ├───┼─┼─┼─────────┼─┼─► Segment File 0 │ │ │ │ │ │ └────────────┘ │ │ │ │ │ └────────────────┘ │ │ Delete ─────┼─┼─► ┌────────────┐ │ │ │ mmap │ │ ┌────────────────┐ │ │ │ │ │ │ Segment 1 ├───┼─┼─┼─────────┼─┼─► Segment File 1 │ │ │ │ │ │ └────────────┘ │ │ │ │ │ └────────────────┘ │ │ │ │ │ ┌────────────┐ │ │ │ mmap │ │ ┌────────────────┐ │ │ │ │ │ │ Segment 2 ├───┼─┼─┼─────────┼─┼─► Segment File 2 │ │ │ │ │ │ └────────────┘ │ │ │ │ │ └────────────────┘ │ │ │ │ │ ...... │ │ │ │ │ ...... │ │ │ │ └────────────────────┘ │ │ │ └────────────────────┘ │ │ │ ...... │ │ │ ...... │ │ └────────────────────────┘ │ └────────────────────────┘ └────────────────────────────┘ Data Model File Paths Each region has its own directory to manage all segments for that region. The directory is named after the region’s ID. Each segment is named using the format seg_\u003cid\u003e, with IDs starting from 0 and incrementing.\nSegment Format Logs for all tables within a region are stored in segments, arranged in ascending order of sequence numbers. The structure of the segment files is as follows:\nSegment0 Segment1 ┌────────────┐ ┌────────────┐ │ Magic Num │ │ Magic Num │ ├────────────┤ ├────────────┤ │ Record │ │ Record │ ├────────────┤ ├────────────┤ │ Record │ │ Record │ ├────────────┤ ├────────────┤ .... │ Record │ │ Record │ ├────────────┤ ├────────────┤ │ ... │ │ ... │ │ │ │ │ └────────────┘ └────────────┘ seg_0 seg_1 In memory, each segment stores additional information used for read, write, and delete operations:\n1 2 3 4 5 6 7 8 9 10 pub struct Segment { /// A hashmap storing both min and max sequence numbers of records within /// this segment for each `TableId`. table_ranges: HashMap\u003cTableId, (SequenceNumber, SequenceNumber)\u003e, /// An optional vector of positions within the segment. record_position: Vec\u003cPosition\u003e, ... } Log Format The log format within a segment is as follows:\n+---------+--------+------------+--------------+--------------+-------+ | version | crc | table id | sequence num | value length | value | | (u8) | (u32) | (u64) | (u64) | (u32) |(bytes)| +---------+--------+------------+--------------+--------------+-------+ Field Descriptions:\nversion: Log version number.\ncrc: Used to ensure data consistency. Computes the CRC checksum from the table id to the end of the record.\ntable id: The unique identifier of the table.\nsequence num: The sequence number of the record.\nvalue length: The byte length of the value.\nvalue: The value in the general log format.\nThe region ID is not stored in the log because it can be obtained from the file path.\nMain Processes Opening the WAL Identify all region directories under the WAL directory.\nIn each region directory, identify all segment files.\nOpen each segment file, traverse all logs within it, record the start and end offsets of each log, and record the minimum and maximum sequence numbers of each TableId in the segment, then close the file.\nIf there is no region directory or there are no segment files under the directory, automatically create the corresponding directory and files.\nReading Logs Based on the metadata of the segments, determine all segments involved in the current read operation.\nOpen these segments in order of their IDs from smallest to largest, and decode the raw bytes into logs.\nWriting Logs Serialize the logs to be written into byte data and append them to the segment file with the largest ID.\nWhen a segment is created, it pre-allocates a fixed size of 64MB and will not change dynamically. When the pre-allocated space is used up, a new segment is created, and appending continues in the new segment.\nAfter each append, flush is not called immediately; by default, flush is performed every ten writes or when the segment file is closed.\nUpdate the segment’s metadata table_ranges in memory.\nDeleting Logs Suppose logs in the table with ID table_id and sequence numbers less than seq_num need to be marked as deleted:\nUpdate the table_ranges field of the relevant segments in memory, updating the minimum sequence number of the table to seq_num + 1.\nIf after modification, the minimum sequence number of the table in this segment is greater than the maximum sequence number, remove the table from table_ranges.\nIf a segment’s table_ranges is empty and it is not the segment with the largest ID, delete the segment file.\n","categories":"","description":"","excerpt":"Architecture This section introduces the implementation of a …","ref":"/docs/design/wal_on_disk/","tags":"","title":"WAL on Disk"},{"body":"Architecture In this section we present a distributed WAL implementation(based on Kafka). Write-ahead logs(hereinafter referred to as logs) of tables are managed here by region, which can be simply understood as a shared log file of multiple tables.\nAs shown in the following figure, regions are mapped to topics(with only one partition) in Kafka. And usually two topics are needed by a region, one is used for storing logs and the other is used for storing metadata.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 ┌──────────────────────────┐ │ Kafka │ │ │ │ ...... │ │ │ │ ┌─────────────────────┐ │ │ │ Meta Topic │ │ │ │ │ │ Delete │ │ ┌─────────────────┐ │ │ ┌──────────────────────┐ ┌───────┼─┼─► Partition │ │ │ │ HoraeDB │ │ │ │ │ │ │ │ │ │ │ │ │ └─────────────────┘ │ │ │ ┌──────────────────┐ │ │ │ │ │ │ │ │ WAL │ │ │ │ └─────────────────────┘ │ │ │ ...... │ │ │ │ │ │ │ ┌──────────────┐ │ │ │ │ ┌──────────────────────┐ │ │ │ │ Region │ │ │ │ │ │ Data Topic │ │ │ │ │ ├─┼─┼──┘ │ │ │ │ | | | ┌──────────┐ │ │ │ │ │ ┌──────────────────┐ │ │ │ │ │ │ Metadata │ │ │ │ │ │ │ Partition │ │ │ │ │ │ └──────────┘ │ │ │ Write │ │ │ │ │ │ Write ─────────┼─┼─► ├─┼─┼───┐ │ │ │ ┌──┬──┬──┬──┬──┐ │ │ │ │ │ │ ┌──────────┐ │ │ │ └──────┼─┼─┼─► │ │ │ │ ├─┼─┼─┼────┐ │ │ │ │ Client │ │ │ │ │ │ │ └──┴──┴──┴──┴──┘ │ │ │ │ Read ◄─────────┼─┼─┤ └──────────┘ │ │ │ │ │ │ │ │ │ │ │ │ │ │ │ │ │ │ └──────────────────┘ │ │ │ │ │ └──▲───────────┘ │ │ │ │ │ │ │ │ │ │ ...... │ │ │ └──────────────────────┘ │ │ │ └────┼─────────────┘ │ │ ...... │ │ │ │ │ └──────────────────────────┘ │ └──────┼───────────────┘ │ │ │ │ │ │ Read │ └──────────────────────────────────────────────────────────┘ Data Model Log Format The common log format described in WAL on RocksDB is used here.\nMetadata Each region will maintain its metadata both in memory and in Kafka, we call it RegionMeta here. It can be thought of as a map, taking table id as a key and TableMeta as a value. We briefly introduce the variables in TableMeta here:\nnext_seq_num, the sequence number allocated to the next log entry. latest_marked_deleted, the last flushed sequence number, all logs in the table with a lower sequence number than it can be removed. current_high_watermark, the high watermark in the Kafka partition after the last writing of this table. seq_offset_mapping, mapping from sequence numbers to offsets will be done on every write and will removed to the updated latest_marked_deleted after flushing. ┌─────────────────────────────────────────┐ │ RegionMeta │ │ │ │ Map\u003cTableId, TableMeta\u003e table_metas │ └─────────────────┬───────────────────────┘ │ │ │ └─────┐ │ │ ┌──────────────────────┴──────────────────────────────┐ │ TableMeta │ │ │ │ SequenceNumber next_seq_num │ │ │ │ SequenceNumber latest_mark_deleted │ │ │ │ KafkaOffset high_watermark │ │ │ │ Map\u003cSequenceNumber, KafkaOffset\u003e seq_offset_mapping │ └─────────────────────────────────────────────────────┘ Main Process We focus on the main process in one region, following process will be introduced:\nOpen or create region. Write and read logs. Delete logs. Open or Create Region Steps Search the region in the opened namespace. If the region found, the most important thing we need to do is to recover its metadata, we will introduce this later. If the region not found and auto creating is defined, just create the corresponding topic in Kafka. Add the found or created region to cache, return it afterwards. Recovery As mentioned above, the RegionMeta is actually a map of the TableMeta. So here we will focus on recovering a specific TableMeta, and examples will be given to better illustrate this process.\nFirst, recover the RegionMeta from snapshot. We will take a snapshot of the RegionMeta in some scenarios (e.g. mark logs deleted, clean logs) and put it to the meta topic. The snapshot is actually the RegionMeta at a particular point in time. When recovering a region, we can use it to avoid scanning all logs in the data topic. The following is the example, we recover from the snapshot taken at the time when Kafka high watermark is 64: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 high watermark in snapshot: 64 ┌──────────────────────────────┐ │ RegionMeta │ │ │ │ ...... │ │ ┌──────────────────────────┐ │ │ │ TableMeta │ │ │ │ │ │ │ │ next_seq_num: 5 │ │ │ │ │ │ │ │ latest_mark_deleted: 2 │ │ │ │ │ │ │ │ high_watermark: 32 │ │ │ │ │ │ │ │ seq_offset_mapping: │ │ │ │ │ │ │ │ (2, 16) (3, 16) (4, 31) │ │ │ └──────────────────────────┘ │ │ ...... │ └──────────────────────────────┘ Recovering from logs. After recovering from snapshot, we can continue to recover by scanning logs in data topic from the Kafka high watermark when snapshot is taken, and obviously that avoid scanning the whole data topic. Let’s see the example: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 ┌────────────────────────────────────┐ │ │ │ high_watermark in snapshot: 64 │ │ │ │ ┌──────────────────────────────┐ │ │ │ RegionMeta │ │ │ │ │ │ │ │ ...... │ │ │ │ ┌──────────────────────────┐ │ │ │ │ │ TableMeta │ │ │ │ │ │ │ │ │ │ │ │ next_seq_num: 5 │ │ │ ┌────────────────────────────────┐ │ │ │ │ │ │ │ RegionMeta │ │ │ │ latest_mark_deleted: 2 │ │ │ │ │ │ │ │ │ │ │ │ ...... │ │ │ │ high_watermark: 32 │ │ │ │ ┌────────────────────────────┐ │ │ │ │ │ │ │ │ │ TableMeta │ │ │ │ │ seq_offset_mapping: │ │ │ │ │ │ │ │ │ │ │ │ │ │ │ next_seq_num: 8 │ │ │ │ │ (2, 16) (3, 16) (4, 31) │ │ │ │ │ │ │ │ │ └──────────────────────────┘ │ │ │ │ latest_mark_deleted: 2 │ │ │ │ ...... │ │ │ │ │ │ │ └──────────────────────────────┘ ├──────────────────► │ high_watermark: 32 │ │ │ │ │ │ │ │ │ ┌────────────────────────────────┐ │ │ │ seq_offset_mapping: │ │ │ │ Data topic │ │ │ │ │ │ │ │ │ │ │ │ (2, 16) (3, 16) (4, 31) │ │ │ │ ┌────────────────────────────┐ │ │ │ │ │ │ │ │ │ Partition │ │ │ │ │ (5, 72) (6, 81) (7, 90) │ │ │ │ │ │ │ │ │ │ │ │ │ │ │ ┌────┬────┬────┬────┬────┐ │ │ │ │ └────────────────────────────┘ │ │ │ │ │ 64 │ 65 │ ...│ 99 │100 │ │ │ │ │ ...... │ │ │ │ └────┴────┴────┴────┴────┘ │ │ │ └────────────────────────────────┘ │ │ │ │ │ │ │ │ └────────────────────────────┘ │ │ │ │ │ │ │ └────────────────────────────────┘ │ │ │ └────────────────────────────────────┘ Write and Read Logs The writing and reading process in a region is simple.\nFor writing:\nOpen the specified region (auto create it if necessary). Put the logs to specified Kafka partition by client. Update next_seq_num, current_high_watermark and seq_offset_mapping in corresponding TableMeta. For reading:\nOpen the specified region. Just read all the logs of the region, and the split and replay work will be done by the caller. Delete Logs Log deletion can be divided into two steps:\nMark the logs deleted. Do delayed cleaning work periodically in a background thread. Mark Update latest_mark_deleted and seq_offset_mapping(just retain the entries whose’s sequence \u003e= updated latest_mark_deleted) in TableMeta. Maybe we need to make and sync the RegionMeta snapshot to Kafka while dropping table. Clean The cleaning logic done in a background thread called cleaner:\nMake RegionMeta snapshot. Decide whether to clean the logs based on the snapshot. If so, sync the snapshot to Kafka first, then clean the logs. ","categories":"","description":"","excerpt":"Architecture In this section we present a distributed WAL …","ref":"/docs/design/wal_on_kafka/","tags":"","title":"WAL on Kafka"},{"body":"Architecture In this section we present a standalone WAL implementation (based on RocksDB). Write-ahead logs(hereinafter referred to as logs) of tables are managed here by table, and we call the corresponding storage data structure TableUnit. All related data (logs or some metadata) is stored in a single column family for simplicity.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 ┌───────────────────────────────┐ │ HoraeDB │ │ │ │ ┌─────────────────────┐ │ │ │ WAL │ │ │ │ │ │ │ │ ...... │ │ │ │ │ │ │ │ ┌────────────────┐ │ │ Write ─────┼─┼──► TableUnit │ │Delete │ │ │ │ │ ◄────── │ Read ─────┼─┼──► ┌────────────┐ │ │ │ │ │ │ │ RocksDBRef │ │ │ │ │ │ │ └────────────┘ │ │ │ │ │ │ | | | │ │ └────────────────┘ │ │ │ │ ...... │ │ │ └─────────────────────┘ │ │ │ └───────────────────────────────┘ Data Model Common Log Format We use the common key and value format here. Here is the defined key format, and the following is introduction for fields in it:\nnamespace: multiple instances of WAL can exist for different purposes (e.g. manifest also needs wal). The namespace is used to distinguish them. region_id: in some WAL implementations we may need to manage logs from multiple tables, region is the concept to describe such a set of table logs. Obviously the region id is the identification of the region. table_id: identification of the table logs to which they belong. sequence_num: each login table can be assigned an identifier, called a sequence number here. version: for compatibility with old and new formats. 1 2 3 +---------------+----------------+-------------------+--------------------+-------------+ | namespace(u8) | region_id(u64) | table_id(u64) | sequence_num(u64) | version(u8) | +---------------+----------------+-------------------+--------------------+-------------+ Here is the defined value format, version is the same as the key format, payload can be understood as encoded log content.\n1 2 3 +-------------+----------+ | version(u8) | payload | +-------------+----------+ Metadata The metadata here is stored in the same key-value format as the log. Actually only the last flushed sequence is stored in this implementation. Here is the defined metadata key format and field instructions:\nnamespace, table_id, version are the same as the log format. key_type, used to define the type of metadata. MaxSeq now defines that metadata of this type will only record the most recently flushed sequence in the table. Because it is only used in wal on RocksDB, which manages the logs at table level, so there is no region id in this key. 1 2 3 +---------------+--------------+----------------+-------------+ | namespace(u8) | key_type(u8) | table_id(u64) | version(u8) | +---------------+--------------+----------------+-------------+ Here is the defined metadata value format, as you can see, just the version and max_seq(flushed sequence) in it:\n1 2 3 +-------------+--------------+ | version(u8) | max_seq(u64) | +-------------+--------------+ Main Process Open TableUnit: Read the latest log entry of all tables to recover the next sequence numbers of tables mainly. Scan the metadata to recover next sequence num as a supplement (because some table has just triggered flush and no new written logs after this, so no logs exists now). Write and read logs. Just write and read key-value from RocksDB. Delete logs. For simplicity It will remove corresponding logs synchronously. ","categories":"","description":"","excerpt":"Architecture In this section we present a standalone WAL …","ref":"/docs/design/wal_on_rocksdb/","tags":"","title":"WAL on RocksDB"},{"body":"本文展示如何部署一个由 HoraeMeta 控制的 HoraeDB 集群，有了 HoraeMeta 提供的服务，如果 HoraeDB 使用存储不在本地的话，就可以实现很多分布式特性，比如水平扩容、负载均衡、服务高可用等。\n部署 HoraeMeta HoraeMeta 是 HoraeDB 分布式模式的核心服务之一，用于管理 HoraeDB 节点的调度，为 HoraeDB 集群提供高可用、负载均衡、集群管控等能力。 HoraeMeta 本身通过嵌入式的 ETCD 保障高可用。此外，ETCD 的服务也被暴露给 HoraeDB 用于实现分布式锁使用。\n编译打包 安装 Golang，版本号 \u003e= 1.19。 在项目根目录下使用 make build 进行编译打包。 部署方式 启动配置 目前 HoraeMeta 支持以配置文件和环境变量两种方式来指定服务启动配置。我们提供了配置文件方式启动的示例，具体可以参考 config。 环境变量的配置优先级高于配置文件，当同时存在时，以环境变量为准。\n动态拓扑和静态拓扑 即使使用了 HoraeMeta 来部署 HoraeDB 集群，也可以选择静态拓扑或动态拓扑。对于静态拓扑，表的分布在集群初始化后是静态的，而对于动态拓扑，表可以在不同的 HoraeDB 节点之间进行动态迁移以达到负载平衡或者 failover 的目的。但是动态拓扑只有在 HoraeDB 节点使用的存储是非本地的情况下才能启用，否则会因为表的数据是持久化在本地，当表转移到不同的 HoraeDB 节点时会导致数据损坏。\n目前，HoraeMeta 默认关闭集群拓扑的动态调度，并且在本篇指南中，这个选项也不会被开启，因为指南中的例子采用的是本地存储。如果要启用动态调度，可以将 TOPOLOGY_TYPE 设置为 dynamic（默认为 static），之后负载均衡和 failover 将会起作用。但是需要注意的是，如果底层存储是本地磁盘，则不要启用它。\n此外对于静态拓扑，参数 DEFAULT_CLUSTER_NODE_COUNT 表示已部署集群中 HoraeDB 节点的数量，应该被设置为 HoraeDB 服务器的实际机器数，这个参数非常重要，因为集群初始化完毕之后，HoraeDB 集群将无法再增减机器。\n启动实例 HoraeMeta 基于 etcd 实现高可用，在线上环境我们一般部署多个节点，但是在本地环境和测试时，可以直接部署单个节点来简化整个部署流程。\n单节点 1 2 3 docker run -d --name horaemeta-server \\ -p 2379:2379 \\ ghcr.io/apache/horaemeta-server:nightly-20231225-ab067bf0 多节点 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 wget https://horaedb.apache.org/config-horaemeta-cluster0.toml docker run -d --network=host --name horaemeta-server0 \\ -v $(pwd)/config-horaemeta-cluster0.toml:/etc/horaemeta/horaemeta.toml \\ ghcr.io/apache/horaemeta-server:nightly-20231225-ab067bf0 wget https://horaedb.apache.org/config-horaemeta-cluster1.toml docker run -d --network=host --name horaemeta-server1 \\ -v $(pwd)/config-horaemeta-cluster1.toml:/etc/horaemeta/horaemeta.toml \\ ghcr.io/apache/horaemeta-server:nightly-20231225-ab067bf0 wget https://horaedb.apache.org/config-horaemeta-cluster2.toml docker run -d --network=host --name horaemeta-server2 \\ -v $(pwd)/config-horaemeta-cluster2.toml:/etc/horaemeta/horaemeta.toml \\ ghcr.io/apache/horaemeta-server:nightly-20231225-ab067bf0 如果 HoraeDB 底层采用的是远程存储，可以环境变量来开启动态调度：只需将 -e ENABLE_SCHEDULE=true 加入到 docker run 命令中去。\n部署 HoraeDB 在 NoMeta 模式中，由于 HoraeDB 集群拓扑是静态的，因此 HoraeDB 只需要一个本地存储来作为底层的存储层即可。但是在 WithMeta 模式中，集群的拓扑是可以变化的，因此如果 HoraeDB 的底层存储使用一个独立的存储服务的话，HoraeDB 集群就可以获得分布式系统的一些特性：高可用、负载均衡、水平扩展等。 当然，HoraeDB 仍然可以使用本地存储，这样的话，集群的拓扑仍然是静态的。\n存储相关的配置主要包括两个部分：\nObject Storage WAL Storage 注意：在生产环境中如果我们把 HoraeDB 部署在多个节点上时，请按照如下方式把机器的网络地址设置到环境变量中：\n1 export HORAEDB_SERVER_ADDR=\"{server_addr}:8831\" 注意，此网络地址用于 HoraeMeta 和 HoraeDB 通信使用，需保证网络联通可用。\nObject Storage 本地存储 类似 NoMeta 模式，我们仍然可以为 HoraeDB 配置一个本地磁盘作为底层存储：\n1 2 3 [analytic.storage.object_store] type = \"Local\" data_dir = \"/home/admin/data/horaedb\" OSS Aliyun OSS 也可以作为 HoraeDB 的底层存储，以此提供数据容灾能力。下面是一个配置示例，示例中的模版变量需要被替换成实际的 OSS 参数才可以真正的使用：\n1 2 3 4 5 6 7 [analytic.storage.object_store] type = \"Aliyun\" key_id = \"{key_id}\" key_secret = \"{key_secret}\" endpoint = \"{endpoint}\" bucket = \"{bucket}\" prefix = \"{data_dir}\" S3 Amazon S3 也可以作为 HoraeDB 的底层存储，下面是一个配置示例，示例中的模版变量需要被替换成实际的 S3 参数才可以真正的使用：\n1 2 3 4 5 6 7 8 [analytic.storage.object_store] type = \"S3\" region = \"{region}\" key_id = \"{key_id}\" key_secret = \"{key_secret}\" endpoint = \"{endpoint}\" bucket = \"{bucket}\" prefix = \"{prefix}\" WAL Storage RocksDB 基于 RocksDB 的 WAL 也是一种本地存储，无第三方依赖，可以很方便的快速部署：\n1 2 3 [analytic.wal] type = \"RocksDB\" data_dir = \"/home/admin/data/horaedb\" OceanBase 如果已经有了一个部署好的 OceanBase 集群的话，HoraeDB 可以使用它作为 WAL Storage 来保证其数据的容灾性。下面是一个配置示例，示例中的模版变量需要被替换成实际的 OceanBase 集群的参数才可以真正的使用：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 [analytic.wal] type = \"Obkv\" [analytic.wal.data_namespace] ttl = \"365d\" [analytic.wal.obkv] full_user_name = \"{full_user_name}\" param_url = \"{param_url}\" password = \"{password}\" [analytic.wal.obkv.client] sys_user_name = \"{sys_user_name}\" sys_password = \"{sys_password}\" Kafka 如果你已经部署了一个 Kafka 集群，HoraeDB 可以也可以使用它作为 WAL Storage。下面是一个配置示例，示例中的模版变量需要被替换成实际的 Kafka 集群的参数才可以真正的使用：\n1 2 3 4 5 [analytic.wal] type = \"Kafka\" [analytic.wal.kafka.client] boost_broker = \"{boost_broker}\" Meta 客户端配置 除了存储层的配置外，HoraeDB 需要 HoraeMeta 相关的配置来与 HoraeMeta 集群进行通信：\n[cluster.meta_client] cluster_name = 'defaultCluster' meta_addr = 'http://{HoraeMetaAddr}:2379' lease = \"10s\" timeout = \"5s\" [cluster_deployment.etcd_client] server_addrs = ['http://{HoraeMetaAddr}:2379'] 完整配置 将上面提到的所有关键配置合并之后，我们可以得到一个完整的、可运行的配置。为了让这个配置可以直接运行起来，配置中均采用了本地存储：基于 RocksDB 的 WAL 和本地磁盘的 Object Storage：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 [server] bind_addr = \"0.0.0.0\" http_port = 5440 grpc_port = 8831 [logger] level = \"info\" [runtime] read_thread_num = 20 write_thread_num = 16 background_thread_num = 12 [cluster_deployment] mode = \"WithMeta\" [cluster_deployment.meta_client] cluster_name = 'defaultCluster' meta_addr = 'http://127.0.0.1:2379' lease = \"10s\" timeout = \"5s\" [cluster_deployment.etcd_client] server_addrs = ['127.0.0.1:2379'] [analytic] write_group_worker_num = 16 replay_batch_size = 100 max_replay_tables_per_batch = 128 write_group_command_channel_cap = 1024 sst_background_read_parallelism = 8 [analytic.manifest] scan_batch_size = 100 snapshot_every_n_updates = 10000 scan_timeout = \"5s\" store_timeout = \"5s\" [analytic.wal] type = \"RocksDB\" data_dir = \"/home/admin/data/horaedb\" [analytic.storage] mem_cache_capacity = \"20GB\" # 1\u003c\u003c8=256 mem_cache_partition_bits = 8 [analytic.storage.object_store] type = \"Local\" data_dir = \"/home/admin/data/horaedb/\" [analytic.table_opts] arena_block_size = 2097152 write_buffer_size = 33554432 [analytic.compaction] schedule_channel_len = 16 schedule_interval = \"30m\" max_ongoing_tasks = 8 memory_limit = \"4G\" 将这个配置命名成 config.toml。至于使用远程存储的配置示例在下面我们也提供了，需要注意的是，配置中的相关参数需要被替换成实际的参数才能真正使用：\n本地 RocksDB WAL + OSS OceanBase WAL + OSS Kafka WAL + OSS 启动集群 首先，我们先启动 HoraeMeta：\n1 2 3 docker run -d --name horaemeta-server \\ -p 2379:2379 \\ ghcr.io/apache/horaemeta-server:nightly-20231225-ab067bf0 HoraeMeta 启动好了，没有问题之后，就可以把 HoraeDB 的容器创建出来： TODO: 补充完整\n","categories":"","description":"","excerpt":"本文展示如何部署一个由 HoraeMeta 控制的 HoraeDB 集群，有了 HoraeMeta 提供的服务，如果 HoraeDB 使用存 …","ref":"/cn/docs/user-guide/cluster_deployment/with_meta/","tags":"","title":"WithMeta 模式"},{"body":"Apache HoraeDB 使用源码压缩包进行发布。\n最新发布 最新一次发布版本：2.0.0(2024-05-23)，源码下载地址。\n用户可以按照以下指南使用 signatures 和 checksums 验证此版本。\nDocker 镜像 暂不提供预构建好的二进制文件，用户可以使用源码编译或者使用 docker 镜像：\nhttps://hub.docker.com/r/apache/horaemeta-server https://hub.docker.com/r/apache/horaedb-server 历史版本 历史已发布版本，可以在这里查询得到。\n验证 signatures 和 checksums 强烈建议用户验证下载的文件。\nHoraeDB 为所有在下载站点上的文件提供 SHA digest 文件和 PGP 签名文件，验证文件以原始文件命名，并带有 sha512、asc 扩展名。\n验证 Checksums 用户需要下载 tar.gz 文件和 tar.gz.sha512 文件来验证 Checksums。验证命令：\n1 sha512sum -c apache-horaedb-incubating-v2.0.0-src.tar.gz.sha512 正确结果：\napache-horaedb-incubating-v2.0.0-src.tar.gz: OK 验证 Signatures 验证 PGP Signatures，用户需要下载 release KEYS 文件。\n导入下载的 KEYS 文件：\n1 gpg --import KEYS 验证命令：\n1 gpg --verify apache-horaedb-incubating-v2.0.0-src.tar.gz.asc 正确结果：\ngpg: Signature made Wed 12 Jun 2024 11:05:04 AM CST using RSA key ID 08A0BAB4 gpg: Good signature from \"jiacai2050@apache.org\" gpg: aka \"Jiacai Liu \u003chello@liujiacai.net\u003e\" gpg: aka \"Jiacai Liu \u003cdev@liujiacai.net\u003e\" gpg: WARNING: This key is not certified with a trusted signature! gpg: There is no indication that the signature belongs to the owner. Primary key fingerprint: 6F73 4AE4 297C 7F62 B605 4F91 D302 6E5C 08A0 BAB4 ","categories":"","description":"","excerpt":"Apache HoraeDB 使用源码压缩包进行发布。\n最新发布 最新一次发布版本：2.0.0(2024-05-23)，源码下载地址。\n用户 …","ref":"/cn/downloads/","tags":"","title":"下载"},{"body":"注意：此功能仍在开发中，API 将来可能会发生变化。\n本章讨论 PartitionTable。\nHoraeDB 使用的分区表语法类似于 MySQL 。\n一般的分区表包括Range Partitioning、List Partitoning、Hash Partitioning和Key Partititioning。\nHoraeDB 目前仅支持 Key Partitioning。\n设计 与 MySQL 类似，分区表的不同部分作为单独的表存储在不同的位置。\n目前设计，一个分区表可以在多个 HoraeDB 节点上打开，支持同时写入和查询，可以水平扩展。\n如下图所示，在 node0 和 node1 上打开了PartitionTable，在 node2 和 node3 上打开了存放实际数据的物理子表。\n┌───────────────────────┐ ┌───────────────────────┐ │Node0 │ │Node1 │ │ ┌────────────────┐ │ │ ┌────────────────┐ │ │ │ PartitionTable │ │ │ │ PartitionTable │ │ │ └────────────────┘ │ │ └────────────────┘ │ │ │ │ │ │ │ └────────────┼──────────┘ └───────────┼───────────┘ │ │ │ │ ┌───────────────────────┼─────────────────────────────┼───────────────────────┐ │ │ │ │ ┌────────────┼───────────────────────┼─────────────┐ ┌─────────────┼───────────────────────┼────────────┐ │Node2 │ │ │ │Node3 │ │ │ │ ▼ ▼ │ │ ▼ ▼ │ │ ┌─────────────────────┐ ┌─────────────────────┐ │ │ ┌─────────────────────┐ ┌─────────────────────┐ │ │ │ │ │ │ │ │ │ │ │ │ │ │ │ SubTable_0 │ │ SubTable_1 │ │ │ │ SubTable_2 │ │ SubTable_3 │ │ │ │ │ │ │ │ │ │ │ │ │ │ │ └─────────────────────┘ └─────────────────────┘ │ │ └─────────────────────┘ └─────────────────────┘ │ │ │ │ │ └──────────────────────────────────────────────────┘ └──────────────────────────────────────────────────┘ Key 分区 Key Partitioning支持一列或多列计算，使用 HoraeDB 内置的 hash 算法进行计算。\n使用限制：\n仅支持 tag 列作为分区键。 暂时不支持 LINEAR KEY。 key 分区的建表语句如下：\n1 2 3 4 5 6 7 CREATE TABLE `demo`( `name`string TAG, `id` int TAG, `value` double NOT NULL, `t` timestamp NOT NULL, TIMESTAMP KEY(t) ) PARTITION BY KEY(name) PARTITIONS 2 ENGINE = Analytic 参考 MySQL key partitioning。\n查询 由于分区表数据实际上是存放在不同的物理表中，所以查询时需要根据查询请求计算出实际请求的物理表。\n首先查询会根据查询语句计算出要查询的物理表， 然后通过 HoraeDB 内部服务 remote engine 远程请求物理表所在节点获取数据（支持谓词下推）。\n分区表的实现在 PartitionTableImpl 中。\n第一步：解析查询 sql，根据查询参数计算出要查询的物理表。 第二步：查询物理表数据。 第三步：用拉取的数据进行计算。 │ 1 │ │ ▼ ┌───────────────┐ │Node0 │ │ │ │ │ └───────────────┘ ┬ 2 │ 2 ┌──────────────┴──────────────┐ │ ▲ │ │ 3 │ 3 │ ▼ ─────────────┴───────────── ▼ ┌───────────────┐ ┌───────────────┐ │Node1 │ │Node2 │ │ │ │ │ │ │ │ │ └───────────────┘ └───────────────┘ Key 分区 带有 and, or, in, = 的过滤器将选择特定的子表。 支持模糊匹配过滤器，如 \u003c, \u003e，但可能性能较差，因为它会扫描所有物理表。 Key partitioning 规则实现在 KeyRule。\n写入 写入过程与查询过程类似。\n首先根据分区规则，将写入请求拆分到不同的物理表中，然后通过 remote engine 服务发送到不同的物理节点进行实际的数据写入。\n","categories":"","description":"","excerpt":"注意：此功能仍在开发中，API 将来可能会发生变化。\n本章讨论 PartitionTable。\nHoraeDB …","ref":"/cn/docs/design/table_partitioning/","tags":"","title":"分区表"},{"body":"基础语法 建表的基础语法如下 ( [] 之间的内容是可选部分):\n1 2 3 4 CREATE TABLE [IF NOT EXISTS] table_name ( column_definitions ) ENGINE = engine_type [WITH ( table_options )]; 列定义的语法 :\n1 column_name column_type [[NOT] NULL] {[TAG] | [TIMESTAMP KEY] | [PRIMARY KEY]} [DICTIONARY] [COMMENT ''] 表选项的语法是键-值对，值用单引号（'）来引用。例如：\n1 ... WITH ( enable_ttl='false' ) IF NOT EXISTS 添加 IF NOT EXISTS 时，HoraeDB 在表名已经存在时会忽略建表错误。\n定义列 一个列的定义至少应该包含名称和类型部分，支持的类型见 这里。\n列默认为可空，即 “NULL \" 关键字是隐含的；添加 NOT NULL 时列不可为空。\n1 2 3 4 5 6 7 -- this definition a_nullable int -- equals to a_nullable int NULL -- add NOT NULL to make it required b_not_null NOT NULL 定义列时可以使用相关的关键字将列标记为 特殊列。\n对于 string 的 tag 列，推荐设置为字典类型来减少内存占用：\n1 `tag1` string TAG DICTIONARY 引擎设置 HoraeDB 支持指定某个表使用哪种引擎，目前支持的引擎类型为 Analytic。注意这个属性设置后不可更改。\n分区设置 仅适用于集群部署模式\nCREATE TABLE ... PARTITION BY KEY 下面这个例子创建了一个具有 8 个分区的表，分区键为 name：\n1 2 3 4 5 6 7 8 9 10 11 CREATE TABLE `demo` ( `name` string TAG COMMENT 'client username', `value` double NOT NULL, `t` timestamp NOT NULL, timestamp KEY (t) ) PARTITION BY KEY(name) PARTITIONS 8 ENGINE=Analytic with ( enable_ttl='false' ) ","categories":"","description":"","excerpt":"基础语法 建表的基础语法如下 ( [] 之间的内容是可选部分):\n1 2 3 4 CREATE TABLE [IF NOT EXISTS] …","ref":"/cn/docs/user-guide/sql/ddl/create_table/","tags":"","title":"创建表"},{"body":"","categories":"","description":"","excerpt":"","ref":"/cn/blog/","tags":"","title":"博客"},{"body":"架构 在本节中，将会介绍一种分布式 WAL 实现（基于 Kafka）。表的预写日志（write-ahead logs，以下简称日志）在本实现中是按 region 级别管理的，region 可以简单理解为多个表的共享日志文件。\n如下图所示，在本实现中将 region 映射到 Kafka 中的 topic（只有一个 partition）。 通常一个 region 需要两个 topic ，一个用于存储日志，另一个用于存储元数据。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 ┌──────────────────────────┐ │ Kafka │ │ │ │ ...... │ │ │ │ ┌─────────────────────┐ │ │ │ Meta Topic │ │ │ │ │ │ Delete │ │ ┌─────────────────┐ │ │ ┌──────────────────────┐ ┌───────┼─┼─► Partition │ │ │ │ HoraeDB │ │ │ │ │ │ │ │ │ │ │ │ │ └─────────────────┘ │ │ │ ┌──────────────────┐ │ │ │ │ │ │ │ │ WAL │ │ │ │ └─────────────────────┘ │ │ │ ...... │ │ │ │ │ │ │ ┌──────────────┐ │ │ │ │ ┌──────────────────────┐ │ │ │ │ Region │ │ │ │ │ │ Data Topic │ │ │ │ │ ├─┼─┼──┘ │ │ │ │ | | | ┌──────────┐ │ │ │ │ │ ┌──────────────────┐ │ │ │ │ │ │ Metadata │ │ │ │ │ │ │ Partition │ │ │ │ │ │ └──────────┘ │ │ │ Write │ │ │ │ │ │ Write ─────────┼─┼─► ├─┼─┼───┐ │ │ │ ┌──┬──┬──┬──┬──┐ │ │ │ │ │ │ ┌──────────┐ │ │ │ └──────┼─┼─┼─► │ │ │ │ ├─┼─┼─┼────┐ │ │ │ │ Client │ │ │ │ │ │ │ └──┴──┴──┴──┴──┘ │ │ │ │ Read ◄─────────┼─┼─┤ └──────────┘ │ │ │ │ │ │ │ │ │ │ │ │ │ │ │ │ │ │ └──────────────────┘ │ │ │ │ │ └──▲───────────┘ │ │ │ │ │ │ │ │ │ │ ...... │ │ │ └──────────────────────┘ │ │ │ └────┼─────────────┘ │ │ ...... │ │ │ │ │ └──────────────────────────┘ │ └──────┼───────────────┘ │ │ │ │ │ │ Read │ └──────────────────────────────────────────────────────────┘ 数据模型 日志格式 日志格式采用了在 基于 RocksDB 的 WAL 中定义的通用格式。\n元数据 每个 region 都将在内存和 Kafka 中维护其元数据，我们在这里称之为 RegionMeta。它可以被认为是一张映射表，以表 ID 作为键，以 TableMeta 作为值。我们简要介绍一下 TableMeta 中的变量：\nnext_seq_num，为下一条写入日志分配的 sequence number。 latest_marked_deleted，表最后一次触发 flush 时对应的 sequence number, 所以对应 sequence number 小于该值的日志都将被标记为可以删除。 current_high_watermark， 该表最近一次日志写入后，Kafka 对应 topic 的高水位。 seq_offset_mapping，sequence number 和 Kafka 对应 topic offset 的映射，每次 flush 后，会将 latest_marked_deleted 前的条目进行清理。 ┌─────────────────────────────────────────┐ │ RegionMeta │ │ │ │ Map\u003cTableId, TableMeta\u003e table_metas │ └─────────────────┬───────────────────────┘ │ │ │ └─────┐ │ │ ┌──────────────────────┴──────────────────────────────┐ │ TableMeta │ │ │ │ SequenceNumber next_seq_num │ │ │ │ SequenceNumber latest_mark_deleted │ │ │ │ KafkaOffset high_watermark │ │ │ │ Map\u003cSequenceNumber, KafkaOffset\u003e seq_offset_mapping │ └─────────────────────────────────────────────────────┘ 主要流程 我们主要关于对于单个 region 的主要操作，会介绍以下操作的主要流程：\n打开或创建 region。 读写日志。 删除日志。 打开或创建 region 步骤 在打开的 namespace 中搜索 region。 如果 region 存在，最重要的事是去恢复其元数据，恢复过程将在之后介绍。 如果 region 不存在并且需要自动创建，则需要在 Kafka 上创建对应的 topic。 在 cache 中插入相应 region 并将其返回。 恢复 上面提到，RegionMeta 实际就是以表 ID 为键，以 TableMeta 为值的映射表。因此，我们在本节中只关注特定 TableMeta 的恢复即可，将在每步的介绍中加入例子以作更好的说明。\n从快照中恢复。我们会在某些场景下为 RegionMeta制作快照（例如当标记日志为可删除时，真正清理日志时），并且将其写到 meta topic 中，快照实际上就是在某个时间点的 RegionMeta。当恢复 region 时，我们可以使用快照来避免扫描 data topic 的全部数据。下面为上述过程对应的例子，我们从在 Kafka 高水位为 64 的时间点时制作的快照中恢复 RegionMeta： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 high watermark in snapshot: 64 ┌──────────────────────────────┐ │ RegionMeta │ │ │ │ ...... │ │ ┌──────────────────────────┐ │ │ │ TableMeta │ │ │ │ │ │ │ │ next_seq_num: 5 │ │ │ │ │ │ │ │ latest_mark_deleted: 2 │ │ │ │ │ │ │ │ high_watermark: 32 │ │ │ │ │ │ │ │ seq_offset_mapping: │ │ │ │ │ │ │ │ (2, 16) (3, 16) (4, 31) │ │ │ └──────────────────────────┘ │ │ ...... │ └──────────────────────────────┘ 从日志数据中恢复。 当从快照中恢复的过程完成后，我们以快照被制作时 data topic 中的高水位为起点，扫描其中的日志数据进行后续恢复，明显这能够避免扫描 data topic 中的全部数据。以下为上述过程的例子： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 ┌────────────────────────────────────┐ │ │ │ high_watermark in snapshot: 64 │ │ │ │ ┌──────────────────────────────┐ │ │ │ RegionMeta │ │ │ │ │ │ │ │ ...... │ │ │ │ ┌──────────────────────────┐ │ │ │ │ │ TableMeta │ │ │ │ │ │ │ │ │ │ │ │ next_seq_num: 5 │ │ │ ┌────────────────────────────────┐ │ │ │ │ │ │ │ RegionMeta │ │ │ │ latest_mark_deleted: 2 │ │ │ │ │ │ │ │ │ │ │ │ ...... │ │ │ │ high_watermark: 32 │ │ │ │ ┌────────────────────────────┐ │ │ │ │ │ │ │ │ │ TableMeta │ │ │ │ │ seq_offset_mapping: │ │ │ │ │ │ │ │ │ │ │ │ │ │ │ next_seq_num: 8 │ │ │ │ │ (2, 16) (3, 16) (4, 31) │ │ │ │ │ │ │ │ │ └──────────────────────────┘ │ │ │ │ latest_mark_deleted: 2 │ │ │ │ ...... │ │ │ │ │ │ │ └──────────────────────────────┘ ├──────────────────► │ high_watermark: 32 │ │ │ │ │ │ │ │ │ ┌────────────────────────────────┐ │ │ │ seq_offset_mapping: │ │ │ │ Data topic │ │ │ │ │ │ │ │ │ │ │ │ (2, 16) (3, 16) (4, 31) │ │ │ │ ┌────────────────────────────┐ │ │ │ │ │ │ │ │ │ Partition │ │ │ │ │ (5, 72) (6, 81) (7, 90) │ │ │ │ │ │ │ │ │ │ │ │ │ │ │ ┌────┬────┬────┬────┬────┐ │ │ │ │ └────────────────────────────┘ │ │ │ │ │ 64 │ 65 │ ...│ 99 │100 │ │ │ │ │ ...... │ │ │ │ └────┴────┴────┴────┴────┘ │ │ │ └────────────────────────────────┘ │ │ │ │ │ │ │ │ └────────────────────────────┘ │ │ │ │ │ │ │ └────────────────────────────────┘ │ │ │ └────────────────────────────────────┘ 读写日志 读写流程比较简单。\n写流程:\n打开指定的 region，如果不存在则需要创建。 利用 client 将日志写入到 region 对应的 data topic 中。 更新 TableMeta 中的 next_seq_num, current_high_watermark 和 seq_offset_mapping等元数据， 读流程:\n打开指定的 region。 读取 region 的所有日志数据，按表切分数据和回放等工作需要调用者实现。 删除日志 日志的删除可以划分为两个步骤：\n标记日志为可删除。 利用后台线程做延迟清理。 标记 更新在 TableMeta 中的 latest_mark_deleted 和 seq_offset_mapping（需要进行维护，使得每一条目的 sequence number 大于等于更新后的 latest_mark_deleted）。 或许我们需要在删除表的时候，制作并及时同步 RegionMeta 的快照到 Kafka 中。 清理 清理逻辑如下，会在后台线程中执行：\n制作 RegionMeta 的快照。 根据快照判断是否需要进行清理。 如果需要，先同步快照到 Kafka 中，然后清理日志。 ","categories":"","description":"","excerpt":"架构 在本节中，将会介绍一种分布式 WAL 实现（基于 Kafka）。表的预写日志（write-ahead logs，以下简称日志）在本实现 …","ref":"/cn/docs/design/wal_on_kafka/","tags":"","title":"基于 Kafka 的 WAL"},{"body":"架构 在本节中，我们将介绍单机版 WAL 的实现（基于 RocksDB）。预写日志（write-ahead logs，以下简称日志）在本实现中是按表级别进行管理的，对应的数据结构为 TableUnit。为简单起见，所有相关数据（日志或元数据）都存储在单个 column family（RocksDB 中的概念，可以类比关系型数据库的表） 中。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 ┌─────────────────────────┐ │ HoraeDB │ │ │ │ ┌─────────────────────┐ │ │ │ WAL │ │ │ │ │ │ │ │ ...... │ │ │ │ │ │ │ │ ┌────────────────┐ │ │ Write ─────┼─┼──► TableUnit │ │ │ │ │ │ │ │ │ Read ─────┼─┼──► ┌────────────┐ │ │ │ │ │ │ │ RocksDBRef │ │ │ │ │ │ │ └────────────┘ │ │ │ Delete ─────┼─┼──► │ │ │ │ │ └────────────────┘ │ │ │ │ ...... │ │ │ └─────────────────────┘ │ │ │ └─────────────────────────┘ 数据模型 通用日志格式 通用日志格式分为 key 格式和 value 格式，下面是对 key 格式各个字段的介绍:\nnamespace: 出于不同的目的，可能会存在多个 WAL 实例（例如，manifest 也依赖于 wal）, namespace 用于区分它们。 region_id: 在一些 WAL 实现中我们可能需要在共享日志文件中，管理来自多个表的日志，region 就是描述这样一组表的概念， 而 region id 就是其标识。 table_id: 表的标识。 sequence_num: 特定表中单条日志的标识。 version: 用于兼容新旧格式。 1 2 3 +---------------+----------------+-------------------+--------------------+-------------+ | namespace(u8) | region_id(u64) | table_id(u64) | sequence_num(u64) | version(u8) | +---------------+----------------+-------------------+--------------------+-------------+ 下面是对 value 格式各个字段的介绍(payload 可以理解为编码后的具体日志内容):\n1 2 3 +--------------------+----------+ | version header(u8) | payload | +--------------------+----------+ 元数据 与日志格式相同，元数据以 key-value 格式存储, 本实现的元数据实际只是存储了每张表最近一次 flush 对应的 sequence_num。下面是定义的元数据 key 格式和其中字段的介绍：\nnamespace, table_id, version 和日志格式中相同。 key_type, 用于定义元数据的类型，现在只定义了 MaxSeq 类型的元数据，在。 因为在 RocksDB 版本的 WAL 实现中，日志是按表级别进行管理，所以这个 key 格式里面没有 region_id 字段。 1 2 3 +---------------+--------------+----------------+-------------+ | namespace(u8) | key_type(u8) | table_id(u64) | version(u8) | +---------------+--------------+----------------+-------------+ 这是定义的元数据值格式，如下所示，其中只有 version 和 max_seq(flushed sequence):\n1 2 3 +-------------+--------------+ | version(u8) | max_seq(u64) | +-------------+--------------+ 主要流程 打开 TableUnit: 读取所有表的最新日志条目，目的是恢复表的 next sequence num(将会分配给下一条写入的日志)。 扫描 metadata 恢复上一步遗漏的表的 next sequence num（因为可能有表刚刚触发了 fl​​ush，并且之后没有新的写入日志，所以当前不存在日志数据）。 读写日志。从 RocksDB 读取或者写入相关日志数据。 删除日志。为简单起见，在本实现中只是同步地删除相应的日志数据。 ","categories":"","description":"","excerpt":"架构 在本节中，我们将介绍单机版 WAL 的实现（基于 RocksDB）。预写日志（write-ahead logs，以下简称日志）在本实现 …","ref":"/cn/docs/design/wal_on_rocksdb/","tags":"","title":"基于 RocksDB 的 WAL"},{"body":"架构 本节将介绍基于本地磁盘的单机版 WAL（Write-Ahead Log，以下简称日志）的实现。在此实现中，日志按 region 级别进行管理。\n┌────────────────────────────┐ │ HoraeDB │ │ │ │ ┌────────────────────────┐ │ │ │ WAL │ │ ┌────────────────────────┐ │ │ │ │ │ │ │ │ ...... │ │ │ File System │ │ │ │ │ │ │ │ │ ┌────────────────────┐ │ │ manage │ ┌────────────────────┐ │ Write ─────┼─┼─► Region ├─┼─┼─────────┼─► Region Dir │ │ │ │ │ │ │ │ │ │ │ │ Read ─────┼─┼─► ┌────────────┐ │ │ │ mmap │ │ ┌────────────────┐ │ │ │ │ │ │ Segment 0 ├───┼─┼─┼─────────┼─┼─► Segment File 0 │ │ │ │ │ │ └────────────┘ │ │ │ │ │ └────────────────┘ │ │ Delete ─────┼─┼─► ┌────────────┐ │ │ │ mmap │ │ ┌────────────────┐ │ │ │ │ │ │ Segment 1 ├───┼─┼─┼─────────┼─┼─► SegmenteFile 1 │ │ │ │ │ │ └────────────┘ │ │ │ │ │ └────────────────┘ │ │ │ │ │ ┌────────────┐ │ │ │ mmap │ │ ┌────────────────┐ │ │ │ │ │ │ Segment 2 ├───┼─┼─┼─────────┼─┼─► SegmenteFile 2 │ │ │ │ │ │ └────────────┘ │ │ │ │ │ └────────────────┘ │ │ │ │ │ ...... │ │ │ │ │ ...... │ │ │ │ └────────────────────┘ │ │ │ └────────────────────┘ │ │ │ ...... │ │ │ ...... │ │ └────────────────────────┘ │ └────────────────────────┘ └────────────────────────────┘ 数据模型 文件路径 每个 region 都拥有一个目录，用于管理该 region 的所有 segment。目录名为 region 的 ID。每个 segment 的命名方式为 seg_\u003cid\u003e，ID 从 0 开始递增。\nSegment 的格式 一个 region 中所有表的日志都存储在 segments 中，并按照 sequence number 从小到大排列。segment 文件的结构如下：\nSegment0 Segment1 ┌────────────┐ ┌────────────┐ │ Magic Num │ │ Magic Num │ ├────────────┤ ├────────────┤ │ Record │ │ Record │ ├────────────┤ ├────────────┤ │ Record │ │ Record │ ├────────────┤ ├────────────┤ .... │ Record │ │ Record │ ├────────────┤ ├────────────┤ │ ... │ │ ... │ │ │ │ │ └────────────┘ └────────────┘ seg_0 seg_1 在内存中，每个 segment 还会存储一些额外的信息以供读写和删除操作使用：\npub struct Segment { /// A hashmap storing both min and max sequence numbers of records within /// this segment for each `TableId`. table_ranges: HashMap\u003cTableId, (SequenceNumber, SequenceNumber)\u003e, /// An optional vector of positions within the segment. record_position: Vec\u003cPosition\u003e, ... } 日志格式 segment 中的日志格式如下：\n+---------+--------+------------+--------------+--------------+-------+ | version | crc | table id | sequence num | value length | value | | (u8) | (u32) | (u64) | (u64) | (u32) |(bytes)| +---------+--------+------------+--------------+--------------+-------+ 字段说明：\nversion：日志版本号。\ncrc：用于确保数据一致性。计算从 table id 到该记录结束的 CRC 校验值。\ntable id：表的唯一标识符。\nsequence num：记录的序列号。\nvalue length：value 的字节长度。\nvalue：通用日志格式中的值。\n日志中不存储 region ID，因为可以通过文件路径获取该信息。\n主要流程 打开 Wal 识别 Wal 目录下的所有 region 目录。\n在每个 region 目录下，识别所有 segment 文件。\n打开每个 segment 文件，遍历其中的所有日志，记录其中每个日志开始和结束的偏移量和每个 TableId 在该 segment 中的最小和最大序列号，然后关闭文件。\n如果不存在 region 目录或目录下没有任何 segment 文件，则自动创建相应的目录和文件。\n读日志 根据 segment 的元数据，确定本次读取操作涉及的所有 segment。 按照 id 从小到大的顺序，依次打开这些 segment，将原始字节解码为日志。 写日志 将待写入的日志序列化为字节数据，追加到 id 最大的 segment 文件中。\n每个 segment 创建时预分配固定大小的 64MB，不会动态改变。当预分配的空间用完后，创建一个新的 segment，并切换到新的 segment 继续追加。\n每次追加后不会立即调用 flush；默认情况下，每写入十次或在 segment 文件关闭时才执行 flush。\n在内存中更新 segment 的元数据 table_ranges。\n删除日志 假设需要将 id 为 table_id 的表中，序列号小于 seq_num 的日志标记为删除：\n在内存中更新相关 segment 的 table_ranges 字段，将该表的最小序列号更新为 seq_num + 1。\n如果修改后，该表在此 segment 中的最小序列号大于最大序列号，则从 table_ranges 中删除该表。\n如果一个 segment 的 table_ranges 为空，且不是 id 最大的 segment，则删除该 segment 文件。\n","categories":"","description":"","excerpt":"架构 本节将介绍基于本地磁盘的单机版 WAL（Write-Ahead Log，以下简称日志）的实现。在此实现中，日志按 region 级别进 …","ref":"/cn/docs/design/wal_on_disk/","tags":"","title":"基于本地磁盘的 WAL"},{"body":"HoraeDB 实现了 Table 模型，支持的数据类型和 MySQL 比较类似。 下列表格列出了 HoraeDB 的数据类型和 MySQL 的数据类型的对应关系。\n支持的数据类型 (大小写不敏感) SQL HoraeDB null Null timestamp Timestamp double Double float Float string String Varbinary Varbinary uint64 UInt64 uint32 UInt32 uint16 UInt16 uint8 UInt8 int64/bigint Int64 int32/int Int32 int16/smallint Int16 int8/tinyint Int8 boolean Boolean date Date time Time ","categories":"","description":"","excerpt":"HoraeDB 实现了 Table 模型，支持的数据类型和 MySQL 比较类似。 下列表格列出了 HoraeDB 的数据类型和 MySQL …","ref":"/cn/docs/user-guide/sql/model/data_types/","tags":"","title":"数据类型"},{"body":"HoraeDB SQL 基于 DataFusion 实现，支持的标量函数如下。更多详情请参考： Datafusion\n数值函数 函数 描述 abs(x) 绝对值 acos(x) 反余弦 asin(x) 反正弦 atan(x) 反正切 atan2(y, x) y/x 的反正切 ceil(x) 小于或等于参数的最接近整数 cos(x) 余弦 exp(x) 指数 floor(x) 大于或等于参数的最接近整数 ln(x) 自然对数 log10(x) 以 10 为底的对数 log2(x) 以 2 为底的对数 power(base, exponent) 幂函数 round(x) 四舍五入 signum(x) 根据参数的正负返回 -1、0、+1 sin(x) 正弦 sqrt(x) 平方根 tan(x) 正切 trunc(x) 截断计算，取整（向零取整） 条件函数 函数 描述 coalesce 如果它的参数中有一个不为 null，则返回第一个参数，如果所有参数均为 null，则返回 null。当从数据库中检索数据用于显示时，它经常用于用默认值替换 null 值。 nullif 如果 value1 等于 value2，则返回 null 值；否则返回 value1。这可用于执行与 coalesce 表达式相反的操作 字符函数 函数 描述 ascii 返回参数的第一个字符的 ascii 数字编码。在 UTF8 编码下，返回字符的 Unicode 码点。在其他多字节编码中，参数必须是 ASCII 字符。 bit_length 返回字符串的比特位数。 btrim 从字符串的开头和结尾删除给定字符串中的字符组成的最长字符串 char_length 等效于 length。 character_length 等效于 length。 concat 将两个或多个字符串合并为一个字符串。 concat_ws 使用给定的分隔符组合两个值。 chr 根据数字码返回字符。 initcap 将字符串中每个单词的首字母大写。 left 返回字符串的指定最左边字符。 length 返回字符串中字符的数量。 lower 将字符串中的所有字符转换为它们的小写。 lpad 使用特定字符集将字符串左填充到给定长度。 ltrim 从字符串的开头删除由字符中的字符组成的最长字符串（默认为空格）。 md5 计算给定字符串的 MD5 散列值。 octet_length 等效于 length。 repeat 返回一个由输入字符串重复指定次数组成的字符串。 replace 替换字符串中所有子字符串的出现为新子字符串。 reverse 反转字符串。 right 返回字符串的指定最右边字符。 rpad 使用特定字符集将字符串右填充到给定长度。 rtrim 从字符串的结尾删除包含 characters 中任何字符的最长字符串。 digest 计算给定字符串的散列值。 split_part 按指定分隔符拆分字符串，并从结果数组中返回 starts_with 检查字符串是否以给定字符串开始 strpos 搜索字符串是否包含一个给定的字符串，并返回位置 substr 提取子字符串 translate 把字符串翻译成另一种字符集 Translates one set of characters into another. trim 移除字符串两侧的空白字符或其他指定字符。 upper 将字符串中的所有字符转换为它们的大写。 正则函数 函数 描述 regexp_match 判断一个字符串是否匹配正则表达式 regexp_replace 使用新字符串替换正则匹配的字符串中内容 时间函数 函数 描述 to_timestamp 将字符串转换为 Timestamp(Nanoseconds，None)类型。 to_timestamp_millis 将字符串转换为 Timestamp(Milliseconds，None)类型。 to_timestamp_micros 将字符串转换为 Timestamp(Microseconds，None)类型。 to_timestamp_seconds 将字符串转换为 Timestamp(Seconds，None)类型。 extract 从日期/时间值中检索年份或小时等子字段。 date_part 从日期/时间值中检索子字段。 date_trunc 将日期/时间值截断到指定的精度。 date_bin 将日期/时间值按指定精度进行分组。 from_unixtime 将 Unix 时代转换为 Timestamp(Nanoseconds，None)类型。 now 作为 Timestamp(Nanoseconds，UTC)返回当前时间。 其他函数 Function 描述 array 创建有一个数组 arrow_typeof 返回内置的数据类型 in_list 检测数值是否在 list 里面 random 生成随机值 sha224 sha224 sha256 sha256 sha384 sha384 sha512 sha512 to_hex 转换为 16 进制 ","categories":"","description":"","excerpt":"HoraeDB SQL 基于 DataFusion 实现，支持的标量函数如下。更多详情请参考： Datafusion …","ref":"/cn/docs/user-guide/sql/scalar_functions/","tags":"","title":"标量函数"},{"body":"HoraeDB 的表的约束如下：\n必须有主键 主键必须包含时间列，并且只能包含一个时间列 主键不可为空，并且主键的组成字段也不可为空 Timestamp 列 HoraeDB 的表必须包含一个时间戳列，对应时序数据中的时间，例如 OpenTSDB/Prometheus 的 timestamp。 时间戳列通过关键字 timestamp key 设置，例如 TIMESTAMP KEY(ts)。\nTag 列 Tag 关键字定义了一个字段作为标签列，和时序数据中的 tag 类似，例如 OpenTSDB 的 tag 或 Prometheus 的 label。\n主键 主键用于数据去重和排序，由一些列和一个时间列组成。 主键可以通过以下一些方式设置：\n使用 primary key 关键字 使用 tag 来自动生成 TSID，HoraeDB 默认将使用 (TSID,timestamp) 作为主键。 只设置时间戳列，HoraeDB 将使用 (timestamp) 作为主键。 注意：如果同时指定了主键和 Tag 列，那么 Tag 列只是一个额外的信息标识，不会影响主键生成逻辑。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 CREATE TABLE with_primary_key( ts TIMESTAMP NOT NULL, c1 STRING NOT NULL, c2 STRING NULL, c4 STRING NULL, c5 STRING NULL, TIMESTAMP KEY(ts), PRIMARY KEY(c1, ts) ) ENGINE=Analytic WITH (ttl='7d'); CREATE TABLE with_tag( ts TIMESTAMP NOT NULL, c1 STRING TAG NOT NULL, c2 STRING TAG NULL, c3 STRING TAG NULL, c4 DOUBLE NULL, c5 STRING NULL, c6 STRING NULL, TIMESTAMP KEY(ts) ) ENGINE=Analytic WITH (ttl='7d'); CREATE TABLE with_timestamp( ts TIMESTAMP NOT NULL, c1 STRING NOT NULL, c2 STRING NULL, c3 STRING NULL, c4 DOUBLE NULL, c5 STRING NULL, c6 STRING NULL, TIMESTAMP KEY(ts) ) ENGINE=Analytic WITH (ttl='7d'); TSID 如果建表时没有设置主键，并且提供了 Tag 列，HoraeDB 会自动生成一个 TSID 列和时间戳列作为主键。TSID 由所有 Tag 列的 hash 值生成，本质上这是一种自动生成 ID 的机制。\n","categories":"","description":"","excerpt":"HoraeDB 的表的约束如下：\n必须有主键 主键必须包含时间列，并且只能包含一个时间列 主键不可为空， …","ref":"/cn/docs/user-guide/sql/model/special_columns/","tags":"","title":"特殊字段"},{"body":"HoraeDB 支持使用 Prometheus 和 Grafana 做自监控。\nPrometheus Prometheus 是非常流行的系统和服务监控系统。\n配置 把下面的配置保存到 prometheus.yml 文件中。比如，在 tmp 目录下，文件地址为 /tmp/prometheus.yml。\n有两个 HoraeDB http 服务启动在 localhost:5440、localhost:5441。\n1 2 3 4 5 6 7 8 global: scrape_interval: 30s scrape_configs: - job_name: horaedb-server static_configs: - targets: [your_ip:5440, your_ip:5441] labels: env: horaedbcluster Prometheus 详细配置见这里。\n运行 你可以使用 docker 来运行 Prometheus。Docker 镜像在这里可以找到。\ndocker run \\ -d --name=prometheus \\ -p 9090:9090 \\ -v /tmp/prometheus.yml:/etc/prometheus/prometheus.yml \\ prom/prometheus:v2.41.0 更多 Prometheus 安装方法，参考这里。\nGrafana Grafana 是一个非常流行的可观察性和数据可视化平台。\n运行 你可以使用 docker 来运行 Grafana。Docker 镜像在这里可以找到。\ndocker run -d --name=grafana -p 3000:3000 grafana/grafana:9.3.6 默认用户密码是 admin/admin.\n运行上面命令后，grafana 可以用浏览器打开 http://127.0.0.1:3000。\n更多 Grafana 安装方法，参考这里。\n配置数据源 将光标悬停在配置（齿轮）图标上。 选择数据源。 选择 Prometheus 数据源。 注意: Prometheus 的 url 需要填写成这样 http://your_ip:9090, your_ip 换成本地地址。\n更详细的配置可以参考这里。\n导入监控页面 页面 json\nHoraeDB 指标 当导入完成后，你可以看到如下页面：\nPanels tps: 集群写入请求数。 qps: 集群查询请求数。 99th query/write duration: 查询写入的 99% 分位数。 table query by table: 表查询请求数。 99th write duration details by instance: 写入耗时的 99% 分位数。 99th query duration details by instance: 查询耗时的 99% 分位数。 99th write partition table duration: 分区表查询耗时的 99% 分位数。 table rows: 表的写入行数。 table rows by instance: 实例级别的写入行数。 total tables to write: 有数据写入的表数目。 flush count: HoraeDB flush 的次数。 99th flush duration details by instance: 实例级别的 flush 耗时的 99% 分位数。 99th write stall duration details by instance: 实例级别的写入停顿时间的 99% 分位数 。 ","categories":"","description":"","excerpt":"HoraeDB 支持使用 Prometheus 和 Grafana 做自监控。\nPrometheus Prometheus 是非常流行的系统 …","ref":"/cn/docs/user-guide/operation/observability/","tags":"","title":"监控"},{"body":"查询 Table 信息 类似于 Mysql’s information_schema.tables, HoraeDB 提供 system.public.tables 存储表信息。\nsystem.public.tables 表的列如下 :\ntimestamp([TimeStamp]) catalog([String]) schema([String]) table_name([String]) table_id([Uint64]) engine([String]) 通过表名查询表信息示例如下：\n1 2 3 4 5 curl --location --request POST 'http://localhost:5000/sql' \\ --header 'Content-Type: application/json' \\ -d '{ \"query\": \"select * from system.public.tables where `table_name`=\\\"my_table\\\"\" }' 返回结果\n1 2 3 4 5 6 7 8 9 10 11 { \"rows\":[ { \"timestamp\":0, \"catalog\":\"horaedb\", \"schema\":\"public\", \"table_name\":\"my_table\", \"table_id\":3298534886446, \"engine\":\"Analytic\" } } ","categories":"","description":"","excerpt":"查询 Table 信息 类似于 Mysql’s information_schema.tables, HoraeDB …","ref":"/cn/docs/user-guide/operation/system_table/","tags":"","title":"系统表"},{"body":"HoraeDB SQL 基于 DataFusion 实现，支持的聚合函数如下。更多详情请参考： Datafusion\n常用 函数 描述 min 最小值 max 最大值 count 求行数 avg 平均值 sum 求和 array_agg 把数据放到一个数组 统计 函数 描述 var / var_samp 返回给定列的样本方差 var_pop 返回给定列的总体方差 stddev / stddev_samp 返回给定列的样本标准差 stddev_pop 返回给定列的总体标准差 covar / covar_samp 返回给定列的样本协方差 covar_pop 返回给定列的总体协方差 corr 返回给定列的相关系数 估值函数 函数 描述 approx_distinct 返回输入值的近似去重数量（HyperLogLog） approx_median 返回输入值的近似中位数，它是 approx_percentile_cont(x, 0.5) 的简单写法 approx_percentile_cont 返回输入值的近似百分位数（TDigest），其中 p 是 0 和 1（包括）之间的 float64，等同于 approx_percentile_cont_with_weight(x, 1, p) approx_percentile_cont_with_weight 返回输入值带权重的近似百分位数（TDigest），其中 w 是权重列表达式，p 是 0 和 1（包括）之间的 float64 ","categories":"","description":"","excerpt":"HoraeDB SQL 基于 DataFusion 实现，支持的聚合函数如下。更多详情请参考： Datafusion …","ref":"/cn/docs/user-guide/sql/aggregate_functions/","tags":"","title":"聚合函数"},{"body":"HoraeDB 支持标准的 SQL，用户可以使用 Http 协议创建表和读写表。更多内容可以参考 SQL 语法\n创建表 示例如下\n1 2 3 4 5 curl --location --request POST 'http://127.0.0.1:5000/sql' \\ --header 'Content-Type: application/json' \\ -d '{ \"query\": \"CREATE TABLE `demo` (`name` string TAG, `value` double NOT NULL, `t` timestamp NOT NULL, TIMESTAMP KEY(t)) ENGINE=Analytic with (enable_ttl='\\''false'\\'')\" }' 写数据 示例如下\n1 2 3 4 5 curl --location --request POST 'http://127.0.0.1:5000/sql' \\ --header 'Content-Type: application/json' \\ -d '{ \"query\": \"INSERT INTO demo(t, name, value) VALUES(1651737067000, '\\''horaedb'\\'', 100)\" }' 读数据 示例如下\n1 2 3 4 5 curl --location --request POST 'http://127.0.0.1:5000/sql' \\ --header 'Content-Type: application/json' \\ -d '{ \"query\": \"select * from demo\" }' 查询表信息 示例如下\n1 2 3 4 5 curl --location --request POST 'http://127.0.0.1:5000/sql' \\ --header 'Content-Type: application/json' \\ -d '{ \"query\": \"show create table demo\" }' Drop 表 示例如下\n1 2 3 4 5 curl --location --request POST 'http://127.0.0.1:5000/sql' \\ --header 'Content-Type: application/json' \\ -d '{ \"query\": \"DROP TABLE demo\" }' 查询表路由 示例如下\n1 curl --location --request GET 'http://127.0.0.1:5000/route/{table_name}' ","categories":"","description":"","excerpt":"HoraeDB 支持标准的 SQL，用户可以使用 Http 协议创建表和读写表。更多内容可以参考 SQL 语法\n创建表 示例如下\n1 2 3 …","ref":"/cn/docs/user-guide/operation/table/","tags":"","title":"表操作"},{"body":"集群运维接口的使用前提是，HoraeDB 部署为使用 HoraeMeta 的集群模式。\n运维接口 注意： 如下接口在实际使用时需要将 127.0.0.1 替换为 HoraeMeta 的真实地址。\n查询表元信息 当 tableNames 不为空的时候，使用 tableNames 进行查询。 当 tableNames 为空的时候，使用 ids 进行查询。使用 ids 查询的时候，schemaName 不生效。 curl --location 'http://127.0.0.1:8080/api/v1/table/query' \\ --header 'Content-Type: application/json' \\ -d '{ \"clusterName\":\"defaultCluster\", \"schemaName\":\"public\", \"names\":[\"demo1\", \"__demo1_0\"], }' curl --location 'http://127.0.0.1:8080/api/v1/table/query' \\ --header 'Content-Type: application/json' \\ -d '{ \"clusterName\":\"defaultCluster\", \"ids\":[0, 1] }' 查询表的路由信息 curl --location --request POST 'http://127.0.0.1:8080/api/v1/route' \\ --header 'Content-Type: application/json' \\ -d '{ \"clusterName\":\"defaultCluster\", \"schemaName\":\"public\", \"table\":[\"demo\"] }' 查询节点对应的 Shard 信息 curl --location --request POST 'http://127.0.0.1:8080/api/v1/getNodeShards' \\ --header 'Content-Type: application/json' \\ -d '{ \"ClusterName\":\"defaultCluster\" }' 查询 Shard 对应的表信息 如果 shardIDs 为空时，查询所有 shard 上表信息。 curl --location --request POST 'http://127.0.0.1:8080/api/v1/getShardTables' \\ --header 'Content-Type: application/json' \\ -d '{ \"clusterName\":\"defaultCluster\", \"shardIDs\": [1,2] }' 删除指定表的元数据 curl --location --request POST 'http://127.0.0.1:8080/api/v1/dropTable' \\ --header 'Content-Type: application/json' \\ -d '{ \"clusterName\": \"defaultCluster\", \"schemaName\": \"public\", \"table\": \"demo\" }' Shard 切主 curl --location --request POST 'http://127.0.0.1:8080/api/v1/transferLeader' \\ --header 'Content-Type: application/json' \\ -d '{ \"clusterName\":\"defaultCluster\", \"shardID\": 1, \"oldLeaderNodeName\": \"127.0.0.1:8831\", \"newLeaderNodeName\": \"127.0.0.1:18831\" }' Shard 分裂 curl --location --request POST 'http://127.0.0.1:8080/api/v1/split' \\ --header 'Content-Type: application/json' \\ -d '{ \"clusterName\" : \"defaultCluster\", \"schemaName\" :\"public\", \"nodeName\" :\"127.0.0.1:8831\", \"shardID\" : 0, \"splitTables\":[\"demo\"] }' 创建 HoraeDB 集群 curl --location 'http://127.0.0.1:8080/api/v1/clusters' \\ --header 'Content-Type: application/json' \\ --data '{ \"name\":\"testCluster\", \"nodeCount\":3, \"ShardTotal\":9, \"enableSchedule\":true, \"topologyType\":\"static\" }' 更新 HoraeDB 集群 curl --location --request PUT 'http://127.0.0.1:8080/api/v1/clusters/{NewClusterName}' \\ --header 'Content-Type: application/json' \\ --data '{ \"nodeCount\":28, \"shardTotal\":128, \"enableSchedule\":true, \"topologyType\":\"dynamic\" }' 列出 HoraeDB 集群 curl --location 'http://127.0.0.1:8080/api/v1/clusters' 修改 enableSchedule curl --location --request PUT 'http://127.0.0.1:8080/api/v1/clusters/{ClusterName}/enableSchedule' \\ --header 'Content-Type: application/json' \\ --data '{ \"enable\":true }' 查询 enableSchedule curl --location 'http://127.0.0.1:8080/api/v1/clusters/{ClusterName}/enableSchedule' 更新限流器 curl --location --request PUT 'http://127.0.0.1:8080/api/v1/flowLimiter' \\ --header 'Content-Type: application/json' \\ --data '{ \"limit\":1000, \"burst\":10000, \"enable\":true }' 查询限流器信息 curl --location 'http://127.0.0.1:8080/api/v1/flowLimiter' HoraeMeta 列出节点 curl --location 'http://127.0.0.1:8080/api/v1/etcd/member' HoraeMeta 节点切主 curl --location 'http://127.0.0.1:8080/api/v1/etcd/moveLeader' \\ --header 'Content-Type: application/json' \\ --data '{ \"memberName\":\"meta1\" }' HoraeMeta 节点扩容 curl --location --request PUT 'http://127.0.0.1:8080/api/v1/etcd/member' \\ --header 'Content-Type: application/json' \\ --data '{ \"memberAddrs\":[\"http://127.0.0.1:42380\"] }' HoraeMeta 替换节点 curl --location 'http://127.0.0.1:8080/api/v1/etcd/member' \\ --header 'Content-Type: application/json' \\ --data '{ \"oldMemberName\":\"meta0\", \"newMemberAddr\":[\"http://127.0.0.1:42380\"] }' ","categories":"","description":"","excerpt":"集群运维接口的使用前提是，HoraeDB 部署为使用 HoraeMeta 的集群模式。\n运维接口 注意： …","ref":"/cn/docs/user-guide/operation/horaemeta/","tags":"","title":"集群运维"},{"body":"增加黑名单 如果你想限制某个表的查询，可以把表名加到 read_block_list 中。\n示例如下：\n1 2 3 4 5 6 7 curl --location --request POST 'http://localhost:5000/admin/block' \\ --header 'Content-Type: application/json' \\ -d '{ \"operation\":\"Add\", \"write_block_list\":[], \"read_block_list\":[\"my_table\"] }' 返回结果：\n1 2 3 4 { \"write_block_list\": [], \"read_block_list\": [\"my_table\"] } 设置黑名单 设置黑名单的操作首先会清理已有的列表，然后再把新的表设置进去。\n示例如下：\n1 2 3 4 5 6 7 curl --location --request POST 'http://localhost:5000/admin/block' \\ --header 'Content-Type: application/json' \\ -d '{ \"operation\":\"Set\", \"write_block_list\":[], \"read_block_list\":[\"my_table1\",\"my_table2\"] }' 返回结果：\n1 2 3 4 { \"write_block_list\": [], \"read_block_list\": [\"my_table1\", \"my_table2\"] } 删除黑名单 如果你想把表从黑名单中移除，可以使用如下命令：\n1 2 3 4 5 6 7 curl --location --request POST 'http://localhost:5000/admin/block' \\ --header 'Content-Type: application/json' \\ -d '{ \"operation\":\"Remove\", \"write_block_list\":[], \"read_block_list\":[\"my_table1\"] }' 返回结果：\n1 2 3 4 { \"write_block_list\": [], \"read_block_list\": [\"my_table2\"] } ","categories":"","description":"","excerpt":"增加黑名单 如果你想限制某个表的查询，可以把表名加到 read_block_list 中。\n示例如下：\n1 2 3 4 5 6 7 curl …","ref":"/cn/docs/user-guide/operation/block_list/","tags":"","title":"黑名单"}]