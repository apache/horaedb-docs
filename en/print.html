<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>HoraeDB Documentation</title>
        <meta name="robots" content="noindex" />


        <!-- Custom HTML head -->
        
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="style.css">

    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item affix "><li class="part-title">Introduction</li><li class="chapter-item "><a href="about.html"><strong aria-hidden="true">1.</strong> What is HoraeDB</a></li><li class="chapter-item "><a href="quick_start.html"><strong aria-hidden="true">2.</strong> Quick Start</a></li><li class="chapter-item affix "><li class="part-title">User Guide</li><li class="chapter-item "><a href="sql/README.html"><strong aria-hidden="true">3.</strong> SQL Syntax</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="sql/model/README.html"><strong aria-hidden="true">3.1.</strong> Data Model</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="sql/model/data_types.html"><strong aria-hidden="true">3.1.1.</strong> Data Types</a></li><li class="chapter-item "><a href="sql/model/special_columns.html"><strong aria-hidden="true">3.1.2.</strong> Special Columns</a></li></ol></li><li class="chapter-item "><a href="sql/identifier.html"><strong aria-hidden="true">3.2.</strong> Identifier</a></li><li class="chapter-item "><a href="sql/ddl/README.html"><strong aria-hidden="true">3.3.</strong> Data Definition Statements</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="sql/ddl/create_table.html"><strong aria-hidden="true">3.3.1.</strong> CREATE TABLE</a></li><li class="chapter-item "><a href="sql/ddl/alter_table.html"><strong aria-hidden="true">3.3.2.</strong> ALTER TABLE</a></li><li class="chapter-item "><a href="sql/ddl/drop_table.html"><strong aria-hidden="true">3.3.3.</strong> DROP TABLE</a></li></ol></li><li class="chapter-item "><a href="sql/dml/README.html"><strong aria-hidden="true">3.4.</strong> Data Manipulation Statements</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="sql/dml/insert.html"><strong aria-hidden="true">3.4.1.</strong> INSERT</a></li><li class="chapter-item "><a href="sql/dml/select.html"><strong aria-hidden="true">3.4.2.</strong> SELECT</a></li></ol></li><li class="chapter-item "><a href="sql/utility.html"><strong aria-hidden="true">3.5.</strong> Utility Statements</a></li><li class="chapter-item "><a href="sql/engine_options.html"><strong aria-hidden="true">3.6.</strong> Engine Options</a></li><li class="chapter-item "><a href="sql/functions/scalar_functions.html"><strong aria-hidden="true">3.7.</strong> Scalar Functions</a></li><li class="chapter-item "><a href="sql/functions/aggregate_functions.html"><strong aria-hidden="true">3.8.</strong> Aggregate Functions</a></li></ol></li><li class="chapter-item "><a href="cluster_deployment/README.html"><strong aria-hidden="true">4.</strong> Cluster Deployment</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="cluster_deployment/platform.html"><strong aria-hidden="true">4.1.</strong> Supported Platform</a></li><li class="chapter-item "><a href="cluster_deployment/no_meta.html"><strong aria-hidden="true">4.2.</strong> NoMeta Mode</a></li><li class="chapter-item "><a href="cluster_deployment/with_meta.html"><strong aria-hidden="true">4.3.</strong> WithMeta Mode</a></li></ol></li><li class="chapter-item "><a href="sdk/README.html"><strong aria-hidden="true">5.</strong> SDK</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="sdk/java.html"><strong aria-hidden="true">5.1.</strong> Java</a></li><li class="chapter-item "><a href="sdk/go.html"><strong aria-hidden="true">5.2.</strong> Go</a></li><li class="chapter-item "><a href="sdk/python.html"><strong aria-hidden="true">5.3.</strong> Python</a></li><li class="chapter-item "><a href="sdk/rust.html"><strong aria-hidden="true">5.4.</strong> Rust</a></li></ol></li><li class="chapter-item "><a href="operation/README.html"><strong aria-hidden="true">6.</strong> Operation</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="operation/table.html"><strong aria-hidden="true">6.1.</strong> Table</a></li><li class="chapter-item "><a href="operation/system_table.html"><strong aria-hidden="true">6.2.</strong> System Table</a></li><li class="chapter-item "><a href="operation/block_list.html"><strong aria-hidden="true">6.3.</strong> Block List</a></li><li class="chapter-item "><a href="operation/observability.html"><strong aria-hidden="true">6.4.</strong> Observability</a></li><li class="chapter-item "><a href="operation/horaemeta.html"><strong aria-hidden="true">6.5.</strong> HoraeMeta</a></li></ol></li><li class="chapter-item "><a href="ecosystem/README.html"><strong aria-hidden="true">7.</strong> Ecosystem</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="ecosystem/prometheus.html"><strong aria-hidden="true">7.1.</strong> Prometheus</a></li><li class="chapter-item "><a href="ecosystem/influxdb.html"><strong aria-hidden="true">7.2.</strong> InfluxDB</a></li><li class="chapter-item "><a href="ecosystem/opentsdb.html"><strong aria-hidden="true">7.3.</strong> OpenTSDB</a></li></ol></li><li class="chapter-item "><li class="part-title">Dev Guide</li><li class="chapter-item "><a href="dev/platform.html"><strong aria-hidden="true">8.</strong> Supported Platform</a></li><li class="chapter-item "><a href="dev/compile_run.html"><strong aria-hidden="true">9.</strong> Compile and Running</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="dev/profiling.html"><strong aria-hidden="true">9.1.</strong> Profile</a></li></ol></li><li class="chapter-item "><a href="dev/conventional_commit.html"><strong aria-hidden="true">10.</strong> Conventional Commit</a></li><li class="chapter-item "><a href="dev/style_guide.html"><strong aria-hidden="true">11.</strong> Style guide</a></li><li class="chapter-item "><a href="dev/roadmap.html"><strong aria-hidden="true">12.</strong> Roadmap</a></li><li class="chapter-item affix "><li class="part-title">Technical and Design</li><li class="chapter-item "><a href="design/architecture.html"><strong aria-hidden="true">13.</strong> Architecture</a></li><li class="chapter-item "><a href="design/clustering.html"><strong aria-hidden="true">14.</strong> Cluster</a></li><li class="chapter-item "><a href="design/storage.html"><strong aria-hidden="true">15.</strong> Storage</a></li><li class="chapter-item "><a href="design/wal.html"><strong aria-hidden="true">16.</strong> WAL</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="design/wal_on_rocksdb.html"><strong aria-hidden="true">16.1.</strong> WAL on RocksDB</a></li><li class="chapter-item "><a href="design/wal_on_kafka.html"><strong aria-hidden="true">16.2.</strong> WAL on Kafka</a></li></ol></li><li class="chapter-item "><a href="design/table_partitioning.html"><strong aria-hidden="true">17.</strong> Table Partitioning</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">HoraeDB Documentation</h1>

                    <div class="right-buttons">

                        <button id="lang-toggle" class="icon-button" type="button" title="Change language" aria-label="Change language" aria-haspopup="true" aria-expanded="false" aria-controls="lang-list" >
<!--                             <i class="fa fa-globe"></i> -->
                    <i>
                        <svg t="1675840511805" class="icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="2677" width="16" height="16"><path d="M511.573333 85.333333C276.053333 85.333333 85.333333 276.48 85.333333 512s190.72 426.666667 426.24 426.666667C747.52 938.666667 938.666667 747.52 938.666667 512S747.52 85.333333 511.573333 85.333333z m295.68 256h-125.866666a667.733333 667.733333 0 0 0-58.88-151.893333A342.613333 342.613333 0 0 1 807.253333 341.333333zM512 172.373333c35.413333 51.2 63.146667 107.946667 81.493333 168.96h-162.986666c18.346667-61.013333 46.08-117.76 81.493333-168.96zM181.76 597.333333C174.933333 570.026667 170.666667 541.44 170.666667 512s4.266667-58.026667 11.093333-85.333333h144.213333c-3.413333 28.16-5.973333 56.32-5.973333 85.333333 0 29.013333 2.56 57.173333 5.973333 85.333333H181.76z m34.986667 85.333334h125.866666c13.653333 53.333333 33.28 104.533333 58.88 151.893333A340.778667 340.778667 0 0 1 216.746667 682.666667z m125.866666-341.333334H216.746667a340.778667 340.778667 0 0 1 184.746666-151.893333A667.733333 667.733333 0 0 0 342.613333 341.333333zM512 851.626667c-35.413333-51.2-63.146667-107.946667-81.493333-168.96h162.986666c-18.346667 61.013333-46.08 117.76-81.493333 168.96zM611.84 597.333333H412.16c-3.84-28.16-6.826667-56.32-6.826667-85.333333 0-29.013333 2.986667-57.6 6.826667-85.333333h199.68c3.84 27.733333 6.826667 56.32 6.826667 85.333333 0 29.013333-2.986667 57.173333-6.826667 85.333333z m10.666667 237.226667c25.6-47.36 45.226667-98.56 58.88-151.893333h125.866666a342.613333 342.613333 0 0 1-184.746666 151.893333zM698.026667 597.333333c3.413333-28.16 5.973333-56.32 5.973333-85.333333 0-29.013333-2.56-57.173333-5.973333-85.333333h144.213333c6.826667 27.306667 11.093333 55.893333 11.093333 85.333333s-4.266667 58.026667-11.093333 85.333333h-144.213333z" fill="#000000" fill-opacity=".87" p-id="2678"></path></svg>
                        <a id="lang_comment"></a>
                    </i>

                        </button>
                        <ul id="lang-list" class="theme-popup" style="left: auto;" aria-label="Languages" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="en">English</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="cn">中文</button></li>
                        </ul>

                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://github.com/apache/incubator-horaedb" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>

                    </div>
                </div>

                <script>
                window.onload = function(){
                    var path_lang = window.location.pathname.split('/')[1];
                    document.getElementById('lang_comment').innerHTML = path_lang=='cn'?"Language":"语言";
                };
                </script>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <!-- Page table of contents -->
                        <div class="sidetoc"><nav class="pagetoc"></nav></div>

                        <p><img src="https://github.com/apache/incubator-horaedb/raw/main/docs/logo/horaedb-banner-white-small.png" alt="HoraeDB" /></p>
<p><img src="https://img.shields.io/badge/license-Apache--2.0-green.svg" alt="License" />
<a href="https://github.com/apache/incubator-horaedb/actions/workflows/ci.yml"><img src="https://github.com/apache/incubator-horaedb/actions/workflows/ci.yml/badge.svg" alt="CI" /></a>
<a href="https://github.com/apache/incubator-horaedb/issues"><img src="https://img.shields.io/github/issues/apache/incubator-horaedb" alt="OpenIssue" /></a></p>
<p>HoraeDB is a high-performance, distributed, cloud native time-series database.</p>
<h1 id="motivation"><a class="header" href="#motivation">Motivation</a></h1>
<p>In the classic timeseries database, the <code>Tag</code> columns (InfluxDB calls them <code>Tag</code> and Prometheus calls them <code>Label</code>) are normally indexed by generating an inverted index. However, it is found that the cardinality of <code>Tag</code> varies in different scenarios. And in some scenarios the cardinality of <code>Tag</code> is very high, and it takes a very high cost to store and retrieve the inverted index. On the other hand, it is observed that scanning+pruning often used by the analytical databases can do a good job to handle such these scenarios.</p>
<p>The basic design idea of HoraeDB is to adopt a hybrid storage format and the corresponding query method for a better performance in processing both timeseries and analytic workloads.</p>
<h1 id="how-does-horaedb-work"><a class="header" href="#how-does-horaedb-work">How does HoraeDB work?</a></h1>
<ul>
<li>See <a href="quick_start.html">Quick Start</a> to learn about how to get started</li>
<li>For data model of HoraeDB, see <a href="sql/model/README.html">Data Model</a></li>
<li>For the supported SQL data types, operators, and commands, please navigate to <a href="sql/README.html">SQL reference</a></li>
<li>For the supported SDKs, please navigate to <a href="sdk/README.html">SDK</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="quick-start"><a class="header" href="#quick-start">Quick Start</a></h1>
<p>This page shows you how to get started with HoraeDB quickly. You'll start a standalone HoraeDB server, and then insert and read some sample data using SQL.</p>
<h2 id="start-server"><a class="header" href="#start-server">Start server</a></h2>
<p><a href="https://github.com/apache/incubator-horaedb/pkgs/container/horaedb-server">HoraeDB docker image</a> is the easiest way to get started, if you haven't installed Docker, go <a href="https://www.docker.com/products/docker-desktop/">there</a> to install it first.</p>
<blockquote>
<p>Note: please choose tag version &gt;= v1.0.0, others are mainly for testing.</p>
</blockquote>
<p>You can use command below to start a standalone server</p>
<pre><code class="language-bash">docker run -d --name horaedb-server \
  -p 8831:8831 \
  -p 3307:3307 \
  -p 5440:5440 \
  ghcr.io/apache/horaedb-server:nightly-20231222-f57b3827
</code></pre>
<p>HoraeDB will listen three ports when start:</p>
<ul>
<li>8831, gRPC port</li>
<li>3307, MySQL port</li>
<li>5440, HTTP port</li>
</ul>
<p>The easiest to use is HTTP, so sections below will use it for demo. For production environments, gRPC/MySQL are recommended.</p>
<h3 id="customize-docker-configuration"><a class="header" href="#customize-docker-configuration">Customize docker configuration</a></h3>
<p>Refer the command as below, you can customize the configuration of horaedb-server in docker, and mount the data directory <code>/data</code> to the hard disk of the docker host machine.</p>
<pre><code>wget -c https://raw.githubusercontent.com/apache/incubator-horaedb/main/docs/minimal.toml -O horaedb.toml

sed -i 's/\/tmp\/horaedb/\/data/g' horaedb.toml

docker run -d --name horaedb-server \
  -p 8831:8831 \
  -p 3307:3307 \
  -p 5440:5440 \
  -v ./horaedb.toml:/etc/horaedb/horaedb.toml \
  -v ./data:/data \
  ghcr.io/apache/horaedb-server:nightly-20231222-f57b3827
</code></pre>
<h2 id="write-and-read-data"><a class="header" href="#write-and-read-data">Write and read data</a></h2>
<h3 id="create-table"><a class="header" href="#create-table">Create table</a></h3>
<pre><code class="language-shell">curl --location --request POST 'http://127.0.0.1:5440/sql' \
-d '
CREATE TABLE `demo` (
    `name` string TAG,
    `value` double NOT NULL,
    `t` timestamp NOT NULL,
    timestamp KEY (t))
ENGINE=Analytic
  with
(enable_ttl=&quot;false&quot;)
'
</code></pre>
<h3 id="write-data"><a class="header" href="#write-data">Write data</a></h3>
<pre><code class="language-shell">curl --location --request POST 'http://127.0.0.1:5440/sql' \
-d '
INSERT INTO demo (t, name, value)
    VALUES (1651737067000, &quot;horaedb&quot;, 100)
'
</code></pre>
<h3 id="read-data"><a class="header" href="#read-data">Read data</a></h3>
<pre><code class="language-shell">curl --location --request POST 'http://127.0.0.1:5440/sql' \
-d '
SELECT
    *
FROM
    `demo`
'
</code></pre>
<h3 id="show-create-table"><a class="header" href="#show-create-table">Show create table</a></h3>
<pre><code class="language-shell">curl --location --request POST 'http://127.0.0.1:5440/sql' \
-d '
SHOW CREATE TABLE `demo`
'
</code></pre>
<h3 id="drop-table"><a class="header" href="#drop-table">Drop table</a></h3>
<pre><code class="language-shell">curl --location --request POST 'http://127.0.0.1:5440/sql' \
-d '
DROP TABLE `demo`
'
</code></pre>
<h2 id="using-the-sdks"><a class="header" href="#using-the-sdks">Using the SDKs</a></h2>
<p>See <a href="./sdk/README.html">sdk</a></p>
<h2 id="next-step"><a class="header" href="#next-step">Next Step</a></h2>
<p>Congrats, you have finished this tutorial. For more information about HoraeDB, see the following:</p>
<ul>
<li><a href="sql/README.html">SQL Syntax</a></li>
<li><a href="cluster_deployment/README.html">Deployment</a></li>
<li><a href="operation/README.html">Operation</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="sql-syntax"><a class="header" href="#sql-syntax">SQL Syntax</a></h1>
<p>This chapter introduces the SQL statements of HoraeDB.</p>
<ul>
<li><a href="sql/model/README.html">Data Model</a>
<ul>
<li><a href="sql/model/data_types.html">Data Types</a></li>
<li><a href="sql/model/special_columns.html">Special Columns</a></li>
</ul>
</li>
<li><a href="sql/identifier.html">Identifier</a></li>
<li><a href="sql/ddl/README.html">Data Definition Statements</a>
<ul>
<li><a href="sql/ddl/create_table.html">CREATE TABLE</a></li>
<li><a href="sql/ddl/alter_table.html">ALTER TABLE</a></li>
<li><a href="sql/ddl/drop_table.html">DROP TABLE</a></li>
</ul>
</li>
<li><a href="sql/dml/README.html">Data Manipulation Statements</a>
<ul>
<li><a href="sql/dml/insert.html">INSERT</a></li>
<li><a href="sql/dml/select.html">SELECT</a></li>
</ul>
</li>
<li><a href="sql/utility.html">Utility Statements</a></li>
<li><a href="sql/engine_options.html">Engine Options</a></li>
<li><a href="sql/functions/scalar_functions.html">Scalar Functions</a></li>
<li><a href="sql/functions/aggregate_functions.html">Aggregate Functions</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="data-model"><a class="header" href="#data-model">Data Model</a></h1>
<p>This chapter introduces the data model of HoraeDB.</p>
<ul>
<li><a href="sql/model/data_types.html">Data Types</a></li>
<li><a href="sql/model/special_columns.html">Special Columns</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="data-types"><a class="header" href="#data-types">Data Types</a></h1>
<p>HoraeDB implements table model, and the supported data types are similar to MySQL.
The following table lists the mapping relationship between MySQL and HoraeDB.</p>
<h2 id="support-data-typecase-insensitive"><a class="header" href="#support-data-typecase-insensitive">Support Data Type(case-insensitive)</a></h2>
<div class="table-wrapper"><table><thead><tr><th>SQL</th><th>HoraeDB</th></tr></thead><tbody>
<tr><td>null</td><td>Null</td></tr>
<tr><td>timestamp</td><td>Timestamp</td></tr>
<tr><td>double</td><td>Double</td></tr>
<tr><td>float</td><td>Float</td></tr>
<tr><td>string</td><td>String</td></tr>
<tr><td>Varbinary</td><td>Varbinary</td></tr>
<tr><td>uint64</td><td>UInt64</td></tr>
<tr><td>uint32</td><td>UInt32</td></tr>
<tr><td>uint16</td><td>UInt16</td></tr>
<tr><td>uint8</td><td>UInt8</td></tr>
<tr><td>int64/bigint</td><td>Int64</td></tr>
<tr><td>int32/int</td><td>Int32</td></tr>
<tr><td>int16/smallint</td><td>Int16</td></tr>
<tr><td>int8/tinyint</td><td>Int8</td></tr>
<tr><td>boolean</td><td>Boolean</td></tr>
<tr><td>date</td><td>Date</td></tr>
<tr><td>time</td><td>Time</td></tr>
</tbody></table>
</div><div style="break-before: page; page-break-before: always;"></div><h1 id="special-columns"><a class="header" href="#special-columns">Special Columns</a></h1>
<p>Tables in HoraeDB have the following constraints:</p>
<ul>
<li>Primary key is required</li>
<li>The primary key must contain a time column, and can only contain one time column</li>
<li>The primary key must be non-null, so all columns in primary key must be non-null.</li>
</ul>
<h2 id="timestamp-column"><a class="header" href="#timestamp-column">Timestamp Column</a></h2>
<p>Tables in HoraeDB must have one timestamp column maps to timestamp in timeseries data, such as timestamp in OpenTSDB/Prometheus.
The timestamp column can be set with <code>timestamp key</code> keyword, like <code>TIMESTAMP KEY(ts)</code>.</p>
<h2 id="tag-column"><a class="header" href="#tag-column">Tag Column</a></h2>
<p><code>Tag</code> is use to defined column as tag column, similar to tag in timeseries data, such as tag in OpenTSDB and label in Prometheus.</p>
<h2 id="primary-key"><a class="header" href="#primary-key">Primary Key</a></h2>
<p>The primary key is used for data deduplication and sorting. The primary key is composed of some columns and one time column.
The primary key can be set in the following some ways：</p>
<ul>
<li>use <code>primary key</code> keyword</li>
<li>use <code>tag</code> to auto generate TSID, HoraeDB will use <code>(TSID,timestamp)</code> as primary key</li>
<li>only set Timestamp column, HoraeDB will use <code>(timestamp)</code> as primary key</li>
</ul>
<p>Notice: If the primary key and tag are specified at the same time, then the tag column is just an additional information identification and will not affect the logic.</p>
<pre><code class="language-sql">CREATE TABLE with_primary_key(
  ts TIMESTAMP NOT NULL,
  c1 STRING NOT NULL,
  c2 STRING NULL,
  c4 STRING NULL,
  c5 STRING NULL,
  TIMESTAMP KEY(ts),
  PRIMARY KEY(c1, ts)
) ENGINE=Analytic WITH (ttl='7d');

CREATE TABLE with_tag(
    ts TIMESTAMP NOT NULL,
    c1 STRING TAG NOT NULL,
    c2 STRING TAG NULL,
    c3 STRING TAG NULL,
    c4 DOUBLE NULL,
    c5 STRING NULL,
    c6 STRING NULL,
    TIMESTAMP KEY(ts)
) ENGINE=Analytic WITH (ttl='7d');

CREATE TABLE with_timestamp(
    ts TIMESTAMP NOT NULL,
    c1 STRING NOT NULL,
    c2 STRING NULL,
    c3 STRING NULL,
    c4 DOUBLE NULL,
    c5 STRING NULL,
    c6 STRING NULL,
    TIMESTAMP KEY(ts)
) ENGINE=Analytic WITH (ttl='7d');
</code></pre>
<h2 id="tsid"><a class="header" href="#tsid">TSID</a></h2>
<p>If <code>primary key</code>is not set, and tag columns is provided, TSID will auto generated from hash of tag columns.
In essence, this is also a mechanism for automatically generating id.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="identifier"><a class="header" href="#identifier">Identifier</a></h1>
<p>Identifier in HoraeDB can be used as table name, column name etc. It cannot be preserved keywords or start with number and punctuation symbols. HoraeDB allows to quote identifiers with back quotes (`). In this case it can be any string like <code>00_table</code> or <code>select</code>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="data-definition-statements"><a class="header" href="#data-definition-statements">Data Definition Statements</a></h1>
<p>This chapter introduces the data definition statements.</p>
<ul>
<li><a href="sql/ddl/./create_table.html">Create Table</a></li>
<li><a href="sql/ddl/./alter_table.html">Alter Table</a></li>
<li><a href="sql/ddl/./drop_table.html">Drop Table</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="create-table-1"><a class="header" href="#create-table-1">CREATE TABLE</a></h1>
<h2 id="basic-syntax"><a class="header" href="#basic-syntax">Basic syntax</a></h2>
<p>Basic syntax:</p>
<pre><code class="language-sql">CREATE TABLE [IF NOT EXISTS]
    table_name ( column_definitions )
    [partition_options]
    ENGINE = engine_type
    [WITH ( table_options )];
</code></pre>
<p>Column definition syntax:</p>
<pre><code class="language-sql">column_name column_type [[NOT] NULL] [TAG | TIMESTAMP KEY | PRIMARY KEY] [DICTIONARY] [COMMENT '']
</code></pre>
<p>Partition options syntax:</p>
<pre><code class="language-sql">PARTITION BY KEY (column_list) [PARTITIONS num]
</code></pre>
<p>Table options syntax are key-value pairs. Value should be quoted with quotation marks (<code>'</code>). E.g.:</p>
<pre><code class="language-sql">... WITH ( enable_ttl='false' )
</code></pre>
<h2 id="if-not-exists"><a class="header" href="#if-not-exists">IF NOT EXISTS</a></h2>
<p>Add <code>IF NOT EXISTS</code> to tell HoraeDB to ignore errors if the table name already exists.</p>
<h2 id="define-column"><a class="header" href="#define-column">Define Column</a></h2>
<p>A column's definition should at least contains the name and type parts. All supported types are listed <a href="sql/ddl/../model/data_types.html">here</a>.</p>
<p>Column is default be nullable. i.e. <code>NULL</code> keyword is implied. Adding <code>NOT NULL</code> constrains to make it required.</p>
<pre><code class="language-sql">-- this definition
a_nullable int
-- equals to
a_nullable int NULL

-- add NOT NULL to make it required
b_not_null NOT NULL
</code></pre>
<p>A column can be marked as <a href="sql/ddl/../model/special_columns.html">special column</a> with related keyword.</p>
<p>For string tag column, we recommend to define it as dictionary to reduce memory consumption:</p>
<pre><code class="language-sql">`tag1` string TAG DICTIONARY
</code></pre>
<h2 id="engine"><a class="header" href="#engine">Engine</a></h2>
<p>Specifies which engine this table belongs to. HoraeDB current support <code>Analytic</code> engine type. This attribute is immutable.</p>
<h2 id="partition-options"><a class="header" href="#partition-options">Partition Options</a></h2>
<blockquote>
<p>Note: This feature is only supported in distributed version.</p>
</blockquote>
<pre><code class="language-sql">CREATE TABLE ... PARTITION BY KEY
</code></pre>
<p>Example below creates a table with 8 partitions, and partitioned by <code>name</code>:</p>
<pre><code class="language-sql">CREATE TABLE `demo` (
    `name` string TAG COMMENT 'client username',
    `value` double NOT NULL,
    `t` timestamp NOT NULL,
    timestamp KEY (t)
)
    PARTITION BY KEY(name) PARTITIONS 8
    ENGINE=Analytic
    with (
    enable_ttl='false'
)
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="alter-table"><a class="header" href="#alter-table">ALTER TABLE</a></h1>
<p><code>ALTER TABLE</code> can change the schema or options of a table.</p>
<h2 id="alter-table-schema"><a class="header" href="#alter-table-schema">ALTER TABLE SCHEMA</a></h2>
<p>HoraeDB current supports <code>ADD COLUMN</code> to alter table schema.</p>
<pre><code class="language-sql">-- create a table and add a column to it
CREATE TABLE `t`(a int, t timestamp NOT NULL, TIMESTAMP KEY(t)) ENGINE = Analytic;
ALTER TABLE `t` ADD COLUMN (b string);
</code></pre>
<p>It now becomes:</p>
<pre><code>-- DESCRIBE TABLE `t`;

name    type        is_primary  is_nullable is_tag

t       timestamp   true        false       false
tsid    uint64      true        false       false
a       int         false       true        false
b       string      false       true        false
</code></pre>
<h2 id="alter-table-options"><a class="header" href="#alter-table-options">ALTER TABLE OPTIONS</a></h2>
<p>HoraeDB current supports <code>MODIFY SETTING</code> to alter table schema.</p>
<pre><code class="language-sql">-- create a table and add a column to it
CREATE TABLE `t`(a int, t timestamp NOT NULL, TIMESTAMP KEY(t)) ENGINE = Analytic;
ALTER TABLE `t` MODIFY SETTING write_buffer_size='300M';
</code></pre>
<p>The SQL above tries to modify the <code>write_buffer_size</code> of the table, and the table's option becomes:</p>
<pre><code class="language-sql">CREATE TABLE `t` (`tsid` uint64 NOT NULL, `t` timestamp NOT NULL, `a` int, PRIMARY KEY(tsid,t), TIMESTAMP KEY(t)) ENGINE=Analytic WITH(arena_block_size='2097152', compaction_strategy='default', compression='ZSTD', enable_ttl='true', num_rows_per_row_group='8192', segment_duration='', storage_format='AUTO', ttl='7d', update_mode='OVERWRITE', write_buffer_size='314572800')
</code></pre>
<p>Besides, the <code>ttl</code> can be altered from 7 days to 10 days by such SQL:</p>
<pre><code class="language-sql">ALTER TABLE `t` MODIFY SETTING ttl='10d';
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="drop-table-1"><a class="header" href="#drop-table-1">DROP TABLE</a></h1>
<h2 id="basic-syntax-1"><a class="header" href="#basic-syntax-1">Basic syntax</a></h2>
<p>Basic syntax:</p>
<pre><code class="language-sql">DROP TABLE [IF EXISTS] table_name
</code></pre>
<p><code>Drop Table</code> removes a specific table. This statement should be used with caution, because it removes both the table definition and table data, and this removal is not recoverable.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="data-manipulation-statements"><a class="header" href="#data-manipulation-statements">Data Manipulation Statements</a></h1>
<p>This chapter introduces the data manipulation statements.</p>
<ul>
<li><a href="sql/dml/./insert.html">Insert</a></li>
<li><a href="sql/dml/./select.html">Select</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="insert"><a class="header" href="#insert">INSERT</a></h1>
<h2 id="basic-syntax-2"><a class="header" href="#basic-syntax-2">Basic syntax</a></h2>
<p>Basic syntax:</p>
<pre><code class="language-sql">INSERT [INTO] tbl_name
    [(col_name [, col_name] ...)]
    { {VALUES | VALUE} (value_list) [, (value_list)] ... }
</code></pre>
<p><code>INSERT</code> inserts new rows into a HoraeDB table. Here is an example:</p>
<pre><code class="language-sql">INSERT INTO demo(`time_stammp`, tag1) VALUES(1667374200022, 'horaedb')
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="select"><a class="header" href="#select">SELECT</a></h1>
<h2 id="basic-syntax-3"><a class="header" href="#basic-syntax-3">Basic syntax</a></h2>
<p>Basic syntax (parts between <code>[]</code> are optional):</p>
<pre><code class="language-sql">SELECT select_expr [, select_expr] ...
    FROM table_name
    [WHERE where_condition]
    [GROUP BY {col_name | expr} ... ]
    [ORDER BY {col_name | expr}
    [ASC | DESC]
    [LIMIT [offset,] row_count ]
</code></pre>
<p><code>Select</code> syntax in HoraeDB is similar to mysql, here is an example:</p>
<pre><code class="language-sql">SELECT * FROM `demo` WHERE time_stamp &gt; '2022-10-11 00:00:00' AND time_stamp &lt; '2022-10-12 00:00:00' LIMIT 10
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="utility-statements"><a class="header" href="#utility-statements">Utility Statements</a></h1>
<p>There are serval utilities SQL in HoraeDB that can help in table manipulation or query inspection.</p>
<h2 id="show-create-table-1"><a class="header" href="#show-create-table-1">SHOW CREATE TABLE</a></h2>
<pre><code class="language-sql">SHOW CREATE TABLE table_name;
</code></pre>
<p><code>SHOW CREATE TABLE</code> returns a <code>CREATE TABLE</code> DDL that will create a same table with the given one. Including columns, table engine and options. The schema and options shows in <code>CREATE TABLE</code> will based on the current version of the table. An example:</p>
<pre><code class="language-sql">-- create one table
CREATE TABLE `t` (a bigint, b int default 3, c string default 'x', d smallint null, t timestamp NOT NULL, TIMESTAMP KEY(t)) ENGINE = Analytic;
-- Result: affected_rows: 0

-- show how one table should be created.
SHOW CREATE TABLE `t`;

-- Result DDL:
CREATE TABLE `t` (
    `t` timestamp NOT NULL,
    `tsid` uint64 NOT NULL,
    `a` bigint,
    `b` int,
    `c` string,
    `d` smallint,
    PRIMARY KEY(t,tsid),
    TIMESTAMP KEY(t)
) ENGINE=Analytic WITH (
    arena_block_size='2097152',
    compaction_strategy='default',
    compression='ZSTD',
    enable_ttl='true',
    num_rows_per_row_group='8192',
    segment_duration='',
    ttl='7d',
    update_mode='OVERWRITE',
    write_buffer_size='33554432'
)
</code></pre>
<h2 id="describe"><a class="header" href="#describe">DESCRIBE</a></h2>
<pre><code class="language-sql">DESCRIBE table_name;
</code></pre>
<p><code>DESCRIBE</code> will show a detailed schema of one table. The attributes include column name and type, whether it is tag and primary key (todo: ref) and whether it's nullable. The auto created column <code>tsid</code> will also be included (todo: ref).</p>
<p>Example:</p>
<pre><code class="language-sql">CREATE TABLE `t`(a int, b string, t timestamp NOT NULL, TIMESTAMP KEY(t)) ENGINE = Analytic;

DESCRIBE TABLE `t`;
</code></pre>
<p>The result is:</p>
<pre><code>name    type        is_primary  is_nullable is_tag

t       timestamp   true        false       false
tsid    uint64      true        false       false
a       int         false       true        false
b       string      false       true        false
</code></pre>
<h2 id="explain"><a class="header" href="#explain">EXPLAIN</a></h2>
<pre><code class="language-sql">EXPLAIN query;
</code></pre>
<p><code>EXPLAIN</code> shows how a query will be executed. Add it to the beginning of a query like</p>
<pre><code class="language-sql">EXPLAIN SELECT max(value) AS c1, avg(value) AS c2 FROM `t` GROUP BY name;
</code></pre>
<p>will give</p>
<pre><code>logical_plan
Projection: #MAX(07_optimizer_t.value) AS c1, #AVG(07_optimizer_t.value) AS c2
  Aggregate: groupBy=[[#07_optimizer_t.name]], aggr=[[MAX(#07_optimizer_t.value), AVG(#07_optimizer_t.value)]]
    TableScan: 07_optimizer_t projection=Some([name, value])

physical_plan
ProjectionExec: expr=[MAX(07_optimizer_t.value)@1 as c1, AVG(07_optimizer_t.value)@2 as c2]
  AggregateExec: mode=FinalPartitioned, gby=[name@0 as name], aggr=[MAX(07_optimizer_t.value), AVG(07_optimizer_t.value)]
    CoalesceBatchesExec: target_batch_size=4096
      RepartitionExec: partitioning=Hash([Column { name: \&quot;name\&quot;, index: 0 }], 6)
        AggregateExec: mode=Partial, gby=[name@0 as name], aggr=[MAX(07_optimizer_t.value), AVG(07_optimizer_t.value)]
          ScanTable: table=07_optimizer_t, parallelism=8, order=None
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="options"><a class="header" href="#options">Options</a></h1>
<p>Options below can be used when create table for analytic engine</p>
<ul>
<li>
<p><code>enable_ttl</code>, <code>bool</code>. When enable TTL on a table, rows older than <code>ttl</code> will be deleted and can't be querid, default <code>true</code></p>
</li>
<li>
<p><code>ttl</code>, <code>duration</code>, lifetime of a row, only used when <code>enable_ttl</code> is <code>true</code>. default <code>7d</code>.</p>
</li>
<li>
<p><code>storage_format</code>, <code>string</code>. The underlying column's format. Availiable values:</p>
<ul>
<li><code>columnar</code>, default</li>
<li><code>hybrid</code>, Note: This feature is still in development, and it may change in the future.</li>
</ul>
<p>The meaning of those two values are in <a href="sql/engine_options.html#storage-format">Storage format</a> section.</p>
</li>
</ul>
<h2 id="storage-format"><a class="header" href="#storage-format">Storage Format</a></h2>
<p>There are mainly two formats supported in analytic engine. One is <code>columnar</code>, which is the traditional columnar format, with one table column in one physical column:</p>
<pre><code class="language-plaintext">| Timestamp | Device ID | Status Code | Tag 1 | Tag 2 |
| --------- |---------- | ----------- | ----- | ----- |
| 12:01     | A         | 0           | v1    | v1    |
| 12:01     | B         | 0           | v2    | v2    |
| 12:02     | A         | 0           | v1    | v1    |
| 12:02     | B         | 1           | v2    | v2    |
| 12:03     | A         | 0           | v1    | v1    |
| 12:03     | B         | 0           | v2    | v2    |
| .....     |           |             |       |       |
</code></pre>
<p>The other one is <code>hybrid</code>, an experimental format used to simulate row-oriented storage in columnar storage to accelerate classic time-series query.</p>
<p>In classic time-series user cases like IoT or DevOps, queries will typically first group their result by series id(or device id), then by timestamp. In order to achieve good performance in those scenarios, the data physical layout should match this style, so the <code>hybrid</code> format is proposed like this:</p>
<pre><code class="language-plaintext"> | Device ID | Timestamp           | Status Code | Tag 1 | Tag 2 | minTime | maxTime |
 |-----------|---------------------|-------------|-------|-------|---------|---------|
 | A         | [12:01,12:02,12:03] | [0,0,0]     | v1    | v1    | 12:01   | 12:03   |
 | B         | [12:01,12:02,12:03] | [0,1,0]     | v2    | v2    | 12:01   | 12:03   |
 | ...       |                     |             |       |       |         |         |
</code></pre>
<ul>
<li>Within one file, rows belonging to the same primary key(eg: series/device id) are collapsed into one row</li>
<li>The columns besides primary key are divided into two categories:
<ul>
<li><code>collapsible</code>, those columns will be collapsed into a list. Used to encode <code>fields</code> in time-series table
<ul>
<li>Note: only fixed-length type is supported now</li>
</ul>
</li>
<li><code>non-collapsible</code>, those columns should only contain one distinct value. Used to encode <code>tags</code> in time-series table
<ul>
<li>Note: only string type is supported now</li>
</ul>
</li>
</ul>
</li>
<li>Two more columns are added, <code>minTime</code> and <code>maxTime</code>. Those are used to cut unnecessary rows out in query.
<ul>
<li>Note: Not implemented yet.</li>
</ul>
</li>
</ul>
<h3 id="example"><a class="header" href="#example">Example</a></h3>
<pre><code class="language-sql">CREATE TABLE `device` (
    `ts` timestamp NOT NULL,
    `tag1` string tag,
    `tag2` string tag,
    `value1` double,
    `value2` int,
    timestamp KEY (ts)) ENGINE=Analytic
  with (
    enable_ttl = 'false',
    storage_format = 'hybrid'
);
</code></pre>
<p>This will create a table with hybrid format, users can inspect data format with <a href="https://formulae.brew.sh/formula/parquet-tools">parquet-tools</a>. The table above should have following parquet schema:</p>
<pre><code>message arrow_schema {
  optional group ts (LIST) {
    repeated group list {
      optional int64 item (TIMESTAMP(MILLIS,false));
    }
  }
  required int64 tsid (INTEGER(64,false));
  optional binary tag1 (STRING);
  optional binary tag2 (STRING);
  optional group value1 (LIST) {
    repeated group list {
      optional double item;
    }
  }
  optional group value2 (LIST) {
    repeated group list {
      optional int32 item;
    }
  }
}
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="scalar-functions"><a class="header" href="#scalar-functions">Scalar Functions</a></h1>
<p>HoraeDB SQL is implemented with <a href="https://github.com/CeresDB/arrow-datafusion">DataFusion</a>, Here is the list of scalar functions. See more detail, Refer to <a href="https://github.com/CeresDB/arrow-datafusion/blob/master/docs/source/user-guide/sql/scalar_functions.md">Datafusion</a></p>
<h2 id="math-functions"><a class="header" href="#math-functions">Math Functions</a></h2>
<div class="table-wrapper"><table><thead><tr><th>Function</th><th>Description</th></tr></thead><tbody>
<tr><td>abs(x)</td><td>absolute value</td></tr>
<tr><td>acos(x)</td><td>inverse cosine</td></tr>
<tr><td>asin(x)</td><td>inverse sine</td></tr>
<tr><td>atan(x)</td><td>inverse tangent</td></tr>
<tr><td>atan2(y, x)</td><td>inverse tangent of y / x</td></tr>
<tr><td>ceil(x)</td><td>nearest integer greater than or equal to argument</td></tr>
<tr><td>cos(x)</td><td>cosine</td></tr>
<tr><td>exp(x)</td><td>exponential</td></tr>
<tr><td>floor(x)</td><td>nearest integer less than or equal to argument</td></tr>
<tr><td>ln(x)</td><td>natural logarithm</td></tr>
<tr><td>log10(x)</td><td>base 10 logarithm</td></tr>
<tr><td>log2(x)</td><td>base 2 logarithm</td></tr>
<tr><td>power(base, exponent)</td><td>base raised to the power of exponent</td></tr>
<tr><td>round(x)</td><td>round to nearest integer</td></tr>
<tr><td>signum(x)</td><td>sign of the argument (-1, 0, +1)</td></tr>
<tr><td>sin(x)</td><td>sine</td></tr>
<tr><td>sqrt(x)</td><td>square root</td></tr>
<tr><td>tan(x)</td><td>tangent</td></tr>
<tr><td>trunc(x)</td><td>truncate toward zero</td></tr>
</tbody></table>
</div>
<h2 id="conditional-functions"><a class="header" href="#conditional-functions">Conditional Functions</a></h2>
<div class="table-wrapper"><table><thead><tr><th>Function</th><th>Description</th></tr></thead><tbody>
<tr><td>coalesce</td><td>Returns the first of its arguments that is not null. Null is returned only if all arguments are null. It is often used to substitute a default value for null values when data is retrieved for display.</td></tr>
<tr><td>nullif</td><td>Returns a null value if value1 equals value2; otherwise it returns value1. This can be used to perform the inverse operation of the coalesce expression.</td></tr>
</tbody></table>
</div>
<h2 id="string-functions"><a class="header" href="#string-functions">String Functions</a></h2>
<div class="table-wrapper"><table><thead><tr><th>Function</th><th>Description</th></tr></thead><tbody>
<tr><td>ascii</td><td>Returns the numeric code of the first character of the argument. In UTF8 encoding, returns the Unicode code point of the character. In other multibyte encodings, the argument must be an ASCII character.</td></tr>
<tr><td>bit_length</td><td>Returns the number of bits in a character string expression.</td></tr>
<tr><td>btrim</td><td>Removes the longest string containing any of the specified characters from the start and end of string.</td></tr>
<tr><td>char_length</td><td>Equivalent to length.</td></tr>
<tr><td>character_length</td><td>Equivalent to length.</td></tr>
<tr><td>concat</td><td>Concatenates two or more strings into one string.</td></tr>
<tr><td>concat_ws</td><td>Combines two values with a given separator.</td></tr>
<tr><td>chr</td><td>Returns the character based on the number code.</td></tr>
<tr><td>initcap</td><td>Capitalizes the first letter of each word in a string.</td></tr>
<tr><td>left</td><td>Returns the specified leftmost characters of a string.</td></tr>
<tr><td>length</td><td>Returns the number of characters in a string.</td></tr>
<tr><td>lower</td><td>Converts all characters in a string to their lower case equivalent.</td></tr>
<tr><td>lpad</td><td>Left-pads a string to a given length with a specific set of characters.</td></tr>
<tr><td>ltrim</td><td>Removes the longest string containing any of the characters in characters from the start of string.</td></tr>
<tr><td>md5</td><td>Calculates the MD5 hash of a given string.</td></tr>
<tr><td>octet_length</td><td>Equivalent to length.</td></tr>
<tr><td>repeat</td><td>Returns a string consisting of the input string repeated a specified number of times.</td></tr>
<tr><td>replace</td><td>Replaces all occurrences in a string of a substring with a new substring.</td></tr>
<tr><td>reverse</td><td>Reverses a string.</td></tr>
<tr><td>right</td><td>Returns the specified rightmost characters of a string.</td></tr>
<tr><td>rpad</td><td>Right-pads a string to a given length with a specific set of characters.</td></tr>
<tr><td>rtrim</td><td>Removes the longest string containing any of the characters in characters from the end of string.</td></tr>
<tr><td>digest</td><td>Calculates the hash of a given string.</td></tr>
<tr><td>split_part</td><td>Splits a string on a specified delimiter and returns the specified field from the resulting array.</td></tr>
<tr><td>starts_with</td><td>Checks whether a string starts with a particular substring.</td></tr>
<tr><td>strpos</td><td>Searches a string for a specific substring and returns its position.</td></tr>
<tr><td>substr</td><td>Extracts a substring of a string.</td></tr>
<tr><td>translate</td><td>Translates one set of characters into another.</td></tr>
<tr><td>trim</td><td>Removes the longest string containing any of the characters in characters from either the start or end of string.</td></tr>
<tr><td>upper</td><td>Converts all characters in a string to their upper case equivalent.</td></tr>
</tbody></table>
</div>
<h2 id="regular-expression-functions"><a class="header" href="#regular-expression-functions">Regular Expression Functions</a></h2>
<div class="table-wrapper"><table><thead><tr><th>Function</th><th>Description</th></tr></thead><tbody>
<tr><td>regexp_match</td><td>Determines whether a string matches a regular expression pattern.</td></tr>
<tr><td>regexp_replace</td><td>Replaces all occurrences in a string of a substring that matches a regular expression pattern with a new substring.</td></tr>
</tbody></table>
</div>
<h2 id="temporal-functions"><a class="header" href="#temporal-functions">Temporal Functions</a></h2>
<div class="table-wrapper"><table><thead><tr><th>Function</th><th>Description</th></tr></thead><tbody>
<tr><td>to_timestamp</td><td>Converts a string to type Timestamp(Nanoseconds, None).</td></tr>
<tr><td>to_timestamp_millis</td><td>Converts a string to type Timestamp(Milliseconds, None).</td></tr>
<tr><td>to_timestamp_micros</td><td>Converts a string to type Timestamp(Microseconds, None).</td></tr>
<tr><td>to_timestamp_seconds</td><td>Converts a string to type Timestamp(Seconds, None).</td></tr>
<tr><td>extract</td><td>Retrieves subfields such as year or hour from date/time values.</td></tr>
<tr><td>date_part</td><td>Retrieves subfield from date/time values.</td></tr>
<tr><td>date_trunc</td><td>Truncates date/time values to specified precision.</td></tr>
<tr><td>date_bin</td><td>Bin date/time values to specified precision.</td></tr>
<tr><td>from_unixtime</td><td>Converts Unix epoch to type Timestamp(Nanoseconds, None).</td></tr>
<tr><td>now</td><td>Returns current time as Timestamp(Nanoseconds, UTC).</td></tr>
</tbody></table>
</div>
<h2 id="other-functions"><a class="header" href="#other-functions">Other Functions</a></h2>
<div class="table-wrapper"><table><thead><tr><th>Function</th><th>Description</th></tr></thead><tbody>
<tr><td>array</td><td>Create an array.</td></tr>
<tr><td>arrow_typeof</td><td>Returns underlying type.</td></tr>
<tr><td>in_list</td><td>Check if value in list.</td></tr>
<tr><td>random</td><td>Generate random value.</td></tr>
<tr><td>sha224</td><td>sha224</td></tr>
<tr><td>sha256</td><td>sha256</td></tr>
<tr><td>sha384</td><td>sha384</td></tr>
<tr><td>sha512</td><td>sha512</td></tr>
<tr><td>to_hex</td><td>Convert to hex.</td></tr>
</tbody></table>
</div><div style="break-before: page; page-break-before: always;"></div><h1 id="aggregate-functions"><a class="header" href="#aggregate-functions">Aggregate Functions</a></h1>
<p>HoraeDB SQL is implemented with <a href="https://github.com/apache/arrow-datafusion">DataFusion</a>, Here is the list of aggregate functions. See more detail, Refer to <a href="https://github.com/apache/arrow-datafusion/blob/master/docs/source/user-guide/sql/aggregate_functions.md">Datafusion</a></p>
<h2 id="general"><a class="header" href="#general">General</a></h2>
<div class="table-wrapper"><table><thead><tr><th>Function</th><th>Description</th></tr></thead><tbody>
<tr><td>min</td><td>Returns the minimum value in a numerical column</td></tr>
<tr><td>max</td><td>Returns the maximum value in a numerical column</td></tr>
<tr><td>count</td><td>Returns the number of rows</td></tr>
<tr><td>avg</td><td>Returns the average of a numerical column</td></tr>
<tr><td>sum</td><td>Sums a numerical column</td></tr>
<tr><td>array_agg</td><td>Puts values into an array</td></tr>
</tbody></table>
</div>
<h2 id="statistical"><a class="header" href="#statistical">Statistical</a></h2>
<div class="table-wrapper"><table><thead><tr><th>Function</th><th>Description</th></tr></thead><tbody>
<tr><td>var / var_samp</td><td>Returns the variance of a given column</td></tr>
<tr><td>var_pop</td><td>Returns the population variance of a given column</td></tr>
<tr><td>stddev / stddev_samp</td><td>Returns the standard deviation of a given column</td></tr>
<tr><td>stddev_pop</td><td>Returns the population standard deviation of a given column</td></tr>
<tr><td>covar / covar_samp</td><td>Returns the covariance of a given column</td></tr>
<tr><td>covar_pop</td><td>Returns the population covariance of a given column</td></tr>
<tr><td>corr</td><td>Returns the correlation coefficient of a given column</td></tr>
</tbody></table>
</div>
<h2 id="approximate"><a class="header" href="#approximate">Approximate</a></h2>
<div class="table-wrapper"><table><thead><tr><th>Function</th><th>Description</th></tr></thead><tbody>
<tr><td>approx_distinct</td><td>Returns the approximate number (HyperLogLog) of distinct input values</td></tr>
<tr><td>approx_median</td><td>Returns the approximate median of input values. It is an alias of approx_percentile_cont(x, 0.5).</td></tr>
<tr><td>approx_percentile_cont</td><td>Returns the approximate percentile (TDigest) of input values, where p is a float64 between 0 and 1 (inclusive). It supports raw data as input and build Tdigest sketches during query time, and is approximately equal to approx_percentile_cont_with_weight(x, 1, p).</td></tr>
<tr><td>approx_percentile_cont_with_weight</td><td>Returns the approximate percentile (TDigest) of input values with weight, where w is weight column expression and p is a float64 between 0 and 1 (inclusive). It supports raw data as input or pre-aggregated TDigest sketches, then builds or merges Tdigest sketches during query time. TDigest sketches are a list of centroid (x, w), where x stands for mean and w stands for weight.</td></tr>
</tbody></table>
</div><div style="break-before: page; page-break-before: always;"></div><h1 id="cluster-deployment"><a class="header" href="#cluster-deployment">Cluster Deployment</a></h1>
<p>In the <a href="cluster_deployment/../quick_start.html">Quick Start</a> section, we have introduced the deployment of single HoraeDB instance.</p>
<p>Besides, as a distributed timeseries database, multiple HoraeDB instances can be deployed as a cluster to serve with high availability and scalability.</p>
<p>Currently, work about the integration with kubernetes is still in process, so HoraeDB cluster can only be deployed manually. And there are two modes of cluster deployment:</p>
<ul>
<li><a href="cluster_deployment/no_meta.html">NoMeta Mode(Only for testing)</a></li>
<li><a href="cluster_deployment/with_meta.html">WithMeta Mode</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><p>As an open source cloud-native, HoraeDB can be deployed in the Intel/ARM-based architecture server, and major virtualization environments.</p>
<div class="table-wrapper"><table><thead><tr><th style="text-align: center">OS</th><th style="text-align: center">status</th></tr></thead><tbody>
<tr><td style="text-align: center">Ubuntu LTS 16.06 or later</td><td style="text-align: center">✅</td></tr>
<tr><td style="text-align: center">CentOS 7.3 or later</td><td style="text-align: center">✅</td></tr>
<tr><td style="text-align: center">Red Hat Enterprise Linux 7.3 or later 7.x releases</td><td style="text-align: center">✅</td></tr>
<tr><td style="text-align: center">macOS 11 or later</td><td style="text-align: center">✅</td></tr>
<tr><td style="text-align: center">Windows</td><td style="text-align: center">❌</td></tr>
</tbody></table>
</div>
<ul>
<li>For production workloads, Linux is the preferred platform.</li>
<li>macOS is mainly used for development</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><p><strong>Note: This feature is for testing use only, not recommended for production use, related features may change in the future.</strong></p>
<h1 id="nometa"><a class="header" href="#nometa">NoMeta</a></h1>
<p>This guide shows how to deploy a HoraeDB cluster without HoraeMeta, but with static, rule-based routing.</p>
<p>The crucial point here is that HoraeDB server provides configurable routing function on table name so what we need is just a valid config containing routing rules which will be shipped to every HoraeDB instance in the cluster.</p>
<h2 id="target"><a class="header" href="#target">Target</a></h2>
<p>First, let's assume that our target is to deploy a cluster consisting of two HoraeDB instances on the same machine. And a large cluster of more HoraeDB instances can be deployed according to the two-instance example.</p>
<h2 id="prepare-config"><a class="header" href="#prepare-config">Prepare Config</a></h2>
<h3 id="basic"><a class="header" href="#basic">Basic</a></h3>
<p>Suppose the basic config of HoraeDB is:</p>
<pre><code class="language-toml">[server]
bind_addr = &quot;0.0.0.0&quot;
http_port = 5440
grpc_port = 8831

[logger]
level = &quot;info&quot;

[tracing]
dir = &quot;/tmp/horaedb&quot;

[analytic.storage.object_store]
type = &quot;Local&quot;
data_dir = &quot;/tmp/horaedb&quot;

[analytic.wal]
type = &quot;RocksDB&quot;
data_dir = &quot;/tmp/horaedb&quot;
</code></pre>
<p>In order to deploy two HoraeDB instances on the same machine, the config should choose different ports to serve and data directories to store data.</p>
<p>Say the <code>HoraeDB_0</code>'s config is:</p>
<pre><code class="language-toml">[server]
bind_addr = &quot;0.0.0.0&quot;
http_port = 5440
grpc_port = 8831

[logger]
level = &quot;info&quot;

[tracing]
dir = &quot;/tmp/horaedb_0&quot;

[analytic.storage.object_store]
type = &quot;Local&quot;
data_dir = &quot;/tmp/horaedb_0&quot;

[analytic.wal]
type = &quot;RocksDB&quot;
data_dir = &quot;/tmp/horaedb_0&quot;
</code></pre>
<p>Then the <code>HoraeDB_1</code>'s config is:</p>
<pre><code class="language-toml">[server]
bind_addr = &quot;0.0.0.0&quot;
http_port = 15440
grpc_port = 18831

[logger]
level = &quot;info&quot;

[tracing]
dir = &quot;/tmp/horaedb_1&quot;

[analytic.storage.object_store]
type = &quot;Local&quot;
data_dir = &quot;/tmp/horaedb_1&quot;

[analytic.wal]
type = &quot;RocksDB&quot;
data_dir = &quot;/tmp/horaedb_1&quot;
</code></pre>
<h3 id="schemashard-declaration"><a class="header" href="#schemashard-declaration">Schema&amp;Shard Declaration</a></h3>
<p>Then we should define the common part -- schema&amp;shard declaration and routing rules.</p>
<p>Here is the config for schema&amp;shard declaration:</p>
<pre><code class="language-toml">[cluster_deployment]
mode = &quot;NoMeta&quot;

[[cluster_deployment.topology.schema_shards]]
schema = 'public_0'
[[cluster_deployment.topology.schema_shards.shard_views]]
shard_id = 0
[cluster_deployment.topology.schema_shards.shard_views.endpoint]
addr = '127.0.0.1'
port = 8831
[[cluster_deployment.topology.schema_shards.shard_views]]
shard_id = 1
[cluster_deployment.topology.schema_shards.shard_views.endpoint]
addr = '127.0.0.1'
port = 8831

[[cluster_deployment.topology.schema_shards]]
schema = 'public_1'
[[cluster_deployment.topology.schema_shards.shard_views]]
shard_id = 0
[cluster_deployment.topology.schema_shards.shard_views.endpoint]
addr = '127.0.0.1'
port = 8831
[[cluster_deployment.topology.schema_shards.shard_views]]
shard_id = 1
[cluster_deployment.topology.schema_shards.shard_views.endpoint]
addr = '127.0.0.1'
port = 18831
</code></pre>
<p>In the config above, two schemas are declared:</p>
<ul>
<li><code>public_0</code> has two shards served by <code>HoraeDB_0</code>.</li>
<li><code>public_1</code> has two shards served by both <code>HoraeDB_0</code> and <code>HoraeDB_1</code>.</li>
</ul>
<h3 id="routing-rules"><a class="header" href="#routing-rules">Routing Rules</a></h3>
<p>Provided with schema&amp;shard declaration, routing rules can be defined and here is an example of prefix rule:</p>
<pre><code class="language-toml">[[cluster_deployment.route_rules.prefix_rules]]
schema = 'public_0'
prefix = 'prod_'
shard = 0
</code></pre>
<p>This rule means that all the table with <code>prod_</code> prefix belonging to <code>public_0</code> should be routed to <code>shard_0</code> of <code>public_0</code>, that is to say, <code>HoraeDB_0</code>. As for the other tables whose names are not prefixed by <code>prod_</code> will be routed by hash to both <code>shard_0</code> and <code>shard_1</code> of <code>public_0</code>.</p>
<p>Besides prefix rule, we can also define a hash rule:</p>
<pre><code class="language-toml">[[cluster_deployment.route_rules.hash_rules]]
schema = 'public_1'
shards = [0, 1]
</code></pre>
<p>This rule tells HoraeDB to route <code>public_1</code>'s tables to both <code>shard_0</code> and <code>shard_1</code> of <code>public_1</code>, that is to say, <code>HoraeDB0</code> and <code>HoraeDB_1</code>. And actually this is default routing behavior if no such rule provided for schema <code>public_1</code>.</p>
<p>For now, we can provide the full example config for <code>HoraeDB_0</code> and <code>HoraeDB_1</code>:</p>
<pre><code class="language-toml">[server]
bind_addr = &quot;0.0.0.0&quot;
http_port = 5440
grpc_port = 8831
mysql_port = 3307

[logger]
level = &quot;info&quot;

[tracing]
dir = &quot;/tmp/horaedb_0&quot;

[analytic.storage.object_store]
type = &quot;Local&quot;
data_dir = &quot;/tmp/horaedb_0&quot;

[analytic.wal]
type = &quot;RocksDB&quot;
data_dir = &quot;/tmp/horaedb_0&quot;

[cluster_deployment]
mode = &quot;NoMeta&quot;

[[cluster_deployment.topology.schema_shards]]
schema = 'public_0'
[[cluster_deployment.topology.schema_shards.shard_views]]
shard_id = 0
[cluster_deployment.topology.schema_shards.shard_views.endpoint]
addr = '127.0.0.1'
port = 8831
[[cluster_deployment.topology.schema_shards.shard_views]]
shard_id = 1
[cluster_deployment.topology.schema_shards.shard_views.endpoint]
addr = '127.0.0.1'
port = 8831

[[cluster_deployment.topology.schema_shards]]
schema = 'public_1'
[[cluster_deployment.topology.schema_shards.shard_views]]
shard_id = 0
[cluster_deployment.topology.schema_shards.shard_views.endpoint]
addr = '127.0.0.1'
port = 8831
[[cluster_deployment.topology.schema_shards.shard_views]]
shard_id = 1
[cluster_deployment.topology.schema_shards.shard_views.endpoint]
addr = '127.0.0.1'
port = 18831
</code></pre>
<pre><code class="language-toml">[server]
bind_addr = &quot;0.0.0.0&quot;
http_port = 15440
grpc_port = 18831
mysql_port = 13307

[logger]
level = &quot;info&quot;

[tracing]
dir = &quot;/tmp/horaedb_1&quot;

[analytic.storage.object_store]
type = &quot;Local&quot;
data_dir = &quot;/tmp/horaedb_1&quot;

[analytic.wal]
type = &quot;RocksDB&quot;
data_dir = &quot;/tmp/horaedb_1&quot;

[cluster_deployment]
mode = &quot;NoMeta&quot;

[[cluster_deployment.topology.schema_shards]]
schema = 'public_0'
[[cluster_deployment.topology.schema_shards.shard_views]]
shard_id = 0
[cluster_deployment.topology.schema_shards.shard_views.endpoint]
addr = '127.0.0.1'
port = 8831
[[cluster_deployment.topology.schema_shards.shard_views]]
shard_id = 1
[cluster_deployment.topology.schema_shards.shard_views.endpoint]
addr = '127.0.0.1'
port = 8831

[[cluster_deployment.topology.schema_shards]]
schema = 'public_1'
[[cluster_deployment.topology.schema_shards.shard_views]]
shard_id = 0
[cluster_deployment.topology.schema_shards.shard_views.endpoint]
addr = '127.0.0.1'
port = 8831
[[cluster_deployment.topology.schema_shards.shard_views]]
shard_id = 1
[cluster_deployment.topology.schema_shards.shard_views.endpoint]
addr = '127.0.0.1'
port = 18831
</code></pre>
<p>Let's name the two different config files as <code>config_0.toml</code> and <code>config_1.toml</code> but you should know in the real environment the different HoraeDB instances can be deployed across different machines, that is to say, there is no need to choose different ports and data directories for different HoraeDB instances so that all the HoraeDB instances can share one exactly <strong>same</strong> config file.</p>
<h2 id="start-horaedbs"><a class="header" href="#start-horaedbs">Start HoraeDBs</a></h2>
<p>After the configs are prepared, what we should to do is to start HoraeDB container with the specific config.</p>
<p>Just run the commands below:</p>
<pre><code class="language-shell">sudo docker run -d -t --name horaedb_0 -p 5440:5440 -p 8831:8831 -v $(pwd)/config_0.toml:/etc/horaedb/horaedb.toml horaedb/horaedb-server
sudo docker run -d -t --name horaedb_1 -p 15440:15440 -p 18831:18831 -v $(pwd)/config_1.toml:/etc/horaedb/horaedb.toml horaedb/horaedb-server
</code></pre>
<p>After the two containers are created and starting running, read and write requests can be served by the two-instances HoraeDB cluster.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="withmeta"><a class="header" href="#withmeta">WithMeta</a></h1>
<p>This guide shows how to deploy a HoraeDB cluster with HoraeMeta. And with the HoraeMeta, the whole HoraeDB cluster will feature: high availability, load balancing and horizontal scalability if the underlying storage used by HoraeDB is separated service.</p>
<h2 id="deploy-horaemeta"><a class="header" href="#deploy-horaemeta">Deploy HoraeMeta</a></h2>
<h3 id="introduce"><a class="header" href="#introduce">Introduce</a></h3>
<p>HoraeMeta is one of the core services of HoraeDB distributed mode, it is used to manage and schedule the HoraeDB cluster. By the way, the high availability of HoraeMeta is ensured by embedding <a href="https://github.com/etcd-io/etcd">ETCD</a>. Also, the ETCD service is provided for HoraeDB servers to manage the distributed shard locks.</p>
<h3 id="build"><a class="header" href="#build">Build</a></h3>
<ul>
<li>Golang version &gt;= 1.19.</li>
<li>run <code>make build</code> in root path of <a href="https://github.com/apache/incubator-horaedb-meta">HoraeMeta</a>.</li>
</ul>
<h3 id="deploy"><a class="header" href="#deploy">Deploy</a></h3>
<h4 id="config"><a class="header" href="#config">Config</a></h4>
<p>At present, HoraeMeta supports specifying service startup configuration in two ways: configuration file and environment variable. We provide an example of configuration file startup. For details, please refer to <a href="https://github.com/apache/incubator-horaedb-meta/tree/main/config">config</a>. The configuration priority of environment variables is higher than that of configuration files. When they exist at the same time, the environment variables shall prevail.</p>
<h4 id="dynamic-or-static"><a class="header" href="#dynamic-or-static">Dynamic or Static</a></h4>
<p>Even with the HoraeMeta, the HoraeDB cluster can be deployed with a static or a dynamic topology. With a static topology, the table distribution is static after the cluster is initialized while with the dynamic topology, the tables can migrate between different HoraeDB nodes to achieve load balance or failover. However, the dynamic topology can be enabled only if the storage used by the HoraeDB node is remote, otherwise the data may be corrupted when tables are transferred to a different HoraeDB node when the data of HoraeDB is persisted locally.</p>
<p>Currently, the dynamic scheduling over the cluster topology is disabled by default in HoraeMeta, and in this guide, we won't enable it because local storage is adopted here. If you want to enable the dynamic scheduling, the <code>TOPOLOGY_TYPE</code> can be set as <code>dynamic</code> (<code>static</code> by default), and after that, load balancing and failover will work. However, don't enable it if what the underlying storage is local disk.</p>
<p>With the static topology, the params <code>DEFAULT_CLUSTER_NODE_COUNT</code>, which denotes the number of the HoraeDB nodes in the deployed cluster and should be set to the real number of machines for HoraeDB server, matters a lot because after cluster initialization the HoraeDB nodes can't be changed any more.</p>
<h4 id="start-horaemeta-instances"><a class="header" href="#start-horaemeta-instances">Start HoraeMeta Instances</a></h4>
<p>HoraeMeta is based on etcd to achieve high availability. In product environment, we usually deploy multiple nodes, but in local environment and testing, we can directly deploy a single node to simplify the entire deployment process.</p>
<ul>
<li>Standalone</li>
</ul>
<pre><code class="language-bash">docker run -d --name horaemeta-server \
  -p 2379:2379 \
  ghcr.io/apache/horaemeta-server:nightly-20231225-ab067bf0
</code></pre>
<ul>
<li>Cluster</li>
</ul>
<pre><code class="language-bash">wget https://raw.githubusercontent.com/apache/incubator-horaedb-docs/main/docs/src/resources/config-horaemeta-cluster0.toml

docker run -d --network=host --name horaemeta-server0 \
  -v $(pwd)/config-horaemeta-cluster0.toml:/etc/horaemeta/horaemeta.toml \
  ghcr.io/apache/horaemeta-server:nightly-20231225-ab067bf0

wget https://raw.githubusercontent.com/apache/incubator-horaedb-docs/main/docs/src/resources/config-horaemeta-cluster1.toml

docker run -d --network=host --name horaemeta-server1 \
  -v $(pwd)/config-horaemeta-cluster1.toml:/etc/horaemeta/horaemeta.toml \
  ghcr.io/apache/horaemeta-server:nightly-20231225-ab067bf0

wget https://raw.githubusercontent.com/apache/incubator-horaedb-docs/main/docs/src/resources/config-horaemeta-cluster2.toml

docker run -d --network=host --name horaemeta-server2 \
  -v $(pwd)/config-horaemeta-cluster2.toml:/etc/horaemeta/horaemeta.toml \
  ghcr.io/apache/horaemeta-server:nightly-20231225-ab067bf0
</code></pre>
<p>And if the storage used by the HoraeDB is remote and you want to enable the dynamic schedule features of the HoraeDB cluster, the <code>-e TOPOLOGY_TYPE=dynamic</code> can be added to the docker run command.</p>
<h2 id="deploy-horaedb"><a class="header" href="#deploy-horaedb">Deploy HoraeDB</a></h2>
<p>In the <code>NoMeta</code> mode, HoraeDB only requires the local disk as the underlying storage because the topology of the HoraeDB cluster is static. However, with HoraeMeta, the cluster topology can be dynamic, that is to say, HoraeDB can be configured to use a non-local storage service for the features of a distributed system: HA, load balancing, scalability and so on. And HoraeDB can be still configured to use a local storage with HoraeMeta, which certainly leads to a static cluster topology.</p>
<p>The relevant storage configurations include two parts:</p>
<ul>
<li>Object Storage</li>
<li>WAL Storage</li>
</ul>
<p>Note: If you are deploying HoraeDB over multiple nodes in a production environment, please set the environment variable for the server address as follows:</p>
<pre><code class="language-shell">export HORAEDB_SERVER_ADDR=&quot;{server_address}:8831&quot;
</code></pre>
<p>This address is used for communication between HoraeMeta and HoraeDB, please ensure it is valid.</p>
<h3 id="object-storage"><a class="header" href="#object-storage">Object Storage</a></h3>
<h4 id="local-storage"><a class="header" href="#local-storage">Local Storage</a></h4>
<p>Similarly, we can configure HoraeDB to use a local disk as the underlying storage:</p>
<pre><code class="language-toml">[analytic.storage.object_store]
type = &quot;Local&quot;
data_dir = &quot;/home/admin/data/horaedb&quot;
</code></pre>
<h4 id="oss"><a class="header" href="#oss">OSS</a></h4>
<p>Aliyun OSS can be also used as the underlying storage for HoraeDB, with which the data is replicated for disaster recovery. Here is a example config, and you have to replace the templates with the real OSS parameters:</p>
<pre><code class="language-toml">[analytic.storage.object_store]
type = &quot;Aliyun&quot;
key_id = &quot;{key_id}&quot;
key_secret = &quot;{key_secret}&quot;
endpoint = &quot;{endpoint}&quot;
bucket = &quot;{bucket}&quot;
prefix = &quot;{data_dir}&quot;
</code></pre>
<h4 id="s3"><a class="header" href="#s3">S3</a></h4>
<p>Amazon S3 can be also used as the underlying storage for HoraeDB. Here is a example config, and you have to replace the templates with the real S3 parameters:</p>
<pre><code class="language-toml">[analytic.storage.object_store]
type = &quot;S3&quot;
region = &quot;{region}&quot;
key_id = &quot;{key_id}&quot;
key_secret = &quot;{key_secret}&quot;
endpoint = &quot;{endpoint}&quot;
bucket = &quot;{bucket}&quot;
prefix = &quot;{prefix}&quot;
</code></pre>
<h3 id="wal-storage"><a class="header" href="#wal-storage">WAL Storage</a></h3>
<h4 id="rocksdb"><a class="header" href="#rocksdb">RocksDB</a></h4>
<p>The WAL based on RocksDB is also a kind of local storage for HoraeDB, which is easy for a quick start:</p>
<pre><code class="language-toml">[analytic.wal]
type = &quot;RocksDB&quot;
data_dir = &quot;/home/admin/data/horaedb&quot;
</code></pre>
<h4 id="oceanbase"><a class="header" href="#oceanbase">OceanBase</a></h4>
<p>If you have deployed a OceanBase cluster, HoraeDB can use it as the WAL storage for data disaster recovery. Here is a example config for such WAL, and you have to replace the templates with real OceanBase parameters:</p>
<pre><code class="language-toml">[analytic.wal]
type = &quot;Obkv&quot;

[analytic.wal.data_namespace]
ttl = &quot;365d&quot;

[analytic.wal.obkv]
full_user_name = &quot;{full_user_name}&quot;
param_url = &quot;{param_url}&quot;
password = &quot;{password}&quot;

[analytic.wal.obkv.client]
sys_user_name = &quot;{sys_user_name}&quot;
sys_password = &quot;{sys_password}&quot;
</code></pre>
<h4 id="kafka"><a class="header" href="#kafka">Kafka</a></h4>
<p>If you have deployed a Kafka cluster, HoraeDB can also use it as the WAL storage. Here is example config for it, and you have to replace the templates with real parameters of the Kafka cluster:</p>
<pre><code class="language-toml">[analytic.wal]
type = &quot;Kafka&quot;

[analytic.wal.kafka.client]
boost_broker = &quot;{boost_broker}&quot;
</code></pre>
<h3 id="meta-client-config"><a class="header" href="#meta-client-config">Meta Client Config</a></h3>
<p>Besides the storage configurations, HoraeDB must be configured to start in <code>WithMeta</code> mode and connect to the deployed HoraeMeta:</p>
<pre><code class="language-toml">[cluster_deployment]
mode = &quot;WithMeta&quot;

[cluster_deployment.meta_client]
cluster_name = 'defaultCluster'
meta_addr = 'http://{HoraeMetaAddr}:2379'
lease = &quot;10s&quot;
timeout = &quot;5s&quot;

[cluster_deployment.etcd_client]
server_addrs = ['http://{HoraeMetaAddr}:2379']
</code></pre>
<h3 id="complete-config-of-horaedb"><a class="header" href="#complete-config-of-horaedb">Complete Config of HoraeDB</a></h3>
<p>With all the parts of the configurations mentioned above, a runnable complete config for HoraeDB can be made. In order to make the HoraeDB cluster runnable, we can decide to adopt RocksDB-based WAL and local-disk-based Object Storage:</p>
<pre><code class="language-toml">[server]
bind_addr = &quot;0.0.0.0&quot;
http_port = 5440
grpc_port = 8831

[logger]
level = &quot;info&quot;

[runtime]
read_thread_num = 20
write_thread_num = 16
background_thread_num = 12

[cluster_deployment]
mode = &quot;WithMeta&quot;

[cluster_deployment.meta_client]
cluster_name = 'defaultCluster'
meta_addr = 'http://127.0.0.1:2379'
lease = &quot;10s&quot;
timeout = &quot;5s&quot;

[cluster_deployment.etcd_client]
server_addrs = ['127.0.0.1:2379']

[analytic]
write_group_worker_num = 16
replay_batch_size = 100
max_replay_tables_per_batch = 128
write_group_command_channel_cap = 1024
sst_background_read_parallelism = 8

[analytic.manifest]
scan_batch_size = 100
snapshot_every_n_updates = 10000
scan_timeout = &quot;5s&quot;
store_timeout = &quot;5s&quot;

[analytic.wal]
type = &quot;RocksDB&quot;
data_dir = &quot;/home/admin/data/horaedb&quot;

[analytic.storage]
mem_cache_capacity = &quot;20GB&quot;
# 1&lt;&lt;8=256
mem_cache_partition_bits = 8

[analytic.storage.object_store]
type = &quot;Local&quot;
data_dir = &quot;/home/admin/data/horaedb/&quot;

[analytic.table_opts]
arena_block_size = 2097152
write_buffer_size = 33554432

[analytic.compaction]
schedule_channel_len = 16
schedule_interval = &quot;30m&quot;
max_ongoing_tasks = 8
memory_limit = &quot;4G&quot;
</code></pre>
<p>Let's name this config file as <code>config.toml</code>. And the example configs, in which the templates must be replaced with real parameters before use, for remote storages are also provided:</p>
<ul>
<li><a href="cluster_deployment/../../resources/config_local_oss.toml">RocksDB WAL + OSS</a></li>
<li><a href="cluster_deployment/../../resources/config_obkv_oss.toml">OceanBase WAL + OSS</a></li>
<li><a href="cluster_deployment/../../resources/config_kafka_oss.toml">Kafka WAL + OSS</a></li>
</ul>
<h2 id="run-horaedb-cluster-with-horaemeta"><a class="header" href="#run-horaedb-cluster-with-horaemeta">Run HoraeDB cluster with HoraeMeta</a></h2>
<p>Firstly, let's start the HoraeMeta:</p>
<pre><code class="language-bash">docker run -d --name horaemeta-server \
  -p 2379:2379 \
  ghcr.io/apache/horaemeta-server:nightly-20231225-ab067bf0
</code></pre>
<p>With the started HoraeMeta cluster, let's start the HoraeDB instance:
TODO: complete it later</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="sdk"><a class="header" href="#sdk">SDK</a></h1>
<ul>
<li><a href="sdk/./rust.html">Rust</a></li>
<li><a href="sdk/./java.html">Java</a></li>
<li><a href="sdk/./python.html">Python</a></li>
<li><a href="sdk/./go.html">Go</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="java"><a class="header" href="#java">Java</a></h1>
<h2 id="introduction"><a class="header" href="#introduction">Introduction</a></h2>
<p>HoraeDB Client is a high-performance Java client for HoraeDB.</p>
<h2 id="requirements"><a class="header" href="#requirements">Requirements</a></h2>
<ul>
<li>Java 8 or later is required for compilation</li>
</ul>
<h2 id="dependency"><a class="header" href="#dependency">Dependency</a></h2>
<pre><code class="language-xml">&lt;dependency&gt;
  &lt;groupId&gt;io.ceresdb&lt;/groupId&gt;
  &lt;artifactId&gt;ceresdb-all&lt;/artifactId&gt;
  &lt;version&gt;${CERESDB.VERSION}&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>
<p>You can get latest version <a href="https://github.com/apache/incubator-horaedb-client-java/blob/main/docs/CHANGELOG.md">here</a>.</p>
<h2 id="init-horaedb-client"><a class="header" href="#init-horaedb-client">Init HoraeDB Client</a></h2>
<pre><code class="language-java">final CeresDBOptions opts = CeresDBOptions.newBuilder(&quot;127.0.0.1&quot;, 8831, DIRECT) // CeresDB default grpc port 8831，use DIRECT RouteMode
        .database(&quot;public&quot;) // use database for client, can be overridden by the RequestContext in request
        // maximum retry times when write fails
        // (only some error codes will be retried, such as the routing table failure)
        .writeMaxRetries(1)
        // maximum retry times when read fails
        // (only some error codes will be retried, such as the routing table failure)
        .readMaxRetries(1).build();

final CeresDBClient client = new CeresDBClient();
if (!client.init(opts)) {
    throw new IllegalStateException(&quot;Fail to start CeresDBClient&quot;);
}
</code></pre>
<p>The initialization requires at least three parameters:</p>
<ul>
<li><code>Endpoint</code>: 127.0.0.1</li>
<li><code>Port</code>: 8831</li>
<li><code>RouteMode</code>: DIRECT/PROXY</li>
</ul>
<p>Here is the explanation of <code>RouteMode</code>. There are two kinds of <code>RouteMode</code>,The <code>Direct</code> mode should be adopted to avoid forwarding overhead if all the servers are accessible to the client.
However, the <code>Proxy</code> mode is the only choice if the access to the servers from the client must go through a gateway.
For more configuration options, see <a href="https://github.com/apache/incubator-horaedb-client-java/tree/main/docs/configuration.md">configuration</a></p>
<p>Notice: HoraeDB currently only supports the default database <code>public</code> now, multiple databases will be supported in the future;</p>
<h2 id="create-table-example"><a class="header" href="#create-table-example">Create Table Example</a></h2>
<p>For ease of use, when using gRPC's write interface for writing, if a table does not exist, HoraeDB will automatically create a table based on the first write.</p>
<p>Of course, you can also use <code>create table</code> statement to manage the table more finely (such as adding indexes).</p>
<p>The following table creation statement（using the SQL API included in SDK ）shows all field types supported by HoraeDB：</p>
<pre><code class="language-java">// Create table manually, creating table schema ahead of data ingestion is not required
String createTableSql = &quot;CREATE TABLE IF NOT EXISTS machine_table(&quot; +
        &quot;ts TIMESTAMP NOT NULL,&quot; +
        &quot;city STRING TAG NOT NULL,&quot; +
        &quot;ip STRING TAG NOT NULL,&quot; +
        &quot;cpu DOUBLE NULL,&quot; +
        &quot;mem DOUBLE NULL,&quot; +
        &quot;TIMESTAMP KEY(ts)&quot; + // timestamp column must be specified
        &quot;) ENGINE=Analytic&quot;;

Result&lt;SqlQueryOk, Err&gt; createResult = client.sqlQuery(new SqlQueryRequest(createTableSql)).get();
if (!createResult.isOk()) {
        throw new IllegalStateException(&quot;Fail to create table&quot;);
}
</code></pre>
<h2 id="drop-table-example"><a class="header" href="#drop-table-example">Drop Table Example</a></h2>
<p>Here is an example of dropping table：</p>
<pre><code class="language-java">String dropTableSql = &quot;DROP TABLE machine_table&quot;;

Result&lt;SqlQueryOk, Err&gt; dropResult = client.sqlQuery(new SqlQueryRequest(dropTableSql)).get();
if (!dropResult.isOk()) {
        throw new IllegalStateException(&quot;Fail to drop table&quot;);
}
</code></pre>
<h2 id="write-data-example"><a class="header" href="#write-data-example">Write Data Example</a></h2>
<p>Firstly, you can use <code>PointBuilder</code> to build HoraeDB points:</p>
<pre><code class="language-java">List&lt;Point&gt; pointList = new LinkedList&lt;&gt;();
for (int i = 0; i &lt; 100; i++) {
    // build one point
    final Point point = Point.newPointBuilder(&quot;machine_table&quot;)
            .setTimestamp(t0)
            .addTag(&quot;city&quot;, &quot;Singapore&quot;)
            .addTag(&quot;ip&quot;, &quot;10.0.0.1&quot;)
            .addField(&quot;cpu&quot;, Value.withDouble(0.23))
            .addField(&quot;mem&quot;, Value.withDouble(0.55))
            .build();
    points.add(point);
}
</code></pre>
<p>Then, you can use <code>write</code> interface to write data:</p>
<pre><code class="language-java">final CompletableFuture&lt;Result&lt;WriteOk, Err&gt;&gt; wf = client.write(new WriteRequest(pointList));
// here the `future.get` is just for demonstration, a better async programming practice would be using the CompletableFuture API
final Result&lt;WriteOk, Err&gt; writeResult = wf.get();
        Assert.assertTrue(writeResult.isOk());
        // `Result` class referenced the Rust language practice, provides rich functions (such as mapXXX, andThen) transforming the result value to improve programming efficiency. You can refer to the API docs for detail usage.
        Assert.assertEquals(3, writeResult.getOk().getSuccess());
        Assert.assertEquals(3, writeResult.mapOr(0, WriteOk::getSuccess).intValue());
        Assert.assertEquals(0, writeResult.mapOr(-1, WriteOk::getFailed).intValue());
</code></pre>
<p>See <a href="https://github.com/apache/incubator-horaedb-client-java/tree/main/docs/write.md">write</a></p>
<h2 id="query-data-example"><a class="header" href="#query-data-example">Query Data Example</a></h2>
<pre><code class="language-java">final SqlQueryRequest queryRequest = SqlQueryRequest.newBuilder()
        .forTables(&quot;machine_table&quot;) // table name is optional. If not provided, SQL parser will parse the `ssql` to get the table name and do the routing automaticly
        .sql(&quot;select * from machine_table where ts = %d&quot;, t0) //
        .build();
final CompletableFuture&lt;Result&lt;SqlQueryOk, Err&gt;&gt; qf = client.sqlQuery(queryRequest);
// here the `future.get` is just for demonstration, a better async programming practice would be using the CompletableFuture API
final Result&lt;SqlQueryOk, Err&gt; queryResult = qf.get();

Assert.assertTrue(queryResult.isOk());

final SqlQueryOk queryOk = queryResult.getOk();
Assert.assertEquals(1, queryOk.getRowCount());

// get rows as list
final List&lt;Row&gt; rows = queryOk.getRowList();
Assert.assertEquals(t0, rows.get(0).getColumn(&quot;ts&quot;).getValue().getTimestamp());
Assert.assertEquals(&quot;Singapore&quot;, rows.get(0).getColumn(&quot;city&quot;).getValue().getString());
Assert.assertEquals(&quot;10.0.0.1&quot;, rows.get(0).getColumn(&quot;ip&quot;).getValue().getString());
Assert.assertEquals(0.23, rows.get(0).getColumn(&quot;cpu&quot;).getValue().getDouble(), 0.0000001);
Assert.assertEquals(0.55, rows.get(0).getColumn(&quot;mem&quot;).getValue().getDouble(), 0.0000001);

// get rows as stream
final Stream&lt;Row&gt; rowStream = queryOk.stream();
rowStream.forEach(row -&gt; System.out.println(row.toString()));
</code></pre>
<p>See <a href="https://github.com/apache/incubator-horaedb-client-java/tree/main/docs/read.md">read</a></p>
<h2 id="stream-writeread-example"><a class="header" href="#stream-writeread-example">Stream Write/Read Example</a></h2>
<p>HoraeDB support streaming writing and reading，suitable for large-scale data reading and writing。</p>
<pre><code class="language-java">long start = System.currentTimeMillis();
long t = start;
final StreamWriteBuf&lt;Point, WriteOk&gt; writeBuf = client.streamWrite(&quot;machine_table&quot;);
for (int i = 0; i &lt; 1000; i++) {
        final Point streamData = Point.newPointBuilder(&quot;machine_table&quot;)
                .setTimestamp(t)
                .addTag(&quot;city&quot;, &quot;Beijing&quot;)
                .addTag(&quot;ip&quot;, &quot;10.0.0.3&quot;)
                .addField(&quot;cpu&quot;, Value.withDouble(0.42))
                .addField(&quot;mem&quot;, Value.withDouble(0.67))
                .build();
        writeBuf.writeAndFlush(Collections.singletonList(streamData));
        t = t+1;
}
final CompletableFuture&lt;WriteOk&gt; writeOk = writeBuf.completed();
Assert.assertEquals(1000, writeOk.join().getSuccess());

final SqlQueryRequest streamQuerySql = SqlQueryRequest.newBuilder()
        .sql(&quot;select * from %s where city = '%s' and ts &gt;= %d and ts &lt; %d&quot;, &quot;machine_table&quot;, &quot;Beijing&quot;, start, t).build();
final Result&lt;SqlQueryOk, Err&gt; streamQueryResult = client.sqlQuery(streamQuerySql).get();
Assert.assertTrue(streamQueryResult.isOk());
Assert.assertEquals(1000, streamQueryResult.getOk().getRowCount());
</code></pre>
<p>See <a href="https://github.com/apache/incubator-horaedb-client-java/tree/main/docs/streaming.md">streaming</a></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="go"><a class="header" href="#go">Go</a></h1>
<h2 id="installation"><a class="header" href="#installation">Installation</a></h2>
<pre><code>go get github.com/apache/incubator-horaedb-client-go
</code></pre>
<p>You can get latest version <a href="https://github.com/apache/incubator-horaedb-client-go/tags">here</a>.</p>
<h2 id="how-to-use"><a class="header" href="#how-to-use">How To Use</a></h2>
<h3 id="init-horaedb-client-1"><a class="header" href="#init-horaedb-client-1">Init HoraeDB Client</a></h3>
<pre><code class="language-go">	client, err := horaedb.NewClient(endpoint, horaedb.Direct,
		horaedb.WithDefaultDatabase(&quot;public&quot;),
	)
</code></pre>
<div class="table-wrapper"><table><thead><tr><th>option name</th><th>description</th></tr></thead><tbody>
<tr><td>defaultDatabase</td><td>using database, database can be overwritten by ReqContext in single <code>Write</code> or <code>SQLRequest</code></td></tr>
<tr><td>RPCMaxRecvMsgSize</td><td>configration for grpc <code>MaxCallRecvMsgSize</code>, default 1024 _ 1024 _ 1024</td></tr>
<tr><td>RouteMaxCacheSize</td><td>If the maximum number of router cache size, router client whill evict oldest if exceeded, default is 10000</td></tr>
</tbody></table>
</div>
<p>Notice:</p>
<ul>
<li>HoraeDB currently only supports the default database <code>public</code> now, multiple databases will be supported in the future</li>
</ul>
<h3 id="manage-table"><a class="header" href="#manage-table">Manage Table</a></h3>
<p>HoraeDB uses SQL to manage tables, such as creating tables, deleting tables, or adding columns, etc., which is not much different from when you use SQL to manage other databases.</p>
<p>For ease of use, when using gRPC's write interface for writing, if a table does not exist, HoraeDB will automatically create a table based on the first write.</p>
<p>Of course, you can also use <code>create table</code> statement to manage the table more finely (such as adding indexes).</p>
<p><strong>Example for creating table</strong></p>
<pre><code class="language-go">	createTableSQL := `
		CREATE TABLE IF NOT EXISTS demo (
			name string TAG,
			value double,
			t timestamp NOT NULL,
			TIMESTAMP KEY(t)
		) ENGINE=Analytic with (enable_ttl=false)`

	req := horaedb.SQLQueryRequest{
		Tables: []string{&quot;demo&quot;},
		SQL:    createTableSQL,
	}
	resp, err := client.SQLQuery(context.Background(), req)
</code></pre>
<p><strong>Example for droping table</strong></p>
<pre><code class="language-go">	dropTableSQL := `DROP TABLE demo`
	req := horaedb.SQLQueryRequest{
		Tables: []string{&quot;demo&quot;},
		SQL:    dropTableSQL,
	}
	resp, err := client.SQLQuery(context.Background(), req)
</code></pre>
<h3 id="how-to-build-write-data"><a class="header" href="#how-to-build-write-data">How To Build Write Data</a></h3>
<pre><code class="language-go">	points := make([]horaedb.Point, 0, 2)
	for i := 0; i &lt; 2; i++ {
		point, err := horaedb.NewPointBuilder(&quot;demo&quot;).
			SetTimestamp(now).
			AddTag(&quot;name&quot;, horaedb.NewStringValue(&quot;test_tag1&quot;)).
			AddField(&quot;value&quot;, horaedb.NewDoubleValue(0.4242)).
			Build()
		if err != nil {
			panic(err)
		}
		points = append(points, point)
	}
</code></pre>
<h3 id="write-example"><a class="header" href="#write-example">Write Example</a></h3>
<pre><code class="language-go">	req := horaedb.WriteRequest{
		Points: points,
	}
	resp, err := client.Write(context.Background(), req)
</code></pre>
<h3 id="query-example"><a class="header" href="#query-example">Query Example</a></h3>
<pre><code class="language-go">	querySQL := `SELECT * FROM demo`
	req := horaedb.SQLQueryRequest{
		Tables: []string{&quot;demo&quot;},
		SQL:    querySQL,
	}
	resp, err := client.SQLQuery(context.Background(), req)
	if err != nil {
        panic(err)
	}
	fmt.Printf(&quot;query table success, rows:%+v\n&quot;, resp.Rows)
</code></pre>
<h2 id="example-1"><a class="header" href="#example-1">Example</a></h2>
<p>You can find the complete example <a href="https://github.com/apache/incubator-horaedb-client-go/blob/main/examples/read_write.go">here</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="python"><a class="header" href="#python">Python</a></h1>
<h2 id="introduction-1"><a class="header" href="#introduction-1">Introduction</a></h2>
<p><a href="https://pypi.org/project/ceresdb-client/">horaedb-client</a> is the python client for <a href="https://github.com/apache/incubator-horaedb">HoraeDB</a>.</p>
<p>Thanks to <a href="https://github.com/PyO3">PyO3</a>, the python client is actually a wrapper on the <a href="https://github.com/apache/incubator-horaedb-client-rs">rust client</a>.</p>
<p>The guide will give a basic introduction to the python client by <a href="https://github.com/apache/incubator-horaedb-client-py/blob/main/examples/read_write.py">example</a>.</p>
<h2 id="requirements-1"><a class="header" href="#requirements-1">Requirements</a></h2>
<ul>
<li>Python &gt;= 3.7</li>
</ul>
<h2 id="installation-1"><a class="header" href="#installation-1">Installation</a></h2>
<pre><code class="language-bash">pip install ceresdb-client
</code></pre>
<p>You can get latest version <a href="https://github.com/apache/incubator-horaedb-client-py/tags">here</a>.</p>
<h2 id="init-horaedb-client-2"><a class="header" href="#init-horaedb-client-2">Init HoraeDB Client</a></h2>
<p>The client initialization comes first, here is a code snippet:</p>
<pre><code class="language-python">import asyncio
import datetime
from ceresdb_client import Builder, RpcContext, PointBuilder, ValueBuilder, WriteRequest, SqlQueryRequest, Mode, RpcConfig

rpc_config = RpcConfig()
rpc_config = RpcConfig()
rpc_config.thread_num = 1
rpc_config.default_write_timeout_ms = 1000

builder = Builder('127.0.0.1:8831', Mode.Direct)
builder.set_rpc_config(rpc_config)
builder.set_default_database('public')
client = builder.build()
</code></pre>
<p>Firstly, it's worth noting that the imported packages are used across all the code snippets in this guide, and they will not be repeated in the following.</p>
<p>The initialization requires at least two parameters:</p>
<ul>
<li><code>Endpoint</code>: the server endpoint consisting of ip address and serving port, e.g. <code>127.0.0.1:8831</code>;</li>
<li><code>Mode</code>: The mode of the communication between client and server, and there are two kinds of <code>Mode</code>: <code>Direct</code> and <code>Proxy</code>.</li>
</ul>
<p><code>Endpoint</code> is simple, while <code>Mode</code> deserves more explanation. The <code>Direct</code> mode should be adopted to avoid forwarding overhead if all the servers are accessible to the client. However, the <code>Proxy</code> mode is the only choice if the access to the servers from the client must go through a gateway.</p>
<p>The <code>default_database</code> can be set and will be used if following rpc calling without setting the database in the <code>RpcContext</code>.</p>
<p>By configuring the <code>RpcConfig</code>, resource and performance of the client can be manipulated, and all of the configurations can be referred at <a href="https://github.com/apache/incubator-horaedb-client-py/blob/main/ceresdb_client.pyi">here</a>.</p>
<h2 id="create-table-2"><a class="header" href="#create-table-2">Create Table</a></h2>
<p>For ease of use, when using gRPC's write interface for writing, if a table does not exist, HoraeDB will automatically create a table based on the first write.</p>
<p>Of course, you can also use <code>create table</code> statement to manage the table more finely (such as adding indexes).</p>
<p>Here is a example for creating table by the initialized client:</p>
<pre><code class="language-python">async def async_query(client, ctx, req):
    await client.sql_query(ctx, req)

create_table_sql = 'CREATE TABLE IF NOT EXISTS demo ( \
    name string TAG, \
    value double, \
    t timestamp NOT NULL, \
    TIMESTAMP KEY(t)) ENGINE=Analytic with (enable_ttl=false)'

req = SqlQueryRequest(['demo'], create_table_sql)
rpc_ctx = RpcContext()
rpc_ctx.database = 'public'
rpc_ctx.timeout_ms = 100

event_loop = asyncio.get_event_loop()
event_loop.run_until_complete(async_query(client, rpc_ctx, req))
</code></pre>
<p><code>RpcContext</code> can be used to overwrite the default database and timeout defined in the initialization of the client.</p>
<h2 id="write-data-1"><a class="header" href="#write-data-1">Write Data</a></h2>
<p><code>PointBuilder</code> can be used to construct a point, which is actually a row in data set. The write request consists of multiple points.</p>
<p>The example is simple:</p>
<pre><code class="language-python">async def async_write(client, ctx, req):
    return await client.write(ctx, req)

point_builder = PointBuilder('demo')
point_builder.set_timestamp(1000 * int(round(datetime.datetime.now().timestamp())))
point_builder.set_tag(&quot;name&quot;, ValueBuilder().string(&quot;test_tag1&quot;))
point_builder.set_field(&quot;value&quot;, ValueBuilder().double(0.4242))
point = point_builder.build()

write_request = WriteRequest()
write_request.add_point(point)

event_loop = asyncio.get_event_loop()
event_loop.run_until_complete(async_write(client, ctx, req))
</code></pre>
<h2 id="query-data"><a class="header" href="#query-data">Query Data</a></h2>
<p>By <code>sql_query</code> interface, it is easy to retrieve the data from the server:</p>
<pre><code>req = SqlQueryRequest(['demo'], 'select * from demo')
event_loop = asyncio.get_event_loop()
resp = event_loop.run_until_complete(async_query(client, ctx, req))
</code></pre>
<p>As the example shows, two parameters are needed to construct the <code>SqlQueryRequest</code>:</p>
<ul>
<li>The tables involved by this sql query;</li>
<li>The query sql.</li>
</ul>
<p>Currently, the first parameter is necessary for performance on routing.</p>
<p>With retrieved data, we can process it row by row and column by column:</p>
<pre><code class="language-python"># Access row by index in the resp.
for row_idx in range(0, resp.num_rows()):
    row_tokens = []
    row = resp.row_by_idx(row_idx)
    for col_idx in range(0, row.num_cols()):
        col = row.column_by_idx(col_idx)
        row_tokens.append(f&quot;{col.name()}:{col.value()}#{col.data_type()}&quot;)
    print(f&quot;row#{row_idx}: {','.join(row_tokens)}&quot;)

# Access row by iter in the resp.
for row in resp.iter_rows():
    row_tokens = []
    for col in row.iter_columns():
        row_tokens.append(f&quot;{col.name()}:{col.value()}#{col.data_type()}&quot;)
    print(f&quot;row: {','.join(row_tokens)}&quot;)
</code></pre>
<h2 id="drop-table-2"><a class="header" href="#drop-table-2">Drop Table</a></h2>
<p>Finally, we can drop the table by the sql api, which is similar to the table creation:</p>
<pre><code class="language-python">drop_table_sql = 'DROP TABLE demo'

req = SqlQueryRequest(['demo'], drop_table_sql)

event_loop = asyncio.get_event_loop()
event_loop.run_until_complete(async_query(client, rpc_ctx, req))
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="rust"><a class="header" href="#rust">Rust</a></h1>
<h2 id="install"><a class="header" href="#install">Install</a></h2>
<pre><code class="language-bash">cargo add ceresdb-client
</code></pre>
<p>You can get latest version <a href="https://github.com/apache/incubator-horaedb-client-rs/tags">here</a>.</p>
<h2 id="init-client"><a class="header" href="#init-client">Init Client</a></h2>
<p>At first, we need to init the client.</p>
<ul>
<li>New builder for the client, and you must set <code>endpoint</code> and <code>mode</code>:
<ul>
<li><code>endpoint</code> is a string which is usually like &quot;ip/domain_name:port&quot;.</li>
<li><code>mode</code> is used to define the way to access horaedb server, <a href="https://github.com/apache/incubator-horaedb-client-rs/blob/main/src/db_client/builder.rs#L20">detail about mode</a>.</li>
</ul>
</li>
</ul>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let mut builder = Builder::new(&quot;ip/domain_name:port&quot;, Mode::Direct/Mode::Proxy);
<span class="boring">}</span></code></pre></pre>
<ul>
<li>New and set <code>rpc_config</code>, it can be defined on demand or just use the default value, <a href="https://github.com/apache/incubator-horaedb-client-rs/blob/main/src/options.rs">detail about rpc config</a>:</li>
</ul>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let rpc_config = RpcConfig {
    thread_num: Some(1),
    default_write_timeout: Duration::from_millis(1000),
    ..Default::default()
};
let builder = builder.rpc_config(rpc_config);
<span class="boring">}</span></code></pre></pre>
<ul>
<li>Set <code>default_database</code>, it will be used if following rpc calling without setting the database in the <code>RpcContext</code>(will be introduced in later):</li>
</ul>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>    let builder = builder.default_database(&quot;public&quot;);
<span class="boring">}</span></code></pre></pre>
<ul>
<li>Finally, we build client from builder:</li>
</ul>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>    let client = builder.build();
<span class="boring">}</span></code></pre></pre>
<h2 id="manage-table-1"><a class="header" href="#manage-table-1">Manage Table</a></h2>
<p>For ease of use, when using gRPC's write interface for writing, if a table does not exist, HoraeDB will automatically create a table based on the first write.</p>
<p>Of course, you can also use <code>create table</code> statement to manage the table more finely (such as adding indexes).</p>
<p>You can use the sql query interface to create or drop table, related setting will be introduced in <code>sql query</code> section.</p>
<ul>
<li>Create table:</li>
</ul>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let create_table_sql = r#&quot;CREATE TABLE IF NOT EXISTS horaedb (
            str_tag string TAG,
            int_tag int32 TAG,
            var_tag varbinary TAG,
            str_field string,
            int_field int32,
            bin_field varbinary,
            t timestamp NOT NULL,
            TIMESTAMP KEY(t)) ENGINE=Analytic with
            (enable_ttl='false')&quot;#;
let req = SqlQueryRequest {
    tables: vec![&quot;horaedb&quot;.to_string()],
    sql: create_table_sql.to_string(),
};

let resp = client
    .sql_query(rpc_ctx, &amp;req)
    .await
    .expect(&quot;Should succeed to create table&quot;);
<span class="boring">}</span></code></pre></pre>
<ul>
<li>Drop table:</li>
</ul>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let drop_table_sql = &quot;DROP TABLE horaedb&quot;;
let req = SqlQueryRequest {
    tables: vec![&quot;horaedb&quot;.to_string()],
    sql: drop_table_sql.to_string(),
};

let resp = client
    .sql_query(rpc_ctx, &amp;req)
    .await
    .expect(&quot;Should succeed to create table&quot;);
<span class="boring">}</span></code></pre></pre>
<h2 id="write"><a class="header" href="#write">Write</a></h2>
<p>We support to write with the time series data model like <a href="https://awesome.influxdata.com/docs/part-2/influxdb-data-model/">InfluxDB</a>.</p>
<ul>
<li>Build the <code>point</code> first by <code>PointBuilder</code>, the related data structure of <code>tag value</code> and <code>field value</code> in it is defined as <code>Value</code>, <a href="https://github.com/apache/incubator-horaedb-client-rs/blob/main/src/model/value.rs">detail about Value</a>:</li>
</ul>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let test_table = &quot;horaedb&quot;;
let ts = Local::now().timestamp_millis();
let point = PointBuilder::new(test_table.to_string())
        .timestamp(ts)
        .tag(&quot;str_tag&quot;.to_string(), Value::String(&quot;tag_val&quot;.to_string()))
        .tag(&quot;int_tag&quot;.to_string(), Value::Int32(42))
        .tag(
            &quot;var_tag&quot;.to_string(),
            Value::Varbinary(b&quot;tag_bin_val&quot;.to_vec()),
        )
        .field(
            &quot;str_field&quot;.to_string(),
            Value::String(&quot;field_val&quot;.to_string()),
        )
        .field(&quot;int_field&quot;.to_string(), Value::Int32(42))
        .field(
            &quot;bin_field&quot;.to_string(),
            Value::Varbinary(b&quot;field_bin_val&quot;.to_vec()),
        )
        .build()
        .unwrap();
<span class="boring">}</span></code></pre></pre>
<ul>
<li>Add the <code>point</code> to <code>write request</code>:</li>
</ul>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let mut write_req = WriteRequest::default();
write_req.add_point(point);
<span class="boring">}</span></code></pre></pre>
<ul>
<li>
<p>New <code>rpc_ctx</code>, and it can also be defined on demand or just use the default value, <a href="https://github.com/apache/incubator-horaedb-client-rs/blob/a72e673103463c7962e01a097592fc7edbcc0b79/src/rpc_client/mod.rs#L29">detail about rpc ctx</a>:</p>
</li>
<li>
<p>Finally, write to server by client.</p>
</li>
</ul>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let rpc_ctx = RpcContext {
    database: Some(&quot;public&quot;.to_string()),
    ..Default::default()
};
let resp = client.write(rpc_ctx, &amp;write_req).await.expect(&quot;Should success to write&quot;);
<span class="boring">}</span></code></pre></pre>
<h2 id="sql-query"><a class="header" href="#sql-query">Sql Query</a></h2>
<p>We support to query data with sql.</p>
<ul>
<li>Define related tables and sql in <code>sql query request</code>:</li>
</ul>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let req = SqlQueryRequest {
    tables: vec![table name 1,...,table name n],
    sql: sql string (e.g. select * from xxx),
};
<span class="boring">}</span></code></pre></pre>
<ul>
<li>Query by client:</li>
</ul>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let resp = client.sql_query(rpc_ctx, &amp;req).await.expect(&quot;Should success to write&quot;);
<span class="boring">}</span></code></pre></pre>
<h2 id="example-2"><a class="header" href="#example-2">Example</a></h2>
<p>You can find the <a href="https://github.com/apache/incubator-horaedb-client-rs/blob/main/examples/read_write.rs">complete example</a> in the project.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="operation-and-maintenance"><a class="header" href="#operation-and-maintenance">Operation and Maintenance</a></h1>
<p>This guide introduces the operation and maintenance of HoraeDB, including cluster installation, database&amp;table operations, fault tolerance, disaster recovery, data import and export, etc.</p>
<ul>
<li><a href="operation/./table.html">Table</a></li>
<li><a href="operation/./system_table.html">System Table</a></li>
<li><a href="operation/./block_list.html">Block List</a></li>
<li><a href="operation/./observability.html">Observability</a></li>
<li><a href="operation/./cluster.html">Cluster</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="table-operation"><a class="header" href="#table-operation">Table Operation</a></h1>
<p>HoraeDB supports standard SQL protocols and allows you to create tables and read/write data via http requests. More <a href="operation/../sql/README.html">SQL</a></p>
<h2 id="create-table-3"><a class="header" href="#create-table-3">Create Table</a></h2>
<h3 id="example-3"><a class="header" href="#example-3">Example</a></h3>
<pre><code class="language-shell">curl --location --request POST 'http://127.0.0.1:5000/sql' \
--header 'Content-Type: application/json' \
-d '{
    &quot;query&quot;: &quot;CREATE TABLE `demo` (`name` string TAG, `value` double NOT NULL, `t` timestamp NOT NULL, TIMESTAMP KEY(t)) ENGINE=Analytic with (enable_ttl='\''false'\'')&quot;
}'
</code></pre>
<h2 id="write-data-2"><a class="header" href="#write-data-2">Write Data</a></h2>
<h3 id="example-4"><a class="header" href="#example-4">Example</a></h3>
<pre><code class="language-shell">curl --location --request POST 'http://127.0.0.1:5000/sql' \
--header 'Content-Type: application/json' \
-d '{
    &quot;query&quot;: &quot;INSERT INTO demo(t, name, value) VALUES(1651737067000, '\''horaedb'\'', 100)&quot;
}'
</code></pre>
<h2 id="read-data-1"><a class="header" href="#read-data-1">Read Data</a></h2>
<h3 id="example-5"><a class="header" href="#example-5">Example</a></h3>
<pre><code class="language-shell">curl --location --request POST 'http://127.0.0.1:5000/sql' \
--header 'Content-Type: application/json' \
-d '{
    &quot;query&quot;: &quot;select * from demo&quot;
}'
</code></pre>
<h2 id="query-table-info"><a class="header" href="#query-table-info">Query Table Info</a></h2>
<h3 id="example-6"><a class="header" href="#example-6">Example</a></h3>
<pre><code class="language-shell">curl --location --request POST 'http://127.0.0.1:5000/sql' \
--header 'Content-Type: application/json' \
-d '{
    &quot;query&quot;: &quot;show create table demo&quot;
}'
</code></pre>
<h3 id="drop-table-3"><a class="header" href="#drop-table-3">Drop Table</a></h3>
<h3 id="example-7"><a class="header" href="#example-7">Example</a></h3>
<pre><code class="language-shell">curl --location --request POST 'http://127.0.0.1:5000/sql' \
--header 'Content-Type: application/json' \
-d '{
    &quot;query&quot;: &quot;DROP TABLE demo&quot;
}'
</code></pre>
<h2 id="route-table"><a class="header" href="#route-table">Route Table</a></h2>
<h3 id="example-8"><a class="header" href="#example-8">Example</a></h3>
<pre><code class="language-shell">curl --location --request GET 'http://127.0.0.1:5000/route/{table_name}'
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="table-operation-1"><a class="header" href="#table-operation-1">Table Operation</a></h1>
<h2 id="query-table-information"><a class="header" href="#query-table-information">Query Table Information</a></h2>
<p>Like Mysql's <code>information_schema.tables</code>, HoraeDB provides <code>system.public.tables</code> to save tables information.
Columns:</p>
<ul>
<li>timestamp([TimeStamp])</li>
<li>catalog([String])</li>
<li>schema([String])</li>
<li>table_name([String])</li>
<li>table_id([Uint64])</li>
<li>engine([String])</li>
</ul>
<h3 id="example-9"><a class="header" href="#example-9">Example</a></h3>
<p>Query table information via table_name like this:</p>
<pre><code class="language-shell">curl --location --request POST 'http://localhost:5000/sql' \
--header 'Content-Type: application/json' \
-d '{
    &quot;query&quot;: &quot;select * from system.public.tables where `table_name`=\&quot;my_table\&quot;&quot;
}'
</code></pre>
<h3 id="response"><a class="header" href="#response">Response</a></h3>
<pre><code class="language-json">{
    &quot;rows&quot;:[
        {
            &quot;timestamp&quot;:0,
            &quot;catalog&quot;:&quot;horaedb&quot;,
            &quot;schema&quot;:&quot;public&quot;,
            &quot;table_name&quot;:&quot;my_table&quot;,
            &quot;table_id&quot;:3298534886446,
            &quot;engine&quot;:&quot;Analytic&quot;
        }
}
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="block-list"><a class="header" href="#block-list">Block List</a></h1>
<h2 id="add-block-list"><a class="header" href="#add-block-list">Add block list</a></h2>
<p>If you want to reject query for a table, you can add table name to 'read_block_list'.</p>
<h3 id="example-10"><a class="header" href="#example-10">Example</a></h3>
<pre><code class="language-shell">curl --location --request POST 'http://localhost:5000/admin/block' \
--header 'Content-Type: application/json' \
-d '{
    &quot;operation&quot;:&quot;Add&quot;,
    &quot;write_block_list&quot;:[],
    &quot;read_block_list&quot;:[&quot;my_table&quot;]
}'
</code></pre>
<h3 id="response-1"><a class="header" href="#response-1">Response</a></h3>
<pre><code class="language-json">{
  &quot;write_block_list&quot;: [],
  &quot;read_block_list&quot;: [&quot;my_table&quot;]
}
</code></pre>
<h2 id="set-block-list"><a class="header" href="#set-block-list">Set block list</a></h2>
<p>You can use set operation to clear exist tables and set new tables to 'read_block_list' like following example.</p>
<h3 id="example-11"><a class="header" href="#example-11">Example</a></h3>
<pre><code class="language-shell">curl --location --request POST 'http://localhost:5000/admin/block' \
--header 'Content-Type: application/json' \
-d '{
    &quot;operation&quot;:&quot;Set&quot;,
    &quot;write_block_list&quot;:[],
    &quot;read_block_list&quot;:[&quot;my_table1&quot;,&quot;my_table2&quot;]
}'
</code></pre>
<h3 id="response-2"><a class="header" href="#response-2">Response</a></h3>
<pre><code class="language-json">{
  &quot;write_block_list&quot;: [],
  &quot;read_block_list&quot;: [&quot;my_table1&quot;, &quot;my_table2&quot;]
}
</code></pre>
<h2 id="remove-block-list"><a class="header" href="#remove-block-list">Remove block list</a></h2>
<p>You can remove tables from 'read_block_list' like following example.</p>
<h3 id="example-12"><a class="header" href="#example-12">Example</a></h3>
<pre><code class="language-shell">curl --location --request POST 'http://localhost:5000/admin/block' \
--header 'Content-Type: application/json' \
-d '{
    &quot;operation&quot;:&quot;Remove&quot;,
    &quot;write_block_list&quot;:[],
    &quot;read_block_list&quot;:[&quot;my_table1&quot;]
}'
</code></pre>
<h3 id="response-3"><a class="header" href="#response-3">Response</a></h3>
<pre><code class="language-json">{
  &quot;write_block_list&quot;: [],
  &quot;read_block_list&quot;: [&quot;my_table2&quot;]
}
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="observability"><a class="header" href="#observability">Observability</a></h1>
<p>HoraeDB is observable with Prometheus and Grafana.</p>
<h2 id="prometheus"><a class="header" href="#prometheus">Prometheus</a></h2>
<p><a href="https://github.com/prometheus/prometheus">Prometheus</a> is a systems and service monitoring system.</p>
<h3 id="configuration"><a class="header" href="#configuration">Configuration</a></h3>
<p>Save the following configuration into the <code>prometheus.yml</code> file. For example, in the <code>tmp</code> directory, <code>/tmp/prometheus.yml</code>.</p>
<p>Two HoraeDB http service are started on localhost:5440 and localhost:5441.</p>
<pre><code class="language-yaml">global:
  scrape_interval: 30s
scrape_configs:
  - job_name: horaedb-server
    static_configs:
      - targets: [your_ip:5440, your_ip:5441]
        labels:
          env: horaedbcluster
</code></pre>
<p>See details about configuration <a href="https://prometheus.io/docs/prometheus/latest/configuration/configuration/">here</a>.</p>
<h3 id="run"><a class="header" href="#run">Run</a></h3>
<p>You can use docker to start Prometheus. The docker image information is <a href="https://hub.docker.com/r/prom/prometheus">here</a>.</p>
<pre><code>docker run \
    -d --name=prometheus \
    -p 9090:9090 \
    -v /tmp/prometheus.yml:/etc/prometheus/prometheus.yml \
    prom/prometheus:v2.41.0
</code></pre>
<p>For more detailed installation methods, refer to <a href="https://prometheus.io/docs/prometheus/latest/installation/">here</a>.</p>
<h2 id="grafana"><a class="header" href="#grafana">Grafana</a></h2>
<p><a href="https://github.com/grafana/grafana">Grafana</a> is an open and composable observability and data visualization platform.</p>
<h3 id="run-1"><a class="header" href="#run-1">Run</a></h3>
<p>You can use docker to start grafana. The docker image information is <a href="https://hub.docker.com/r/grafana/grafana">here</a>.</p>
<pre><code>docker run -d --name=grafana -p 3000:3000 grafana/grafana:9.3.6
</code></pre>
<p>Default admin user credentials are admin/admin.</p>
<p>Grafana is available on http://127.0.0.1:3000.</p>
<p>For more detailed installation methods, refer to <a href="https://grafana.com/docs/grafana/latest/setup-grafana/installation/">here</a>.</p>
<h3 id="configure-data-source"><a class="header" href="#configure-data-source">Configure data source</a></h3>
<ol>
<li>Hover the cursor over the Configuration (gear) icon.</li>
<li>Select Data Sources.</li>
<li>Select the Prometheus data source.</li>
</ol>
<p>Note: The url of Prometheus is <code>http://your_ip:9090</code>.</p>
<img src="operation/../../resources/images/grafana-datasource.png" height="400" width="200"/>
<p>See more details <a href="https://grafana.com/docs/grafana/latest/datasources/prometheus/">here</a>.</p>
<h3 id="import-grafana-dashboard"><a class="header" href="#import-grafana-dashboard">Import grafana dashboard</a></h3>
<p><a href="operation/../../resources/grafana-dashboard.json">dashboard json</a></p>
<img src="operation/../../resources/images/grafana-dashboard.png" height="400" width="200"/>
<h2 id="horaedb-metrics"><a class="header" href="#horaedb-metrics">HoraeDB Metrics</a></h2>
<p>After importing the dashboard, you will see the following page:</p>
<img src="operation/../../resources/images/grafana-horaedb-dashboard.png" height="400" width="600"/>
<h3 id="panels"><a class="header" href="#panels">Panels</a></h3>
<ul>
<li>tps: Number of cluster write requests.</li>
<li>qps: Number of cluster query requests.</li>
<li>99th query/write duration: 99th quantile of write and query duration.</li>
<li>table query: Query group by table.</li>
<li>99th write duration details by instance: 99th quantile of write duration group by instance.</li>
<li>99th query duration details by instance: 99th quantile of query duration group by instance.</li>
<li>99th write partition table duration: 99th quantile of write duration of partition table.</li>
<li>table rows: The rows of data written.</li>
<li>table rows by instance: The written rows by instance.</li>
<li>total tables to write: Number of tables with data written.</li>
<li>flush count: Number of HoraeDB flush.</li>
<li>99th flush duration details by instance: 99th quantile of flush duration group by instance.</li>
<li>99th write stall duration details by instance: 99th quantile of write stall duration group by instance.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="cluster-operation"><a class="header" href="#cluster-operation">Cluster Operation</a></h1>
<p>The Operations for HoraeDB cluster mode, it can only be used when HoraeMeta is deployed.</p>
<h2 id="operation-interface"><a class="header" href="#operation-interface">Operation Interface</a></h2>
<p>You need to replace 127.0.0.1 with the actual project path.</p>
<ul>
<li>Query table
When tableNames is not empty, use tableNames for query.
When tableNames is empty, ids are used for query. When querying with ids, schemaName is useless.</li>
</ul>
<pre><code>curl --location 'http://127.0.0.1:8080/api/v1/table/query' \
--header 'Content-Type: application/json' \
-d '{
    &quot;clusterName&quot;:&quot;defaultCluster&quot;,
    &quot;schemaName&quot;:&quot;public&quot;,
    &quot;names&quot;:[&quot;demo1&quot;, &quot;__demo1_0&quot;],
}'

curl --location 'http://127.0.0.1:8080/api/v1/table/query' \
--header 'Content-Type: application/json' \
-d '{
    &quot;clusterName&quot;:&quot;defaultCluster&quot;,
    &quot;ids&quot;:[0, 1]
}'
</code></pre>
<ul>
<li>Query the route of table</li>
</ul>
<pre><code>curl --location --request POST 'http://127.0.0.1:8080/api/v1/route' \
--header 'Content-Type: application/json' \
-d '{
    &quot;clusterName&quot;:&quot;defaultCluster&quot;,
    &quot;schemaName&quot;:&quot;public&quot;,
    &quot;table&quot;:[&quot;demo&quot;]
}'
</code></pre>
<ul>
<li>Query the mapping of shard and node</li>
</ul>
<pre><code>curl --location --request POST 'http://127.0.0.1:8080/api/v1/getNodeShards' \
--header 'Content-Type: application/json' \
-d '{
    &quot;ClusterName&quot;:&quot;defaultCluster&quot;
}'
</code></pre>
<ul>
<li>Query the mapping of table and shard
If ShardIDs in the request is empty, query with all shardIDs in the cluster.</li>
</ul>
<pre><code>curl --location --request POST 'http://127.0.0.1:8080/api/v1/getShardTables' \
--header 'Content-Type: application/json' \
-d '{
    &quot;clusterName&quot;:&quot;defaultCluster&quot;,
    &quot;shardIDs&quot;: [1,2]
}'
</code></pre>
<ul>
<li>Drop table</li>
</ul>
<pre><code>curl --location --request POST 'http://127.0.0.1:8080/api/v1/dropTable' \
--header 'Content-Type: application/json' \
-d '{
    &quot;clusterName&quot;: &quot;defaultCluster&quot;,
    &quot;schemaName&quot;: &quot;public&quot;,
    &quot;table&quot;: &quot;demo&quot;
}'
</code></pre>
<ul>
<li>Transfer leader shard</li>
</ul>
<pre><code>curl --location --request POST 'http://127.0.0.1:8080/api/v1/transferLeader' \
--header 'Content-Type: application/json' \
-d '{
    &quot;clusterName&quot;:&quot;defaultCluster&quot;,
    &quot;shardID&quot;: 1,
    &quot;oldLeaderNodeName&quot;: &quot;127.0.0.1:8831&quot;,
    &quot;newLeaderNodeName&quot;: &quot;127.0.0.1:18831&quot;
}'
</code></pre>
<ul>
<li>Split shard</li>
</ul>
<pre><code>curl --location --request POST 'http://127.0.0.1:8080/api/v1/split' \
--header 'Content-Type: application/json' \
-d '{
    &quot;clusterName&quot; : &quot;defaultCluster&quot;,
    &quot;schemaName&quot; :&quot;public&quot;,
    &quot;nodeName&quot; :&quot;127.0.0.1:8831&quot;,
    &quot;shardID&quot; : 0,
    &quot;splitTables&quot;:[&quot;demo&quot;]
}'
</code></pre>
<ul>
<li>Create cluster</li>
</ul>
<pre><code>curl --location 'http://127.0.0.1:8080/api/v1/clusters' \
--header 'Content-Type: application/json' \
--data '{
    &quot;name&quot;:&quot;testCluster&quot;,
    &quot;nodeCount&quot;:3,
    &quot;shardTotal&quot;:9,
    &quot;enableScheduler&quot;:true,
    &quot;topologyType&quot;:&quot;static&quot;
}'
</code></pre>
<ul>
<li>Update cluster</li>
</ul>
<pre><code>curl --location --request PUT 'http://127.0.0.1:8080/api/v1/clusters/{NewClusterName}' \
--header 'Content-Type: application/json' \
--data '{
    &quot;nodeCount&quot;:28,
    &quot;shardTotal&quot;:128,
    &quot;enableSchedule&quot;:true,
    &quot;topologyType&quot;:&quot;dynamic&quot;
}'
</code></pre>
<ul>
<li>List clusters</li>
</ul>
<pre><code>curl --location 'http://127.0.0.1:8080/api/v1/clusters'
</code></pre>
<ul>
<li>Update DeployMode</li>
</ul>
<pre><code>curl --location --request PUT 'http://127.0.0.1:8080/api/v1/cluster/{ClusterName}/deployMode' \
--header 'Content-Type: application/json' \
--data '{
    &quot;enable&quot;:true
}'
</code></pre>
<ul>
<li>Query DeployMode</li>
</ul>
<pre><code>curl --location 'http://127.0.0.1:8080/api/v1/cluster/{ClusterName}/deployMode'
</code></pre>
<ul>
<li>Update flow limiter</li>
</ul>
<pre><code>curl --location --request PUT 'http://127.0.0.1:8080/api/v1/flowLimiter' \
--header 'Content-Type: application/json' \
--data '{
    &quot;limit&quot;:1000,
    &quot;burst&quot;:10000,
    &quot;enable&quot;:true
}'
</code></pre>
<ul>
<li>Query information of flow limiter</li>
</ul>
<pre><code>curl --location 'http://127.0.0.1:8080/api/v1/flowLimiter'
</code></pre>
<ul>
<li>List nodes of HoraeMeta cluster</li>
</ul>
<pre><code>curl --location 'http://127.0.0.1:8080/api/v1/etcd/member'
</code></pre>
<ul>
<li>Move leader of HoraeMeta cluster</li>
</ul>
<pre><code>curl --location 'http://127.0.0.1:8080/api/v1/etcd/moveLeader' \
--header 'Content-Type: application/json' \
--data '{
    &quot;memberName&quot;:&quot;meta1&quot;
}'
</code></pre>
<ul>
<li>Add node of HoraeMeta cluster</li>
</ul>
<pre><code>curl --location --request PUT 'http://127.0.0.1:8080/api/v1/etcd/member' \
--header 'Content-Type: application/json' \
--data '{
    &quot;memberAddrs&quot;:[&quot;http://127.0.0.1:42380&quot;]
}'
</code></pre>
<ul>
<li>Replace node of HoraeMeta cluster</li>
</ul>
<pre><code>curl --location 'http://127.0.0.1:8080/api/v1/etcd/member' \
--header 'Content-Type: application/json' \
--data '{
    &quot;oldMemberName&quot;:&quot;meta0&quot;,
    &quot;newMemberAddr&quot;:[&quot;http://127.0.0.1:42380&quot;]
}'
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="ecosystem"><a class="header" href="#ecosystem">Ecosystem</a></h1>
<p>HoraeDB has an open ecosystem that encourages collaboration and innovation, allowing developers to use what suits them best.</p>
<p>Currently following systems are supported:</p>
<ul>
<li><a href="ecosystem/prometheus.html">Prometheus</a></li>
<li><a href="ecosystem/influxdb.html">InfluxDB</a></li>
<li><a href="ecosystem/opentsdb.html">OpenTSDB</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="prometheus-1"><a class="header" href="#prometheus-1">Prometheus</a></h1>
<p><a href="https://prometheus.io/">Prometheus</a> is a popular cloud-native monitoring tool that is widely adopted by organizations due to its scalability, reliability, and scalability. It is used to scrape metrics from cloud-native services, such as Kubernetes and OpenShift, and stores it in a time-series database. Prometheus is also easily extensible, allowing users to extend its features and capabilities with other databases.</p>
<p>HoraeDB can be used as a long-term storage solution for Prometheus. Both remote read and remote write API are supported.</p>
<h2 id="config-1"><a class="header" href="#config-1">Config</a></h2>
<p>You can configure Prometheus to use HoraeDB as a remote storage by adding following lines to <code>prometheus.yml</code>:</p>
<pre><code class="language-yml">remote_write:
  - url: &quot;http://&lt;address&gt;:&lt;http_port&gt;/prom/v1/write&quot;
remote_read:
  - url: &quot;http://&lt;address&gt;:&lt;http_port&gt;/prom/v1/read&quot;
</code></pre>
<p>Each metric will be converted to one table in HoraeDB:</p>
<ul>
<li>labels are mapped to corresponding <code>string</code> tag column</li>
<li>timestamp of sample is mapped to a timestamp <code>timestmap</code> column</li>
<li>value of sample is mapped to a double <code>value</code> column</li>
</ul>
<p>For example, <code>up</code> metric below will be mapped to <code>up</code> table:</p>
<pre><code>up{env=&quot;dev&quot;, instance=&quot;127.0.0.1:9090&quot;, job=&quot;prometheus-server&quot;}
</code></pre>
<p>Its corresponding table in HoraeDB(Note: The TTL for creating a table is 7d, and points written exceed TTL will be discarded directly):</p>
<pre><code>CREATE TABLE `up` (
    `timestamp` timestamp NOT NULL,
    `tsid` uint64 NOT NULL,
    `env` string TAG,
    `instance` string TAG,
    `job` string TAG,
    `value` double,
    PRIMARY KEY (tsid, timestamp),
    timestamp KEY (timestamp)
);

SELECT * FROM up;
</code></pre>
<div class="table-wrapper"><table><thead><tr><th style="text-align: center">tsid</th><th style="text-align: center">timestamp</th><th style="text-align: center">env</th><th style="text-align: center">instance</th><th style="text-align: center">job</th><th style="text-align: center">value</th></tr></thead><tbody>
<tr><td style="text-align: center">12683162471309663278</td><td style="text-align: center">1675824740880</td><td style="text-align: center">dev</td><td style="text-align: center">127.0.0.1:9090</td><td style="text-align: center">prometheus-server</td><td style="text-align: center">1</td></tr>
</tbody></table>
</div><div style="break-before: page; page-break-before: always;"></div><h1 id="influxdb"><a class="header" href="#influxdb">InfluxDB</a></h1>
<p><a href="https://www.influxdata.com/products/influxdb-overview/">InfluxDB</a> is a time series database designed to handle high write and query loads. It is an integral component of the TICK stack. InfluxDB is meant to be used as a backing store for any use case involving large amounts of timestamped data, including DevOps monitoring, application metrics, IoT sensor data, and real-time analytics.</p>
<p>HoraeDB support <a href="https://docs.influxdata.com/influxdb/v1.8/tools/api/#influxdb-1x-http-endpoints">InfluxDB v1.8</a> write and query API.</p>
<blockquote>
<p>Warn: users need to add following config to server's config in order to try InfluxDB write/query.</p>
</blockquote>
<pre><code class="language-toml">[server.default_schema_config]
default_timestamp_column_name = &quot;time&quot;
</code></pre>
<h1 id="write-1"><a class="header" href="#write-1">Write</a></h1>
<pre><code class="language-shell">curl -i -XPOST &quot;http://localhost:5440/influxdb/v1/write&quot; --data-binary '
demo,tag1=t1,tag2=t2 field1=90,field2=100 1679994647000
demo,tag1=t1,tag2=t2 field1=91,field2=101 1679994648000
demo,tag1=t11,tag2=t22 field1=90,field2=100 1679994647000
demo,tag1=t11,tag2=t22 field1=91,field2=101 1679994648000
'
</code></pre>
<p>Post payload is in <a href="https://docs.influxdata.com/influxdb/v1.8/write_protocols/line_protocol_reference/">InfluxDB line protocol</a> format.</p>
<p>Measurement will be mapped to table in HoraeDB, and it will be created automatically in first write(Note: The default TTL is 7d, and points written exceed TTL will be discarded directly).</p>
<p>For example, when inserting data above, table below will be automatically created in HoraeDB:</p>
<pre><code class="language-sql">CREATE TABLE `demo` (
    `tsid` uint64 NOT NULL,
    `time` timestamp NOT NULL,
    `field1` double,
    `field2` double,
    `tag1` string TAG,
    `tag2` string TAG,
    PRIMARY KEY (tsid, time),
    timestamp KEY (time))
</code></pre>
<h2 id="note"><a class="header" href="#note">Note</a></h2>
<ul>
<li>When InfluxDB writes data, the timestamp precision is nanosecond by default, HoraeDB only supports millisecond timestamp, user can specify the data precision by <code>precision</code> parameter, HoraeDB will automatically convert to millisecond precision internally.</li>
<li>Query string parameters such as <code>db</code> aren't supported.</li>
</ul>
<h1 id="query"><a class="header" href="#query">Query</a></h1>
<pre><code class="language-shell"> curl -G 'http://localhost:5440/influxdb/v1/query' --data-urlencode 'q=SELECT * FROM &quot;demo&quot;'
</code></pre>
<p>Query result is same with InfluxDB:</p>
<pre><code class="language-json">{
  &quot;results&quot;: [
    {
      &quot;statement_id&quot;: 0,
      &quot;series&quot;: [
        {
          &quot;name&quot;: &quot;demo&quot;,
          &quot;columns&quot;: [&quot;time&quot;, &quot;field1&quot;, &quot;field2&quot;, &quot;tag1&quot;, &quot;tag2&quot;],
          &quot;values&quot;: [
            [1679994647000, 90.0, 100.0, &quot;t1&quot;, &quot;t2&quot;],
            [1679994647000, 90.0, 100.0, &quot;t11&quot;, &quot;t22&quot;],
            [1679994648000, 91.0, 101.0, &quot;t1&quot;, &quot;t2&quot;],
            [1679994648000, 91.0, 101.0, &quot;t11&quot;, &quot;t22&quot;]
          ]
        }
      ]
    }
  ]
}
</code></pre>
<h2 id="usage-in-grafana"><a class="header" href="#usage-in-grafana">Usage in Grafana</a></h2>
<p>HoraeDB can be used as InfluxDB data source in Grafana.</p>
<ul>
<li>Select InfluxDB type when add data source</li>
<li>Then input <code>http://{ip}:{5440}/influxdb/v1/</code> in HTTP URL. For local deployment, URL is http://localhost:5440/influxdb/v1/</li>
<li><code>Save &amp; test</code></li>
</ul>
<h2 id="note-1"><a class="header" href="#note-1">Note</a></h2>
<p>Query string parameters such as <code>epoch</code>, <code>db</code>, <code>pretty</code> aren't supported.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="opentsdb"><a class="header" href="#opentsdb">OpenTSDB</a></h1>
<p><a href="http://opentsdb.net/">OpenTSDB</a> is a distributed and scalable time series database based on HBase.</p>
<h1 id="write-2"><a class="header" href="#write-2">Write</a></h1>
<p>HoraeDB follows the <a href="http://opentsdb.net/docs/build/html/api_http/put.html">OpenTSDB put</a> write protocol.</p>
<p><code>summary</code> and <code>detailed</code> are not yet supported.</p>
<pre><code>curl --location 'http://localhost:5440/opentsdb/api/put' \
--header 'Content-Type: application/json' \
-d '[{
    &quot;metric&quot;: &quot;sys.cpu.nice&quot;,
    &quot;timestamp&quot;: 1692588459000,
    &quot;value&quot;: 18,
    &quot;tags&quot;: {
       &quot;host&quot;: &quot;web01&quot;,
       &quot;dc&quot;: &quot;lga&quot;
    }
},
{
    &quot;metric&quot;: &quot;sys.cpu.nice&quot;,
    &quot;timestamp&quot;: 1692588459000,
    &quot;value&quot;: 18,
    &quot;tags&quot;: {
       &quot;host&quot;: &quot;web01&quot;
    }
}]'
'
</code></pre>
<p>Metric will be mapped to table in HoraeDB, and it will be created automatically in first write(Note: The default TTL is 7d, and points written exceed TTL will be discarded directly).</p>
<p>For example, when inserting data above, table below will be automatically created in HoraeDB:</p>
<pre><code>CREATE TABLE `sys.cpu.nice`(
    `tsid` uint64 NOT NULL,
    `timestamp` timestamp NOT NULL,
    `dc` string TAG,
    `host` string TAG,
    `value` bigint,
    PRIMARY KEY(tsid, timestamp),
    TIMESTAMP KEY(timestamp))
    ENGINE = Analytic
    WITH(arena_block_size = '2097152', compaction_strategy = 'default',
    compression = 'ZSTD', enable_ttl = 'true', num_rows_per_row_group = '8192',
    segment_duration = '2h', storage_format = 'AUTO', ttl = '7d',
    update_mode = 'OVERWRITE', write_buffer_size = '33554432')
</code></pre>
<h1 id="query-1"><a class="header" href="#query-1">Query</a></h1>
<p>OpenTSDB query protocol is not currently supported, <a href="https://github.com/apache/incubator-horaedb/issues/904">tracking issue</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><div class="table-wrapper"><table><thead><tr><th style="text-align: center">target</th><th style="text-align: center">OS</th><th style="text-align: center">status</th></tr></thead><tbody>
<tr><td style="text-align: center">x86_64-unknown-linux-gnu</td><td style="text-align: center">kernel 4.9+</td><td style="text-align: center">✅</td></tr>
<tr><td style="text-align: center">x86_64-apple-darwin</td><td style="text-align: center">10.15+, Catalina+</td><td style="text-align: center">✅</td></tr>
<tr><td style="text-align: center">aarch64-apple-darwin</td><td style="text-align: center">11+, Big Sur+</td><td style="text-align: center">✅</td></tr>
<tr><td style="text-align: center">aarch64-unknown-linux-gnu</td><td style="text-align: center">TBD</td><td style="text-align: center">tracked on <a href="https://github.com/apache/incubator-horaedb/issues/63">#63</a></td></tr>
<tr><td style="text-align: center">*-windows</td><td style="text-align: center">*</td><td style="text-align: center">❌</td></tr>
</tbody></table>
</div><div style="break-before: page; page-break-before: always;"></div><p>In order to compile HoraeDB, some relevant dependencies(including the <code>Rust</code> toolchain) should be installed.</p>
<h1 id="dependenciesubuntu2004"><a class="header" href="#dependenciesubuntu2004">Dependencies(Ubuntu20.04)</a></h1>
<p>Assuming the development environment is Ubuntu20.04, execute the following command to install the required dependencies:</p>
<pre><code class="language-shell">sudo apt install git curl gcc g++ libssl-dev pkg-config cmake protobuf-compiler
</code></pre>
<p>It should be noted that the compilation of the project has version requirements for dependencies such as cmake, gcc, g++, etc. If your development environment is an old Linux distribution, it is necessary to manually install these dependencies of a higher version.</p>
<h1 id="dependenciesmacos"><a class="header" href="#dependenciesmacos">Dependencies(MacOS)</a></h1>
<p>If the development environment is MacOS, execute the following command to install the required dependencies.</p>
<ol>
<li>Install command line tools:</li>
</ol>
<pre><code class="language-shell">xcode-select --install
</code></pre>
<ol start="2">
<li>Install cmake:</li>
</ol>
<pre><code class="language-shell">brew install cmake
</code></pre>
<ol start="3">
<li>Install protobuf:</li>
</ol>
<pre><code class="language-shell">brew install protobuf
</code></pre>
<h1 id="rust-1"><a class="header" href="#rust-1">Rust</a></h1>
<p><code>Rust</code> can be installed by <a href="https://rustup.rs/">rustup</a>. After installing rustup, when entering the HoraeDB project, the specified <code>Rust</code> version will be automatically downloaded according to the rust-toolchain file.</p>
<p>After execution, you need to add environment variables to use the <code>Rust</code> toolchain. Basically, just put the following commands into your <code>~/.bashrc</code> or <code>~/.bash_profile</code>:</p>
<pre><code class="language-shell">source $HOME/.cargo/env
</code></pre>
<h1 id="compile-and-run"><a class="header" href="#compile-and-run">Compile and Run</a></h1>
<p>Compile HoraeDB by the following command:</p>
<pre><code>cargo build --release
</code></pre>
<p>Then you can run HoraeDB using the default configuration file provided in the codebase.</p>
<pre><code class="language-bash">./target/release/horaedb-server --config ./docs/minimal.toml
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h2 id="profiling"><a class="header" href="#profiling">Profiling</a></h2>
<h3 id="cpu-profiling"><a class="header" href="#cpu-profiling">CPU profiling</a></h3>
<p>HoraeDB provides cpu profiling http api <code>debug/profile/cpu</code>.</p>
<p>Example:</p>
<pre><code>// 60s cpu sampling data
curl 0:5000/debug/profile/cpu/60

// Output file path.
/tmp/flamegraph_cpu.svg
</code></pre>
<h3 id="heap-profiling"><a class="header" href="#heap-profiling">Heap profiling</a></h3>
<p>HoraeDB provides heap profiling http api <code>debug/profile/heap</code>.</p>
<h3 id="install-dependencies"><a class="header" href="#install-dependencies">Install dependencies</a></h3>
<pre><code>sudo yum install -y jemalloc-devel ghostscript graphviz
</code></pre>
<p>Example:</p>
<pre><code>// enable malloc prof
export MALLOC_CONF=prof:true

// run horaedb-server
./horaedb-server ....

// 60s cpu sampling data
curl -L '0:5000/debug/profile/heap/60' &gt; /tmp/heap_profile
jeprof --show_bytes --pdf /usr/bin/horaedb-server /tmp/heap_profile &gt; profile_heap.pdf

jeprof --show_bytes --svg /usr/bin/horaedb-server /tmp/heap_profile &gt; profile_heap.svg
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="conventional-commit-guide"><a class="header" href="#conventional-commit-guide">Conventional Commit Guide</a></h1>
<p>This document describes how we use <a href="https://www.conventionalcommits.org/en/v1.0.0/">conventional commit</a> in our development.</p>
<h1 id="structure"><a class="header" href="#structure">Structure</a></h1>
<p>We would like to structure our commit message like this:</p>
<pre><code>&lt;type&gt;[optional scope]: &lt;description&gt;
</code></pre>
<p>There are three parts. <code>type</code> is used to classify which kind of work this commit does. <code>scope</code> is an optional field that provides additional contextual information. And the last field is your <code>description</code> of this commit.</p>
<h1 id="type"><a class="header" href="#type">Type</a></h1>
<p>Here we list some common <code>type</code>s and their meanings.</p>
<ul>
<li><code>feat</code>: Implement a new feature.</li>
<li><code>fix</code>: Patch a bug.</li>
<li><code>docs</code>: Add document or comment.</li>
<li><code>build</code>: Change the build script or configuration.</li>
<li><code>style</code>: Style change (only). No logic involved.</li>
<li><code>refactor</code>: Refactor an existing module for performance, structure, or other reasons.</li>
<li><code>test</code>: Enhance test coverage or sqlness.</li>
<li><code>chore</code>: None of the above.</li>
</ul>
<h1 id="scope"><a class="header" href="#scope">Scope</a></h1>
<p>The <code>scope</code> is more flexible than <code>type</code>. And it may have different values under different <code>type</code>s.</p>
<p>For example, In a <code>feat</code> or <code>build</code> commit we may use the code module to define scope, like</p>
<pre><code>feat(cluster):
feat(server):

build(ci):
build(image):
</code></pre>
<p>And in <code>docs</code> or <code>refactor</code> commits the motivation is prefer to label the <code>scope</code>, like</p>
<pre><code>docs(comment):
docs(post):

refactor(perf):
refactor(usability):
</code></pre>
<p>But you don't need to add a scope every time. This isn't mandatory. It's just a way to help describe the commit.</p>
<h1 id="after-all"><a class="header" href="#after-all">After all</a></h1>
<p>There are many other rules or scenarios in <a href="https://www.conventionalcommits.org/en/v1.0.0/">conventional commit</a>'s website. We are still exploring a better and more friendly workflow. Please do let us know by <a href="https://github.com/apache/incubator-horaedb/issues/new/choose">open an issue</a> if you have any suggestions ❤️</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="rationale-and-goals"><a class="header" href="#rationale-and-goals">Rationale and Goals</a></h1>
<p>As every Rust programmer knows, the language has many powerful features, and there are often
several patterns which can express the same idea. Also, as every professional programmer comes to
discover, code is almost always read far more than it is written.</p>
<p>Thus, we choose to use a consistent set of idioms throughout our code so that it is easier to read
and understand for both existing and new contributors.</p>
<h2 id="unsafe-and-platform-dependent-conditional-compilation"><a class="header" href="#unsafe-and-platform-dependent-conditional-compilation">Unsafe and Platform-Dependent conditional compilation</a></h2>
<h3 id="avoid-unsafe-rust"><a class="header" href="#avoid-unsafe-rust">Avoid <code>unsafe</code> Rust</a></h3>
<p>One of the main reasons to use Rust as an implementation language is its strong memory safety
guarantees; Almost all of these guarantees are voided by the use of <code>unsafe</code>. Thus, unless there is
an excellent reason and the use is discussed beforehand, it is unlikely HoraeDB will accept patches
with <code>unsafe</code> code.</p>
<p>We may consider taking unsafe code given:</p>
<ul>
<li>performance benchmarks showing a <em>very</em> compelling improvement</li>
<li>a compelling explanation of why the same performance can not be achieved using <code>safe</code> code</li>
<li>tests showing how it works safely across threads</li>
</ul>
<h3 id="avoid-platform-specific-conditional-compilation-cfg"><a class="header" href="#avoid-platform-specific-conditional-compilation-cfg">Avoid platform-specific conditional compilation <code>cfg</code></a></h3>
<p>We hope that HoraeDB is usable across many different platforms and Operating systems, which means we
put a high value on standard Rust.</p>
<p>While some performance critical code may require architecture specific instructions, (e.g.
<code>AVX512</code>) most of the code should not.</p>
<h2 id="errors"><a class="header" href="#errors">Errors</a></h2>
<h3 id="all-errors-should-follow-the-snafu-crate-philosophy-and-use-snafu-functionality"><a class="header" href="#all-errors-should-follow-the-snafu-crate-philosophy-and-use-snafu-functionality">All errors should follow the <a href="https://docs.rs/snafu/0.6.10/snafu/guide/philosophy/index.html">SNAFU crate philosophy</a> and use SNAFU functionality</a></h3>
<p><em>Good</em>:</p>
<ul>
<li>Derives <code>Snafu</code> and <code>Debug</code> functionality</li>
<li>Has a useful, end-user-friendly display message</li>
</ul>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[derive(Snafu, Debug)]
pub enum Error {
    #[snafu(display(r#&quot;Conversion needs at least one line of data&quot;#))]
    NeedsAtLeastOneLine,
    // ...
}
<span class="boring">}</span></code></pre></pre>
<p><em>Bad</em>:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub enum Error {
    NeedsAtLeastOneLine,
    // ...
<span class="boring">}</span></code></pre></pre>
<h3 id="use-the-ensure-macro-to-check-a-condition-and-return-an-error"><a class="header" href="#use-the-ensure-macro-to-check-a-condition-and-return-an-error">Use the <code>ensure!</code> macro to check a condition and return an error</a></h3>
<p><em>Good</em>:</p>
<ul>
<li>Reads more like an <code>assert!</code></li>
<li>Is more concise</li>
</ul>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>ensure!(!self.schema_sample.is_empty(), NeedsAtLeastOneLine);
<span class="boring">}</span></code></pre></pre>
<p><em>Bad</em>:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>if self.schema_sample.is_empty() {
    return Err(Error::NeedsAtLeastOneLine {});
}
<span class="boring">}</span></code></pre></pre>
<h3 id="errors-should-be-defined-in-the-module-they-are-instantiated"><a class="header" href="#errors-should-be-defined-in-the-module-they-are-instantiated">Errors should be defined in the module they are instantiated</a></h3>
<p><em>Good</em>:</p>
<ul>
<li>Groups related error conditions together most closely with the code that produces them</li>
<li>Reduces the need to <code>match</code> on unrelated errors that would never happen</li>
</ul>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[derive(Debug, Snafu)]
pub enum Error {
    #[snafu(display(&quot;Not implemented: {}&quot;, operation_name))]
    NotImplemented { operation_name: String }
}
// ...
ensure!(foo.is_implemented(), NotImplemented {
    operation_name: &quot;foo&quot;,
}
<span class="boring">}</span></code></pre></pre>
<p><em>Bad</em>:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use crate::errors::NotImplemented;
// ...
ensure!(foo.is_implemented(), NotImplemented {
    operation_name: &quot;foo&quot;,
}
<span class="boring">}</span></code></pre></pre>
<h3 id="the-result-type-alias-should-be-defined-in-each-module"><a class="header" href="#the-result-type-alias-should-be-defined-in-each-module">The <code>Result</code> type alias should be defined in each module</a></h3>
<p><em>Good</em>:</p>
<ul>
<li>Reduces repetition</li>
</ul>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub type Result&lt;T, E = Error&gt; = std::result::Result&lt;T, E&gt;;
...
fn foo() -&gt; Result&lt;bool&gt; { true }
<span class="boring">}</span></code></pre></pre>
<p><em>Bad</em>:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>...
fn foo() -&gt; Result&lt;bool, Error&gt; { true }
<span class="boring">}</span></code></pre></pre>
<h3 id="err-variants-should-be-returned-with-fail"><a class="header" href="#err-variants-should-be-returned-with-fail"><code>Err</code> variants should be returned with <code>fail()</code></a></h3>
<p><em>Good</em>:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>return NotImplemented {
    operation_name: &quot;Parquet format conversion&quot;,
}.fail();
<span class="boring">}</span></code></pre></pre>
<p><em>Bad</em>:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>return Err(Error::NotImplemented {
    operation_name: String::from(&quot;Parquet format conversion&quot;),
});
<span class="boring">}</span></code></pre></pre>
<h3 id="use-context-to-wrap-underlying-errors-into-module-specific-errors"><a class="header" href="#use-context-to-wrap-underlying-errors-into-module-specific-errors">Use <code>context</code> to wrap underlying errors into module specific errors</a></h3>
<p><em>Good</em>:</p>
<ul>
<li>Reduces boilerplate</li>
</ul>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>input_reader
    .read_to_string(&amp;mut buf)
    .context(UnableToReadInput {
        input_filename,
    })?;
<span class="boring">}</span></code></pre></pre>
<p><em>Bad</em>:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>input_reader
    .read_to_string(&amp;mut buf)
    .map_err(|e| Error::UnableToReadInput {
        name: String::from(input_filename),
        source: e,
    })?;
<span class="boring">}</span></code></pre></pre>
<p><em>Hint for <code>Box&lt;dyn::std::error::Error&gt;</code> in Snafu</em>:</p>
<p>If your error contains a trait object (e.g. <code>Box&lt;dyn std::error::Error + Send + Sync&gt;</code>), in order
to use <code>context()</code> you need to wrap the error in a <code>Box</code>, we provide a <code>box_err</code> function to help do this conversion:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[derive(Debug, Snafu)]
pub enum Error {

    #[snafu(display(&quot;gRPC planner got error listing partition keys: {}&quot;, source))]
    ListingPartitions {
        source: Box&lt;dyn std::error::Error + Send + Sync&gt;,
    },
}

...

use use common_util::error::BoxError;

  // Wrap error in a box prior to calling context()
database
  .partition_keys()
  .await
  .box_err()
  .context(ListingPartitions)?;
<span class="boring">}</span></code></pre></pre>
<h3 id="each-error-cause-in-a-module-should-have-a-distinct-error-enum-variant"><a class="header" href="#each-error-cause-in-a-module-should-have-a-distinct-error-enum-variant">Each error cause in a module should have a distinct <code>Error</code> enum variant</a></h3>
<p>Specific error types are preferred over a generic error with a <code>message</code> or <code>kind</code> field.</p>
<p><em>Good</em>:</p>
<ul>
<li>Makes it easier to track down the offending code based on a specific failure</li>
<li>Reduces the size of the error enum (<code>String</code> is 3x 64-bit vs no space)</li>
<li>Makes it easier to remove vestigial errors</li>
<li>Is more concise</li>
</ul>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[derive(Debug, Snafu)]
pub enum Error {
    #[snafu(display(&quot;Error writing remaining lines {}&quot;, source))]
    UnableToWriteGoodLines { source: IngestError },

    #[snafu(display(&quot;Error while closing the table writer {}&quot;, source))]
    UnableToCloseTableWriter { source: IngestError },
}

// ...

write_lines.context(UnableToWriteGoodLines)?;
close_writer.context(UnableToCloseTableWriter))?;
<span class="boring">}</span></code></pre></pre>
<p><em>Bad</em>:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub enum Error {
    #[snafu(display(&quot;Error {}: {}&quot;, message, source))]
    WritingError {
        source: IngestError,
        message: String,
    },
}

write_lines.context(WritingError {
    message: String::from(&quot;Error while writing remaining lines&quot;),
})?;
close_writer.context(WritingError {
    message: String::from(&quot;Error while closing the table writer&quot;),
})?;
<span class="boring">}</span></code></pre></pre>
<h3 id="leaf-error-should-contains-backtrace"><a class="header" href="#leaf-error-should-contains-backtrace">Leaf error should contains backtrace</a></h3>
<p>In order to make debugging easier, leaf errors in error chain should contains a backtrace.</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Error in module A
pub enum Error {
    #[snafu(display(&quot;This is a leaf error, source:{}.\nBacktrace:\n{}&quot;, source, backtrace))]
    LeafError {
        source: ErrorFromDependency,
        backtrace: Backtrace
    },
}

// Error in module B
pub enum Error {
    #[snafu(display(&quot;Another error, source:{}.\nBacktrace:\n{}&quot;, source, backtrace))]
    AnotherError {
        /// This error wraps another error that already has a
        /// backtrace. Instead of capturing our own, we forward the
        /// request for the backtrace to the inner error. This gives a
        /// more accurate backtrace.
        #[snafu(backtrace)]
        source: crate::A::Error,
    },
}
<span class="boring">}</span></code></pre></pre>
<h2 id="tests"><a class="header" href="#tests">Tests</a></h2>
<h3 id="dont-return-result-from-test-functions"><a class="header" href="#dont-return-result-from-test-functions">Don't return <code>Result</code> from test functions</a></h3>
<p>At the time of this writing, if you return <code>Result</code> from test functions to use <code>?</code> in the test
function body and an <code>Err</code> value is returned, the test failure message is not particularly helpful.
Therefore, prefer not having a return type for test functions and instead using <code>expect</code> or
<code>unwrap</code> in test function bodies.</p>
<p><em>Good</em>:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[test]
fn google_cloud() {
    let config = Config::new();
    let integration = ObjectStore::new_google_cloud_storage(GoogleCloudStorage::new(
        config.service_account,
        config.bucket,
    ));

    put_get_delete_list(&amp;integration).unwrap();
    list_with_delimiter(&amp;integration).unwrap();
}
<span class="boring">}</span></code></pre></pre>
<p><em>Bad</em>:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>type TestError = Box&lt;dyn std::error::Error + Send + Sync + 'static&gt;;
type Result&lt;T, E = TestError&gt; = std::result::Result&lt;T, E&gt;;

#[test]
fn google_cloud() -&gt; Result&lt;()&gt; {
    let config = Config::new();
    let integration = ObjectStore::new_google_cloud_storage(GoogleCloudStorage::new(
        config.service_account,
        config.bucket,
    ));

    put_get_delete_list(&amp;integration)?;
    list_with_delimiter(&amp;integration)?;
    Ok(())
}
<span class="boring">}</span></code></pre></pre>
<h2 id="thanks"><a class="header" href="#thanks">Thanks</a></h2>
<p>Initial version of this doc is forked from <a href="https://github.com/influxdata/influxdb_iox/blob/main/docs/style_guide.md">influxdb_iox</a>, thanks for their hard work.</p>
<div style="break-before: page; page-break-before: always;"></div><h2 id="roadmap"><a class="header" href="#roadmap">RoadMap</a></h2>
<h3 id="v010"><a class="header" href="#v010">v0.1.0</a></h3>
<ul>
<li><input disabled="" type="checkbox" checked=""/>
Standalone version, local storage</li>
<li><input disabled="" type="checkbox" checked=""/>
Analytical storage format</li>
<li><input disabled="" type="checkbox" checked=""/>
Support SQL</li>
</ul>
<h3 id="v020"><a class="header" href="#v020">v0.2.0</a></h3>
<ul>
<li><input disabled="" type="checkbox" checked=""/>
Distributed version supports static topology defined in config file.</li>
<li><input disabled="" type="checkbox" checked=""/>
The underlying storage supports Aliyun OSS.</li>
<li><input disabled="" type="checkbox" checked=""/>
WAL implementation based on <a href="https://github.com/oceanbase/oceanbase">OBKV</a>.</li>
</ul>
<h3 id="v030"><a class="header" href="#v030">v0.3.0</a></h3>
<ul>
<li><input disabled="" type="checkbox" checked=""/>
Release multi-language clients, including Java, Rust and Python.</li>
<li><input disabled="" type="checkbox" checked=""/>
Static cluster mode with <code>HoraeMeta</code>.</li>
<li><input disabled="" type="checkbox" checked=""/>
Basic implementation of hybrid storage format.</li>
</ul>
<h3 id="v040"><a class="header" href="#v040">v0.4.0</a></h3>
<ul>
<li><input disabled="" type="checkbox" checked=""/>
Implement more sophisticated cluster solution that enhances reliability and scalability of HoraeDB.</li>
<li><input disabled="" type="checkbox" checked=""/>
Set up nightly benchmark with TSBS.</li>
</ul>
<h3 id="v100-alpha-released"><a class="header" href="#v100-alpha-released">v1.0.0-alpha (Released)</a></h3>
<ul>
<li><input disabled="" type="checkbox" checked=""/>
Implement Distributed WAL based on <code>Apache Kafka</code>.</li>
<li><input disabled="" type="checkbox" checked=""/>
Release Golang client.</li>
<li><input disabled="" type="checkbox" checked=""/>
Improve the query performance for classic time series workloads.</li>
<li><input disabled="" type="checkbox" checked=""/>
Support dynamic migration of tables in cluster mode.</li>
</ul>
<h3 id="v100"><a class="header" href="#v100">v1.0.0</a></h3>
<ul>
<li><input disabled="" type="checkbox" checked=""/>
Formally release HoraeDB and its SDKs with all breaking changes finished.</li>
<li><input disabled="" type="checkbox" checked=""/>
Finish the majority of work related to <code>Table Partitioning</code>.</li>
<li><input disabled="" type="checkbox" checked=""/>
Various efforts to improve query performance, especially for cloud-native cluster mode. These works include:
<ul>
<li>Multi-tier cache.</li>
<li>Introduce various methods to reduce the data fetched from remote storage (improve the accuracy of SST data filtering).</li>
<li>Increase the parallelism while fetching data from remote object-store.</li>
</ul>
</li>
<li><input disabled="" type="checkbox" checked=""/>
Improve data ingestion performance by introducing resource control over compaction.</li>
</ul>
<h3 id="afterwards"><a class="header" href="#afterwards">Afterwards</a></h3>
<p>With an in-depth understanding of the time-series database and its various use cases, the majority of our work will focus on performance, reliability, scalability, ease of use, and collaborations with open-source communities.</p>
<ul>
<li><input disabled="" type="checkbox"/>
Add utilities that support <code>PromQL</code>, <code>InfluxQL</code>, <code>OpenTSDB</code> protocol, and so on.</li>
<li><input disabled="" type="checkbox"/>
Provide basic utilities for operation and maintenance. Specifically, the following are included:
<ul>
<li>Deployment tools that fit well for cloud infrastructures like <code>Kubernetes</code>.</li>
<li>Enhance self-observability, especially critical logs and metrics should be supplemented.</li>
</ul>
</li>
<li><input disabled="" type="checkbox"/>
Develop various tools that ease the use of HoraeDB. For example, data import and export tools.</li>
<li><input disabled="" type="checkbox"/>
Explore new storage formats that will improve performance on hybrid workloads (analytical and time-series workloads).</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="introduction-to-horaedbs-architecture"><a class="header" href="#introduction-to-horaedbs-architecture">Introduction to HoraeDB's Architecture</a></h1>
<h2 id="target-1"><a class="header" href="#target-1">Target</a></h2>
<ul>
<li>Provide the overview of HoraeDB to the developers who want to know more about HoraeDB but have no idea where to start.</li>
<li>Make a brief introduction to the important modules of HoraeDB and the connections between these modules but details about their implementations are not be involved.</li>
</ul>
<h2 id="motivation-1"><a class="header" href="#motivation-1">Motivation</a></h2>
<p>HoraeDB is a timeseries database (<strong>TSDB</strong>). However, HoraeDB's goal is to handle both timeseries and analytic workloads compared with the classic TSDB, which usually have a poor performance in handling analytic workloads.</p>
<p>In the classic timeseries database, the <code>Tag</code> columns (InfluxDB calls them <code>Tag</code> and Prometheus calls them <code>Label</code>) are normally indexed by generating an inverted index. However, it is found that the cardinality of <code>Tag</code> varies in different scenarios. And in some scenarios the cardinality of <code>Tag</code> is very high (we name this case after analytic workload), and it takes a very high cost to store and retrieve the inverted index. On the other hand, it is observed that scanning+pruning often used by the analytical databases can do a good job to handle such analytic workload.</p>
<p>The basic design idea of HoraeDB is to adopt a hybrid storage format and the corresponding query method for a better performance in processing both timeseries and analytic workloads.</p>
<h2 id="architecture"><a class="header" href="#architecture">Architecture</a></h2>
<pre><code class="language-plaintext">┌──────────────────────────────────────────┐
│       RPC Layer (HTTP/gRPC/MySQL)        │
└──────────────────────────────────────────┘
┌──────────────────────────────────────────┐
│                 SQL Layer                │
│ ┌─────────────────┐  ┌─────────────────┐ │
│ │     Parser      │  │     Planner     │ │
│ └─────────────────┘  └─────────────────┘ │
└──────────────────────────────────────────┘
┌───────────────────┐  ┌───────────────────┐
│    Interpreter    │  │      Catalog      │
└───────────────────┘  └───────────────────┘
┌──────────────────────────────────────────┐
│               Query Engine               │
│ ┌─────────────────┐  ┌─────────────────┐ │
│ │    Optimizer    │  │    Executor     │ │
│ └─────────────────┘  └─────────────────┘ │
└──────────────────────────────────────────┘
┌──────────────────────────────────────────┐
│         Pluggable Table Engine           │
│  ┌────────────────────────────────────┐  │
│  │              Analytic              │  │
│  │┌────────────────┐┌────────────────┐│  │
│  ││      Wal       ││    Memtable    ││  │
│  │└────────────────┘└────────────────┘│  │
│  │┌────────────────┐┌────────────────┐│  │
│  ││     Flush      ││   Compaction   ││  │
│  │└────────────────┘└────────────────┘│  │
│  │┌────────────────┐┌────────────────┐│  │
│  ││    Manifest    ││  Object Store  ││  │
│  │└────────────────┘└────────────────┘│  │
│  └────────────────────────────────────┘  │
│  ┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │
│           Another Table Engine        │  │
│  └ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │
└──────────────────────────────────────────┘
</code></pre>
<p>The figure above shows the architecture of HoraeDB stand-alone service and the details of some important modules will be described in the following part.</p>
<h3 id="rpc-layer"><a class="header" href="#rpc-layer">RPC Layer</a></h3>
<p>module path: https://github.com/apache/incubator-horaedb/tree/main/server</p>
<p>The current RPC supports multiple protocols including HTTP, gRPC, MySQL.</p>
<p>Basically, HTTP and MySQL are used to debug HoraeDB, query manually and perform DDL operations (such as creating, deleting tables, etc.). And gRPC protocol can be regarded as a customized protocol for high-performance, which is suitable for massive reading and writing operations.</p>
<h3 id="sql-layer"><a class="header" href="#sql-layer">SQL Layer</a></h3>
<p>module path: https://github.com/apache/incubator-horaedb/tree/main/query_frontend</p>
<p>SQL layer takes responsibilities for parsing sql and generating the query plan.</p>
<p>Based on <a href="https://github.com/sqlparser-rs/sqlparser-rs">sqlparser</a> a sql dialect, which introduces some key concepts including <code>Tag</code> and <code>Timestamp</code>, is provided for processing timeseries data. And by utilizing <a href="https://github.com/apache/arrow-datafusion">DataFusion</a> the planner is able to generate both regular logical plans and tailored ones which is used to implement the special operators defined by timeseries queries, e.g <code>PromQL</code>.</p>
<h3 id="interpreter"><a class="header" href="#interpreter">Interpreter</a></h3>
<p>module path: https://github.com/apache/incubator-horaedb/tree/main/interpreters</p>
<p>The <code>Interpreter</code> module encapsulates the SQL <code>CRUD</code> operations. In the query procedure, a sql received by HoraeDB is parsed, converted into the query plan and then executed in some specific interpreter, such as <code>SelectInterpreter</code>, <code>InsertInterpreter</code> and etc.</p>
<h3 id="catalog"><a class="header" href="#catalog">Catalog</a></h3>
<p>module path: https://github.com/apache/incubator-horaedb/tree/main/catalog_impls</p>
<p><code>Catalog</code> is actually the module managing metadata and the levels of metadata adopted by HoraeDB is similar to PostgreSQL: <code>Catalog &gt; Schema &gt; Table</code>, but they are only used as namespace.</p>
<p>At present, <code>Catalog</code> and <code>Schema</code> have two different kinds of implementation for standalone and distributed mode because some strategies to generate ids and ways to persist metadata differ in different mode.</p>
<h3 id="query-engine"><a class="header" href="#query-engine">Query Engine</a></h3>
<p>module path: https://github.com/apache/incubator-horaedb/tree/main/query_engine</p>
<p><code>Query Engine</code> is responsible for optimizing and executing query plan given a basic SQL plan provided by SQL layer and now such work is mainly delegated to <a href="https://github.com/apache/arrow-datafusion">DataFusion</a>.</p>
<p>In addition to the basic functions of SQL, HoraeDB also defines some customized query protocols and optimization rules for some specific query plans by utilizing the extensibility provided by <a href="https://github.com/apache/arrow-datafusion">DataFusion</a>. For example, the implementation of <code>PromQL</code> is implemented in this way and read it if you are interested.</p>
<h3 id="pluggable-table-engine"><a class="header" href="#pluggable-table-engine">Pluggable Table Engine</a></h3>
<p>module path: https://github.com/apache/incubator-horaedb/tree/main/table_engine</p>
<p><code>Table Engine</code> is actually a storage engine for managing tables in HoraeDB and the pluggability of <code>Table Engine</code> is a core design of HoraeDB which matters in achieving our long-term target, e.g supporting handle log or tracing workload by implementing new storage engines. HoraeDB will have multiple kinds of <code>Table Engine</code> for different workloads and the most appropriate one should be chosen as the storage engine according to the workload pattern.</p>
<p>Now the requirements for a <code>Table Engine</code> are:</p>
<ul>
<li>Manage all the shared resources under the engine:
<ul>
<li>Memory</li>
<li>Storage</li>
<li>CPU</li>
</ul>
</li>
<li>Manage metadata of tables such as table schema and table options;</li>
<li>Provide <code>Table</code> instances which provides <code>read</code> and <code>write</code> methods;</li>
<li>Take responsibilities for creating, opening, dropping and closing <code>Table</code> instance;</li>
<li>....</li>
</ul>
<p>Actually the things that a <code>Table Engine</code> needs to process are a little complicated. And now in HoraeDB only one <code>Table Engine</code> called <code>Analytic</code> is provided and does a good job in processing analytical workload, but it is not ready yet to handle the timeseries workload (we plan to enhance it for a better performance by adding some indexes which help handle timeseries workload).</p>
<p>The following part gives a description about details of <code>Analytic Table Engine</code>.</p>
<h4 id="wal"><a class="header" href="#wal">WAL</a></h4>
<p>module path: https://github.com/apache/incubator-horaedb/tree/main/wal</p>
<p>The model of HoraeDB processing data is <code>WAL</code> + <code>MemTable</code> that the recent written data is written to <code>WAL</code> first and then to <code>MemTable</code> and after a certain amount of data in <code>MemTable</code> is accumulated, the data will be organized in a query-friendly form to persistent devices.</p>
<p>Now three implementations of <code>WAL</code> are provided for standalone and distributed mode:</p>
<ul>
<li>For standalone mode, <code>WAL</code> is based on <code>RocksDB</code> and data is persisted on the local disk.</li>
<li>For distributed mode, <code>WAL</code> is required as a distributed component and to be responsible for durability of the newly written data, so now we provide an implementation based on <a href="https://github.com/oceanbase/oceanbase">OceanBase</a>.</li>
<li>For distributed mode, in addition to <a href="https://github.com/oceanbase/oceanbase">OceanBase</a>, we also provide a more lightweight implementation based on <a href="https://github.com/apache/kafka"><code>Apache Kafka</code></a>.</li>
</ul>
<h4 id="memtable"><a class="header" href="#memtable">MemTable</a></h4>
<p>module path: https://github.com/apache/incubator-horaedb/tree/main/analytic_engine/src/memtable</p>
<p>For <code>WAL</code> can't provide efficient data retrieval, the newly written data is also stored in <code>Memtable</code> for efficient data retrieval, after a certain amount of data is reached, HoraeDB organizes the data in <code>MemTable</code> into a query-friendly storage format (<code>SST</code>) and stores it to the persistent device.</p>
<p>The current implementation of <code>MemTable</code> is based on <a href="https://github.com/tikv/agatedb/blob/8510bff2bfde5b766c3f83cf81c00141967d48a4/skiplist">agatedb's skiplist</a>. It allows concurrent reads and writes and can control memory usage based on <a href="https://github.com/apache/incubator-horaedb/tree/main/components/skiplist">Arena</a>.</p>
<h4 id="flush"><a class="header" href="#flush">Flush</a></h4>
<p>module path: https://github.com/apache/incubator-horaedb/blob/main/analytic_engine/src/instance/flush_compaction.rs</p>
<p>What <code>Flush</code> does is that when the memory usage of <code>MemTable</code> reaches the threshold, some <code>MemTables</code> are selected for flushing into query-friendly <code>SST</code>s saved on persistent device.</p>
<p>During the flushing procedure, the data will be divided by a certain time range (which is configured by table option <code>Segment Duration</code>), and any <code>SST</code> is ensured that the timestamps of the data in it are in the same <code>Segment</code>. Actually this is also a common operation in most timeseries databases which organizes data in the time dimension to speed up subsequent time-related operations, such as querying data over a time range and assisting purge data outside the <code>TTL</code>.</p>
<h4 id="compaction"><a class="header" href="#compaction">Compaction</a></h4>
<p>module path: https://github.com/apache/incubator-horaedb/tree/main/analytic_engine/src/compaction</p>
<p>The data of <code>MemTable</code> is flushed as <code>SST</code>s, but the file size of recently flushed <code>SST</code> may be very small. And too small or too many <code>SST</code>s lead to the poor query performance. Therefore, <code>Compaction</code> is then introduced to rearrange the <code>SST</code>s so that the multiple smaller <code>SST</code> files can be compacted into a larger <code>SST</code> file.</p>
<h4 id="manifest"><a class="header" href="#manifest">Manifest</a></h4>
<p>module path: https://github.com/apache/incubator-horaedb/tree/main/analytic_engine/src/meta</p>
<p><code>Manifest</code> takes responsibilities for managing tables' metadata of <code>Analytic Engine</code> including:</p>
<ul>
<li>Table schema and table options;</li>
<li>The sequence number where the newest flush finishes;</li>
<li>The information of all the <code>SST</code>s belonging to the table.</li>
</ul>
<p>Now the <code>Manifest</code> is based on <code>WAL</code> and <code>Object Storage</code>. The newly written updates on the <code>Manifest</code> are persisted as logs in <code>WAL</code>, and in order to avoid infinite expansion of <code>Manifest</code> (actually every <code>Flush</code> leads to an update), <code>Snapshot</code> is also introduced to clean up the history of metadata updates, and the generated <code>Snapshot</code> will be saved to <code>Object Storage</code>.</p>
<h4 id="object-storage-1"><a class="header" href="#object-storage-1">Object Storage</a></h4>
<p>module path: https://github.com/apache/incubator-horaedb/tree/main/components/object_store</p>
<p>The <code>SST</code> generated by <code>Flush</code> needs to be persisted and the abstraction of the persistent storage device is <code>ObjectStore</code> including multiple implementations:</p>
<ul>
<li>Based on local file system;</li>
<li>Based on <a href="https://www.alibabacloud.com/product/object-storage-service">Alibaba Cloud OSS</a>.</li>
</ul>
<p>The distributed architecture of HoraeDB separates storage and computing, which requires <code>Object Store</code> needs to be a highly available and reliable service independent of HoraeDB. Therefore, storage systems like <a href="https://aws.amazon.com/s3/">Amazon S3</a>, <a href="https://www.alibabacloud.com/product/object-storage-service">Alibaba Cloud OSS</a> is a good choice and in the future implementations on storage systems of some other cloud service providers is planned to provide.</p>
<h4 id="sst"><a class="header" href="#sst">SST</a></h4>
<p>module path: https://github.com/apache/incubator-horaedb/tree/main/analytic_engine/src/sst</p>
<p><code>SST</code> is actually an abstraction that can have multiple specific implementations. The current implementation is based on <a href="https://parquet.apache.org/">Parquet</a>, which is a column-oriented data file format designed for efficient data storage and retrieval.</p>
<p>The format of <code>SST</code> is very critical for retrieving data and is also the most important part to perform well in handling both timeseries and analytic workloads. At present, our <a href="https://parquet.apache.org/">Parquet</a>-based implementation is good at processing analytic workload but is poor at processing timeseries workload. In our roadmap, we will explore more storage formats in order to achieve a good performance in both workloads.</p>
<h4 id="space"><a class="header" href="#space">Space</a></h4>
<p>module path: https://github.com/apache/incubator-horaedb/blob/main/analytic_engine/src/space.rs</p>
<p>In <code>Analytic Engine</code>, there is a concept called <code>space</code> and here is an explanation for it to resolve some ambiguities when read source code. Actually <code>Analytic Engine</code> does not have the concept of <code>catalog</code> and <code>schema</code> and only provides two levels of relationship: <code>space</code> and <code>table</code>. And in the implementation, the <code>schema id</code> (which should be unique across all <code>catalog</code>s) on the upper layer is actually mapped to <code>space id</code>.</p>
<p>The <code>space</code> in <code>Analytic Engine</code> serves mainly for isolation of resources for different tenants, such as the usage of memory.</p>
<h2 id="critical-path"><a class="header" href="#critical-path">Critical Path</a></h2>
<p>After a brief introduction to some important modules of HoraeDB, we will give a description for some critical paths in code, hoping to provide interested developers with a guide for reading the code.</p>
<h3 id="query-2"><a class="header" href="#query-2">Query</a></h3>
<pre><code class="language-plaintext">┌───────┐      ┌───────┐      ┌───────┐
│       │──1──▶│       │──2──▶│       │
│Server │      │  SQL  │      │Catalog│
│       │◀─10──│       │◀─3───│       │
└───────┘      └───────┘      └───────┘
                │    ▲
               4│   9│
                │    │
                ▼    │
┌─────────────────────────────────────┐
│                                     │
│             Interpreter             │
│                                     │
└─────────────────────────────────────┘
                           │    ▲
                          5│   8│
                           │    │
                           ▼    │
                   ┌──────────────────┐
                   │                  │
                   │   Query Engine   │
                   │                  │
                   └──────────────────┘
                           │    ▲
                          6│   7│
                           │    │
                           ▼    │
 ┌─────────────────────────────────────┐
 │                                     │
 │            Table Engine             │
 │                                     │
 └─────────────────────────────────────┘
</code></pre>
<p>Take <code>SELECT</code> SQL as an example. The figure above shows the query procedure and the numbers in it indicates the order of calling between the modules.</p>
<p>Here are the details:</p>
<ul>
<li>Server module chooses a proper rpc module (it may be HTTP, gRPC or mysql) to process the requests according the protocol used by the requests;</li>
<li>Parse SQL in the request by the parser;</li>
<li>With the parsed sql and the information provided by catalog/schema module, <a href="https://github.com/apache/arrow-datafusion">DataFusion</a> can generate the logical plan;</li>
<li>With the logical plan, the corresponding <code>Interpreter</code> is created and logical plan will be executed by it;</li>
<li>For the logical plan of normal <code>Select</code> SQL, it will be executed through <code>SelectInterpreter</code>;</li>
<li>In the <code>SelectInterpreter</code> the specific query logic is executed by the <code>Query Engine</code>:
<ul>
<li>Optimize the logical plan;</li>
<li>Generate the physical plan;</li>
<li>Optimize the physical plan;</li>
<li>Execute the physical plan;</li>
</ul>
</li>
<li>The execution of physical plan involves <code>Analytic Engine</code>:
<ul>
<li>Data is obtained by <code>read</code> method of <code>Table</code> instance provided by <code>Analytic Engine</code>;</li>
<li>The source of the table data is <code>SST</code> and <code>Memtable</code>, and the data can be filtered by the pushed down predicates;</li>
<li>After retrieving the table data, <code>Query Engine</code> will complete the specific computation and generate the final results;</li>
</ul>
</li>
<li><code>SelectInterpreter</code> gets the results and feeds them to the protocol module;</li>
<li>After the protocol layer converts the results, the server module responds to the client with them.</li>
</ul>
<p>The following is the flow of function calls in version <a href="https://github.com/apache/incubator-horaedb/releases/tag/v1.2.2">v1.2.2</a>:</p>
<pre><code>                                                       ┌───────────────────────◀─────────────┐    ┌───────────────────────┐
                                                       │      handle_sql       │────────┐    │    │       parse_sql       │
                                                       └───────────────────────┘        │    │    └────────────────┬──────┘
                                                           │             ▲              │    │           ▲         │
                                                           │             │              │    │           │         │
                                                           │             │              │    └36───┐     │        11
                                                          1│             │              │          │     │         │
                                                           │            8│              │          │     │         │
                                                           │             │              │          │    10         │
                                                           │             │              │          │     │         │
                                                           ▼             │              │          │     │         ▼
                                                       ┌─────────────────┴─────┐       9│         ┌┴─────┴────────────────┐───────12─────────▶┌───────────────────────┐
                                                       │maybe_forward_sql_query│        └────────▶│fetch_sql_query_output │                   │   statement_to_plan   │
                                                       └───┬───────────────────┘                  └────┬──────────────────┘◀───────19─────────└───────────────────────┘
                                                           │             ▲                             │              ▲                           │               ▲
                                                           │             │                             │              │                           │               │
                                                           │             │                             │              │                           │               │
                                                           │             │                             │             35                          13              18
                                                          2│            7│                            20              │                           │               │
                                                           │             │                             │              │                           │               │
                                                           │             │                             │              │                           │               │
                                                           │             │                             │              │                           ▼               │
                                                           ▼             │                             ▼              │                       ┌───────────────────────┐
          ┌───────────────────────┐───────────6───────▶┌─────────────────┴─────┐                    ┌─────────────────┴─────┐                 │Planner::statement_to_p│
          │ forward_with_endpoint │                    │        forward        │                    │execute_plan_involving_│                 │          lan          │
          └───────────────────────┘◀────────5──────────└───┬───────────────────┘                 ┌──│    partition_table    │◀────────┐       └───┬───────────────────┘
                                                           │             ▲                       │  └───────────────────────┘         │           │              ▲
                                                           │             │                       │     │              ▲               │           │              │
                                                           │             │                       │     │              │               │          14             17
           ┌───────────────────────┐                       │            4│                       │     │              │               │           │              │
     ┌─────│ PhysicalPlan::execute │                      3│             │                       │    21              │               │           │              │
     │     └───────────────────────┘◀──┐                   │             │                       │     │             22               │           │              │
     │                                 │                   │             │                       │     │              │               │           ▼              │
     │                                 │                   │             │                       │     │              │               │       ┌────────────────────────┐
     │                                 │                   ▼             │                       │     ▼              │              34       │sql_statement_to_datafus│
     │     ┌───────────────────────┐  30               ┌─────────────────┴─────┐                 │  ┌─────────────────┴─────┐         │       │        ion_plan        │
    31     │ build_df_session_ctx  │   │               │         route         │                 │  │   build_interpreter   │         │       └────────────────────────┘
     │     └────┬──────────────────┘   │               └───────────────────────┘                 │  └───────────────────────┘         │           │              ▲
     │          │           ▲          │                                                         │                                    │           │              │
     │         27          26          │                                                        23                                    │          15             16
     │          ▼           │          │                                                         │                                    │           │              │
     └────▶┌────────────────┴──────┐   │               ┌───────────────────────┐                 │                                    │           │              │
           │ execute_logical_plan  ├───┴────32────────▶│       execute         │──────────┐      │   ┌───────────────────────┐        │           ▼              │
           └────┬──────────────────┘◀────────────25────┴───────────────────────┘         33      │   │interpreter_execute_pla│        │       ┌────────────────────────┐
                │           ▲                                           ▲                 └──────┴──▶│           n           │────────┘       │SqlToRel::sql_statement_│
               28           │                                           └──────────24────────────────┴───────────────────────┘                │   to_datafusion_plan   │
                │          29                                                                                                                 └────────────────────────┘
                ▼           │
           ┌────────────────┴──────┐
           │     optimize_plan     │
           └───────────────────────┘

</code></pre>
<ol>
<li>The received request will be forwarded to <code>handle_sql</code> after various protocol conversions, and since the request may not be processed by this node, it may need to be forwarded to <code>maybe_forward_sql_query</code> to handle the forwarding logic.</li>
<li>After constructing the <code>ForwardRequest</code> in <code>maybe_forward_sql_query</code>, call <code>forward</code></li>
<li>After constructing the <code>RouteRequest</code> in <code>forward</code>, call <code>route</code></li>
<li>Use <code>route</code> to get the destination node <code>endpoint</code> and return to <code>forward</code>.</li>
<li>Call <code>forward_with_endpoint</code> to forward the request</li>
<li>return <code>forward</code></li>
<li>return <code>maybe_forward_sql_query</code></li>
<li>return <code>handle_sql</code></li>
<li>If this is a <code>Local</code> request, call <code>fetch_sql_query_output</code> to process it</li>
<li>Call <code>parse_sql</code> to parse <code>sql</code> into <code>Statment</code></li>
<li>return <code>fetch_sql_query_output</code></li>
<li>Call <code>statement_to_plan</code> with <code>Statment</code></li>
<li>Construct <code>Planner</code> with <code>ctx</code> and <code>Statment</code>, and call the <code>statement_to_plan</code> method of <code>Planner</code></li>
<li>The <code>planner</code> will call the corresponding <code>planner</code> method for the requested category, at this point our <code>sql</code> is a query and will call <code>sql_statement_to_plan</code></li>
<li>Call <code>sql_statement_to_datafusion_plan</code> , which will generate the <code>datafusion</code> object, and then call <code>SqlToRel::sql_statement_to_plan</code></li>
<li>The generated logical plan is returned from <code>SqlToRel::sql_statement_to_plan</code></li>
<li>return</li>
<li>return</li>
<li>return</li>
<li>Call <code>execute_plan_involving_partition_table</code> (in the default configuration) for subsequent optimization and execution of this logical plan</li>
<li>Call <code>build_interpreter</code> to generate <code>Interpreter</code></li>
<li>return</li>
<li>Call <code>Interpreter's</code> <code>interpreter_execute_plan</code> method for logical plan execution.</li>
<li>The corresponding <code>execute</code> function is called, at this time the <code>sql</code> is a query, so the execute of the <code>SelectInterpreter</code> will be called</li>
<li>call <code>execute_logical_plan</code> , which will call <code>build_df_session_ctx</code> to generate the optimizer</li>
<li><code>build_df_session_ctx</code> will use the <code>config</code> information to generate the corresponding context, first using datafusion and some custom optimization rules (in logical_optimize_rules()) to generate the logical plan optimizer, using <code>apply_adapters_for_physical_optimize_rules</code> to generate the physical plan optimizer</li>
<li>return optimizer</li>
<li>Call <code>optimize_plan</code>, using the optimizer just generated to first optimize the logical plan and then the physical plan</li>
<li>Return to optimized physical plan</li>
<li>execute physical plan</li>
<li>returned after execution</li>
<li>After collecting the results of all slices, return</li>
<li>return</li>
<li>return</li>
<li>return</li>
<li>Return to the upper layer for network protocol conversion and finally return to the request sender</li>
</ol>
<h3 id="write-3"><a class="header" href="#write-3">Write</a></h3>
<pre><code class="language-plaintext">┌───────┐      ┌───────┐      ┌───────┐
│       │──1──▶│       │──2──▶│       │
│Server │      │  SQL  │      │Catalog│
│       │◀─8───│       │◀─3───│       │
└───────┘      └───────┘      └───────┘
                │    ▲
               4│   7│
                │    │
                ▼    │
┌─────────────────────────────────────┐
│                                     │
│             Interpreter             │
│                                     │
└─────────────────────────────────────┘
      │    ▲
      │    │
      │    │
      │    │
      │    │       ┌──────────────────┐
      │    │       │                  │
     5│   6│       │   Query Engine   │
      │    │       │                  │
      │    │       └──────────────────┘
      │    │
      │    │
      │    │
      ▼    │
 ┌─────────────────────────────────────┐
 │                                     │
 │            Table Engine             │
 │                                     │
 └─────────────────────────────────────┘
</code></pre>
<p>Take <code>INSERT</code> SQL as an example. The figure above shows the query procedure and the numbers in it indicates the order of calling between the modules.</p>
<p>Here are the details:</p>
<ul>
<li>Server module chooses a proper rpc module (it may be HTTP, gRPC or mysql) to process the requests according the protocol used by the requests;</li>
<li>Parse SQL in the request by the parser;</li>
<li>With the parsed sql and the catalog/schema module, <a href="https://github.com/apache/arrow-datafusion">DataFusion</a> can generate the logical plan;</li>
<li>With the logical plan, the corresponding <code>Interpreter</code> is created and logical plan will be executed by it;</li>
<li>For the logical plan of normal <code>INSERT</code> SQL, it will be executed through <code>InsertInterpreter</code>;</li>
<li>In the <code>InsertInterpreter</code>, <code>write</code> method of <code>Table</code> provided <code>Analytic Engine</code> is called:
<ul>
<li>Write the data into <code>WAL</code> first;</li>
<li>Write the data into <code>MemTable</code> then;</li>
</ul>
</li>
<li>Before writing to <code>MemTable</code>, the memory usage will be checked. If the memory usage is too high, the flush process will be triggered:
<ul>
<li>Persist some old MemTables as <code>SST</code>s;</li>
<li>Store updates about the new <code>SST</code>s and the flushed sequence number of <code>WAL</code> to <code>Manifest</code>;</li>
<li>Delete the corresponding <code>WAL</code> entries;</li>
</ul>
</li>
<li>Server module responds to the client with the execution result.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><p>Note: Some of the features mentioned in the article have not yet been implemented.</p>
<h1 id="introduction-to-architecture-of-horaedb-cluster"><a class="header" href="#introduction-to-architecture-of-horaedb-cluster">Introduction to Architecture of HoraeDB Cluster</a></h1>
<h2 id="overview"><a class="header" href="#overview">Overview</a></h2>
<pre><code class="language-plaintext">┌───────────────────────────────────────────────────────────────────────┐
│                                                                       │
│                           HoraeMeta Cluster                           │
│                                                                       │
└───────────────────────────────────────────────────────────────────────┘
                              ▲               ▲                 ▲
                              │               │                 │
                              │               │                 │
                              ▼               ▼                 ▼
┌───────┐Route Info ┌HoraeDB─────┬┬─┐ ┌HoraeDB─────┬┬─┐ ┌HoraeDB─────┬┬─┐
│client │◀────────▶ │  │  │TableN││ │ │  │  │TableN││ │ │  │  │TableN││ │
└───────┘Write/Query└──Shard(L)──┴┴─┘ └──Shard(F)──┴┴─┘ └──Shard(F)──┴┴─┘
                            ▲ │                 ▲               ▲
                              │                 │               │
                            │ Write─────────┐   ├────Sync───────┘
                                            │   │
                            │     ┌────────┬▼───┴────┬──────────────────┐
              Upload/Download     │        │         │                  │
                    SST     │     │WAL     │Region N │                  │
                                  │Service │         │                  │
                            │     └────────┴─────────┴──────────────────┘

                            ▼
┌───────────────────────────────────────────────────────────────────────┐
│                                                                       │
│                            Object Storage                             │
│                                                                       │
└───────────────────────────────────────────────────────────────────────┘
</code></pre>
<p>The diagram above describes the architecture of a HoraeDB cluster, where some key concepts need to be explained:</p>
<ul>
<li><code>HoraeMeta Cluster</code>: Takes responsibilities for managing the metadata and resource scheduling of the cluster;</li>
<li><code>Shard(L)/Shard(F)</code>: Leader shard and follower shard consisting of multiple tables;</li>
<li><code>HoraeDB</code>: One HoraeDB instance consisting of multiple shards;</li>
<li><code>WAL Service</code>: Write-ahead log service for storing new-written real-time data;</li>
<li><code>Object Storage</code>: Object storage service for storing SST converted from memtable;</li>
</ul>
<p>From the architecture diagram above, it can be concluded that the compute and storage are separated in the HoraeDB cluster, which makes it easy to implement useful distributed features, such as elastic autoscaling of compute/storage resources, high availability, load balancing, and so on.</p>
<p>Let's dive into some of the key components mentioned above before explaining how these features are implemented.</p>
<h3 id="shard"><a class="header" href="#shard">Shard</a></h3>
<p><code>Shard</code> is the basic scheduling unit in the cluster, which consists of a group of tables. And the tables in a shard share the same region for better storage locality in the <code>WAL Service</code>, and because of this, it is efficient to recover the data of all tables in the shard by scanning the entire WAL region. For most of implementations of <code>WAL Service</code>, without the shard concept, it costs a lot to recover the table data one by one due to massive random IO, and this case will deteriorate sharply when the number of tables grows to a certain level.</p>
<p>A specific role, <code>Leader</code> or <code>Follower</code>, should be assigned to a shard. A pair of leader-follower shards share the same set of tables, and the leader shard can serve the write and query requests from the client while the follower shard can only serve the read-only requests, and must synchronize the newly written data from the WAL service in order to provide the latest snapshot for data retrieval. Actually, the follower is not needed if the high availability is not required, while with at least one follower, it takes only a short time to resume service by simply switching the <code>Follower</code> to <code>Leader</code> when the HoraeDB instance on which the leader shard exists crashes.</p>
<p>The diagram below concludes the relationship between HoraeDB instance, <code>Shard</code>, <code>Table</code>. As shown in the diagram, the leader and follower shards are interleaved on the HoraeDB instance.</p>
<pre><code class="language-plaintext">┌─HoraeDB Instance0──────┐     ┌─HoraeDB Instance1──────┐
│  ┌─Shard0(L)────────┐  │     │  ┌─Shard0(F)────────┐  │
│  │ ┌────┬────┬────┐ │  │     │  │ ┌────┬────┬────┐ │  │
│  │ │ T0 │ T1 │ T2 │ │  │     │  │ │ T0 │ T1 │ T2 │ │  │
│  │ └────┴────┴────┘ │  │     │  │ └────┴────┴────┘ │  │
│  └──────────────────┘  │     │  └──────────────────┘  │
│                        │     │                        │
│  ┌─Shard1(F)────────┐  │     │  ┌─Shard1(L)────────┐  │
│  │ ┌────┬────┬────┐ │  │     │  │ ┌────┬────┬────┐ │  │
│  │ │ T0 │ T1 │ T2 │ │  │     │  │ │ T0 │ T1 │ T2 │ │  │
│  │ └────┴────┴────┘ │  │     │  │ └────┴────┴────┘ │  │
│  └──────────────────┘  │     │  └──────────────────┘  │
└────────────────────────┘     └────────────────────────┘
</code></pre>
<p>Since <code>Shard</code> is the basic scheduling unit, it is natural to introduce some basic shard operations:</p>
<ul>
<li>Create/Drop table to/from a shard;</li>
<li>Open/Close a shard;</li>
<li>Split one shard into two shards;</li>
<li>Merge two shards into one shard;</li>
<li>Switch the role of a shard;</li>
</ul>
<p>With these basic shard operations, some complex scheduling logic can be implemented, e.g. perform an expansion by splitting one shard into two shards and migrating one of them to the new HoraeDB instance.</p>
<h3 id="horaemeta"><a class="header" href="#horaemeta">HoraeMeta</a></h3>
<p><code>HoraeMeta</code> is implemented by embedding an ETCD inside to ensure consistency and takes responsibilities for cluster metadata management and scheduling.</p>
<p>The cluster metadata includes:</p>
<ul>
<li>Table information, such as table name, table ID, and which cluster the table belongs to;</li>
<li>The mapping between table and shard and between shard and HoraeDB instance;</li>
<li>...</li>
</ul>
<p>As for the cluster scheduling work, it mainly includes:</p>
<ul>
<li>Receiving the heartbeats from the HoraeDB instances and determining the online status of these registered instances;</li>
<li>Assigning specific role shards to the registered HoraeDB instances;</li>
<li>Participating in table creation by assigning a unique table ID and the most appropriate shard to the table;</li>
<li>Performing load balancing through shard operations according to the load information sent with the heartbeats;</li>
<li>Performing expansion through shard operations when new instances are registered;</li>
<li>Initiating failover through shard operations when old instances go offline;</li>
</ul>
<h3 id="route"><a class="header" href="#route">Route</a></h3>
<p>In order to avoid the overhead of forwarding requests, the communication between clients and the HoraeDB instances is peer-to-peer, that is to say, the client should retrieve routing information from the server before sending any specific write/query requests.</p>
<p>Actually, the routing information is decided by the <code>HoraeMeta</code>, but clients are only allowed the access to it through the HoraeDB instances rather than <code>HoraeMeta</code>, to avoid potential performance issues on the <code>HoraeMeta</code>.</p>
<h3 id="wal-service--object-storage"><a class="header" href="#wal-service--object-storage">WAL Service &amp; Object Storage</a></h3>
<p>In the HoraeDB cluster, <code>WAL Service</code> and <code>Object Storage</code> exist as separate distributed systems featured with HA, data replication and scalability. Current distributed implementations for <code>WAL Service</code> includes <code>Kafka</code> and <code>OBKV</code> (access <code>OceanBase</code> by its table api), and the implementations for <code>Object Storage</code> include popular object storage services, such as AWS S3, Azure object storage and Aliyun OSS.</p>
<p>The two components are similar in that they are introduced to serve as the underlying storage layer for separating compute and storage, while the difference between two components is obvious that <code>WAL Service</code> is used to store the newly written data from the real-time write requests whose individual size is small but quantity is large, and <code>Object Storage</code> is used to store the read-friendly data files (SST) organized in the background, whose individual size is large and aggregate size is much larger.</p>
<p>The two components make it much easier to implement the horaedb cluster, which features horizontal scalability, high availability and load balancing.</p>
<h2 id="scalability"><a class="header" href="#scalability">Scalability</a></h2>
<p>Scalability is an important feature for a distributed system. Let's take a look at to how the horizontal scalability of the HoraeDB cluster is achieved.</p>
<p>First, the two storage components (<code>WAL Service</code> and <code>Object Storage</code>) should be horizontally scalable when deciding on the actual implementations for them, so the two storage services can be expanded separately if the storage capacity is not sufficient.</p>
<p>It will be a little bit complex when discussing the scalability of the compute service. Basically, these cases will bring the capacity problem:</p>
<ul>
<li>Massive queries on massive tables;</li>
<li>Massive queries on a single large table;</li>
<li>Massive queries on a normal table;</li>
</ul>
<p>For the first case, it is easy to achieve horizontal scalability just by assigning shards that are created or split from old shards to expanded HoraeDB instances.</p>
<p>For the second case, the table partitioning is proposed and after partitioning, massive queries are distributed across multiple HoraeDB instances.</p>
<p>And the last case is the most important and the most difficult. Actually, the follower shard can handle part of the queries, but the number of follower shards is limited by the throughput threshold of the synchronization from the WAL regions. As shown in the diagram below, a pure compute shard can be introduced if the followers are not enough to handle the massive queries. Such a shard is not required to synchronize data with the leader shard, and retrieves the newly written data from the leader/follower shard only when the query comes. As for the SSTs required by the query, they will be downloaded from <code>Object Storage</code> and cached afterwards. With the two parts of the data, the compute resources are fully utilized to execute the CPU-intensive query plan. As we can see, such a shard can be added with only a little overhead (retrieving some data from the leader/follower shard when it needs), so to some extent, the horizontal scalability is achieved.</p>
<pre><code class="language-plaintext">                                             ┌HoraeDB─────┬┬─┐
                            ┌──newly written─│  │  │TableN││ │
                            ▼                └──Shard(L/F)┴┴─┘
┌───────┐  Query  ┌HoraeDB─────┬┬─┐
│client │────────▶│  │  │TableN││ │
└───────┘         └──Shard─────┴┴─┘          ┌───────────────┐
                            ▲                │    Object     │
                            └───old SST──────│    Storage    │
                                             └───────────────┘
</code></pre>
<h2 id="high-availability"><a class="header" href="#high-availability">High Availability</a></h2>
<p>Assuming that <code>WAL service</code> and <code>Object Storage</code> are highly available, the high availability of the HoraeDB cluster can be achieved by such a procedure:</p>
<ul>
<li>When detecting that the heartbeat is broken, <code>HoraeMeta</code> determines that the HoraeDB instance is offline;</li>
<li>The follower shards whose paired leader shards exist on the offline instance are switched to leader shards for fast failover;</li>
<li>A slow failover can be achieved by opening the crashed shards on another instance if such follower shards don't exist.</li>
</ul>
<pre><code class="language-plaintext">┌─────────────────────────────────────────────────────────┐
│                                                         │
│                    HoraeMeta Cluster                    │
│                                                         │
└─────────────────────────────────────────────────────────┘
                             ▲
             ┌ ─ Heartbeat ─ ┤
                   Broken    │
             │               │
┌ HoraeDB Instance0 ─ ─ ─    │   ┌─HoraeDB Instance1──────┐                   ┌─HoraeDB Instance1──────┐
   ┌─Shard0(L)────────┐  │   │   │  ┌─Shard0(F)────────┐  │                   │  ┌─Shard0(L)────────┐  │
│  │ ┌────┬────┬────┐ │      │   │  │ ┌────┬────┬────┐ │  │                   │  │ ┌────┬────┬────┐ │  │
   │ │ T0 │ T1 │ T2 │ │  │   ├───│  │ │ T0 │ T1 │ T2 │ │  │                   │  │ │ T0 │ T1 │ T2 │ │  │
│  │ └────┴────┴────┘ │      │   │  │ └────┴────┴────┘ │  │                   │  │ └────┴────┴────┘ │  │
   └──────────────────┘  │   │   │  └──────────────────┘  │     Failover      │  └──────────────────┘  │
│                            │   ├─HoraeDB Instance2──────┤   ───────────▶    ├─HoraeDB Instance2──────┤
   ┌─Shard1(L)────────┐  │   │   │  ┌─Shard1(F)────────┐  │                   │  ┌─Shard1(L)────────┐  │
│  │ ┌────┬────┬────┐ │      │   │  │ ┌────┬────┬────┐ │  │                   │  │ ┌────┬────┬────┐ │  │
   │ │ T0 │ T1 │ T2 │ │  │   └───│  │ │ T0 │ T1 │ T2 │ │  │                   │  │ │ T0 │ T1 │ T2 │ │  │
│  │ └────┴────┴────┘ │          │  │ └────┴────┴────┘ │  │                   │  │ └────┴────┴────┘ │  │
   └──────────────────┘  │       │  └──────────────────┘  │                   │  └──────────────────┘  │
└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─        └────────────────────────┘                   └────────────────────────┘
</code></pre>
<h2 id="load-balancing"><a class="header" href="#load-balancing">Load Balancing</a></h2>
<p>HoraeMeta collects the instance load information contained in the received heartbeats to create a load overview of the whole cluster, according to which the load balancing can be implemented as an automatic mechanism:</p>
<ul>
<li>Pick a shard on a low-load instance for the newly created table;</li>
<li>Migrate a shard from a high-load instance load to another low-load instance;</li>
<li>Split the large shard on the high-load instance and migrate the split shards to other low-load instances;</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="storage"><a class="header" href="#storage">Storage</a></h1>
<p>The storage engine mainly provides the following two functions：</p>
<ol>
<li>Persistence of data</li>
<li>Under the premise of ensuring the correctness of the data, organize the data in the most reasonable way to meet the query needs of different scenarios.</li>
</ol>
<p>This document will introduce the internal implementation of the storage engine in HoraeDB. Readers can refer to the content here to explore how to use HoraeDB efficiently.</p>
<h1 id="overall-structure"><a class="header" href="#overall-structure">Overall Structure</a></h1>
<p>HoraeDB is a distributed storage system based on the share-nothing architecture.</p>
<p>Data between different servers is isolated from each other and does not affect each other. The storage engine in each stand-alone machine is a variant of <a href="https://en.wikipedia.org/wiki/Log-structured_merge-tree">log-structured merge-tree</a>, which is optimized for time-series scenarios. The following figure shows its core components:</p>
<p><img src="design/../../resources/images/storage-overview.svg" alt="" /></p>
<h2 id="write-ahead-log-wal"><a class="header" href="#write-ahead-log-wal">Write Ahead Log (WAL)</a></h2>
<p>A write request will be written to</p>
<ol>
<li>memtable in memory</li>
<li>WAL in durable storage</li>
</ol>
<p>Since memtable is not persisted to the underlying storage system in real time, so WAL is required to ensure the reliability of the data in memtable.</p>
<p>On the other hand, due to the design of the <a href="design/cluster.html">distributed architecture</a>, WAL itself is required to be highly available. Now there are following implementations in HoraeDB:</p>
<ul>
<li><a href="design/wal_on_rocksdb.html">Local disk</a> (based on <a href="http://rocksdb.org/">RocksDB</a>, no distributed high availability)</li>
<li><a href="https://www.oceanbase.com">OceanBase</a></li>
<li><a href="design/wal_on_kafka.html">Kafka</a></li>
</ul>
<h2 id="memtable-1"><a class="header" href="#memtable-1">Memtable</a></h2>
<p>Memtable is a memory data structure used to hold recently written table data. Different tables have its corresponding memtable.</p>
<p>Memtable is read-write by default (aka active), and when the write reaches some threshold, it will become read-only and be replaced by a new memtable.</p>
<p>The read-only memtable will be flushed to the underlying storage system in SST format by background thread. After flush is completed, the read-only memtable can be destroyed, and the corresponding data in WAL can also be deleted.</p>
<h2 id="sorted-string-tablesst"><a class="header" href="#sorted-string-tablesst">Sorted String Table（SST）</a></h2>
<p>SST is a persistent format for data, which is stored in the order of primary keys of table. Currently, HoraeDB uses parquet format for this.</p>
<p>For HoraeDB, SST has an important option: segment_duration, only SST within the same segment can be merged, which is benefical for time-series data. And it is also convenient to eliminate expired data.</p>
<p>In addition to storing the original data, the statistical information of the data will also be stored in the SST to speed up the query, such as the maximum value, the minimum value, etc.</p>
<h2 id="compactor"><a class="header" href="#compactor">Compactor</a></h2>
<p>Compactor can merge multiple small SST files into one, which is used to solve the problem of too many small files. In addition, Compactor will also delete expired data and duplicate data during the compaction. In future, compaction maybe add more task, such as downsample.</p>
<p>The current compaction strategy in HoraeDB reference Cassandra:</p>
<ul>
<li><a href="https://cassandra.apache.org/doc/latest/cassandra/operating/compaction/stcs.html">SizeTieredCompactionStrategy</a></li>
<li><a href="https://cassandra.apache.org/doc/latest/cassandra/operating/compaction/twcs.html">TimeWindowCompactionStrategy</a></li>
</ul>
<h2 id="manifest-1"><a class="header" href="#manifest-1">Manifest</a></h2>
<p>Manifest records metadata of table, SST file, such as: the minimum and maximum timestamps of the data in an SST.</p>
<p>Due to the design of the distributed architecture, the manifest itself is required to be highly available. Now in HoraeDB, there are mainly the following implementations：</p>
<ul>
<li>WAL</li>
<li>ObjectStore</li>
</ul>
<h2 id="objectstore"><a class="header" href="#objectstore">ObjectStore</a></h2>
<p>ObjectStore is place where data (i.e. SST) is persisted.</p>
<p>Generally speaking major cloud vendors should provide corresponding services, such as Alibaba Cloud's OSS and AWS's S3.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="wal-1"><a class="header" href="#wal-1">Wal</a></h1>
<ul>
<li><a href="design/wal_on_rocksdb.html">WAL on RocksDB</a></li>
<li><a href="design/wal_on_kafka.html">WAL on Kafka</a></li>
<li>WAL on OceanBase will be introduced in later.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="wal-on-rocksdb"><a class="header" href="#wal-on-rocksdb">WAL on RocksDB</a></h1>
<h2 id="architecture-1"><a class="header" href="#architecture-1">Architecture</a></h2>
<p>In this section we present a standalone WAL implementation (based on RocksDB). Write-ahead logs(hereinafter referred to as logs) of tables are managed here by table, and we call the corresponding storage data structure <code>TableUnit</code>. All related data (logs or some metadata) is stored in a single column family for simplicity.</p>
<pre><code class="language-text">            ┌───────────────────────────────┐
            │         HoraeDB               │
            │                               │
            │ ┌─────────────────────┐       │
            │ │         WAL         │       │
            │ │                     │       │
            │ │        ......       │       │
            │ │                     │       │
            │ │  ┌────────────────┐ │       │
 Write ─────┼─┼──►   TableUnit    │ │Delete │
            │ │  │                │ ◄────── │
 Read  ─────┼─┼──► ┌────────────┐ │ │       │
            │ │  │ │ RocksDBRef │ │ │       │
            │ │  │ └────────────┘ │ │       │
            │ │  │                | |       |
            │ │  └────────────────┘ │       │
            │ │        ......       │       │
            │ └─────────────────────┘       │
            │                               │
            └───────────────────────────────┘
</code></pre>
<h2 id="data-model-1"><a class="header" href="#data-model-1">Data Model</a></h2>
<h3 id="common-log-format"><a class="header" href="#common-log-format">Common Log Format</a></h3>
<p>We use the common key and value format here.
Here is the defined key format, and the following is introduction for fields in it:</p>
<ul>
<li><code>namespace</code>: multiple instances of WAL can exist for different purposes (e.g. manifest also needs wal). The namespace is used to distinguish them.</li>
<li><code>region_id</code>: in some WAL implementations we may need to manage logs from multiple tables, region is the concept to describe such a set of table logs. Obviously the region id is the identification of the region.</li>
<li><code>table_id</code>: identification of the table logs to which they belong.</li>
<li><code>sequence_num</code>: each login table can be assigned an identifier, called a sequence number here.</li>
<li><code>version</code>: for compatibility with old and new formats.</li>
</ul>
<pre><code class="language-text">+---------------+----------------+-------------------+--------------------+-------------+
| namespace(u8) | region_id(u64) |   table_id(u64)   |  sequence_num(u64) | version(u8) |
+---------------+----------------+-------------------+--------------------+-------------+
</code></pre>
<p>Here is the defined value format, <code>version</code> is the same as the key format, <code>payload</code> can be understood as encoded log content.</p>
<pre><code class="language-text">+-------------+----------+
| version(u8) | payload  |
+-------------+----------+
</code></pre>
<h3 id="metadata"><a class="header" href="#metadata">Metadata</a></h3>
<p>The metadata here is stored in the same key-value format as the log. Actually only the last flushed sequence is stored in this implementation. Here is the defined metadata key format and field instructions:</p>
<ul>
<li><code>namespace</code>, <code>table_id</code>, <code>version</code> are the same as the log format.</li>
<li><code>key_type</code>, used to define the type of metadata. MaxSeq now defines that metadata of this type will only record the most recently flushed sequence in the table.
Because it is only used in wal on RocksDB, which manages the logs at table level, so there is no region id in this key.</li>
</ul>
<pre><code class="language-text">+---------------+--------------+----------------+-------------+
| namespace(u8) | key_type(u8) | table_id(u64)  | version(u8) |
+---------------+--------------+----------------+-------------+
</code></pre>
<p>Here is the defined metadata value format, as you can see, just the <code>version</code> and <code>max_seq</code>(flushed sequence) in it:</p>
<pre><code class="language-text">+-------------+--------------+
| version(u8) | max_seq(u64) |
+-------------+--------------+
</code></pre>
<h2 id="main-process"><a class="header" href="#main-process">Main Process</a></h2>
<ul>
<li>Open <code>TableUnit</code>:
<ul>
<li>Read the latest log entry of all tables to recover the next sequence numbers of tables mainly.</li>
<li>Scan the metadata to recover next sequence num as a supplement (because some table has just triggered flush and no new written logs after this, so no logs exists now).</li>
</ul>
</li>
<li>Write and read logs. Just write and read key-value from RocksDB.</li>
<li>Delete logs. For simplicity It will remove corresponding logs synchronously.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="wal-on-kafka"><a class="header" href="#wal-on-kafka">WAL on Kafka</a></h1>
<h2 id="architecture-2"><a class="header" href="#architecture-2">Architecture</a></h2>
<p>In this section we present a distributed WAL implementation(based on Kafka). Write-ahead logs(hereinafter referred to as logs) of tables are managed here by region, which can be simply understood as a shared log file of multiple tables.</p>
<p>As shown in the following figure, regions are mapped to topics(with only one partition) in Kafka. And usually two topics are needed by a region, one is used for storing logs and the other is used for storing metadata.</p>
<pre><code class="language-text">                                                 ┌──────────────────────────┐
                                                 │         Kafka            │
                                                 │                          │
                                                 │         ......           │
                                                 │                          │
                                                 │ ┌─────────────────────┐  │
                                                 │ │      Meta Topic     │  │
                                                 │ │                     │  │
                                         Delete  │ │ ┌─────────────────┐ │  │
               ┌──────────────────────┐  ┌───────┼─┼─►    Partition    │ │  │
               │       HoraeDB        │  │       │ │ │                 │ │  │
               │                      │  │       │ │ └─────────────────┘ │  │
               │ ┌──────────────────┐ │  │       │ │                     │  │
               │ │       WAL        │ │  │       │ └─────────────────────┘  │
               │ │      ......      │ │  │       │                          │
               │ │ ┌──────────────┐ │ │  │       │ ┌──────────────────────┐ │
               │ │ │    Region    │ │ │  │       │ │     Data Topic       │ │
               │ │ │              ├─┼─┼──┘       │ │                      │ │
               | | | ┌──────────┐ │ │ │          │ │ ┌──────────────────┐ │ │
               │ │ │ │ Metadata │ │ │ │          │ │ │    Partition     │ │ │
               │ │ │ └──────────┘ │ │ │    Write │ │ │                  │ │ │
Write ─────────┼─┼─►              ├─┼─┼───┐      │ │ │ ┌──┬──┬──┬──┬──┐ │ │ │
               │ │ │ ┌──────────┐ │ │ │   └──────┼─┼─┼─►  │  │  │  │  ├─┼─┼─┼────┐
               │ │ │ │  Client  │ │ │ │          │ │ │ └──┴──┴──┴──┴──┘ │ │ │    │
Read ◄─────────┼─┼─┤ └──────────┘ │ │ │          │ │ │                  │ │ │    │
               │ │ │              │ │ │          │ │ └──────────────────┘ │ │    │
               │ │ └──▲───────────┘ │ │          │ │                      │ │    │
               │ │    │ ......      │ │          │ └──────────────────────┘ │    │
               │ └────┼─────────────┘ │          │         ......           │    │
               │      │               │          └──────────────────────────┘    │
               └──────┼───────────────┘                                          │
                      │                                                          │
                      │                                                          │
                      │                        Read                              │
                      └──────────────────────────────────────────────────────────┘
</code></pre>
<h2 id="data-model-2"><a class="header" href="#data-model-2">Data Model</a></h2>
<h3 id="log-format"><a class="header" href="#log-format">Log Format</a></h3>
<p>The common log format described in <a href="design/wal_on_rocksdb.html">WAL on RocksDB</a> is used here.</p>
<h3 id="metadata-1"><a class="header" href="#metadata-1">Metadata</a></h3>
<p>Each region will maintain its metadata both in memory and in Kafka, we call it <code>RegionMeta</code> here. It can be thought of as a map, taking table id as a key and <code>TableMeta</code> as a value.
We briefly introduce the variables in <code>TableMeta</code> here:</p>
<ul>
<li><code>next_seq_num</code>, the sequence number allocated to the next log entry.</li>
<li><code>latest_marked_deleted</code>, the last flushed sequence number, all logs in the table with a lower sequence number than it can be removed.</li>
<li><code>current_high_watermark</code>, the high watermark in the Kafka partition after the last writing of this table.</li>
<li><code>seq_offset_mapping</code>, mapping from sequence numbers to offsets will be done on every write and will removed to the updated <code>latest_marked_deleted</code> after flushing.</li>
</ul>
<pre><code>┌─────────────────────────────────────────┐
│              RegionMeta                 │
│                                         │
│ Map&lt;TableId, TableMeta&gt; table_metas     │
└─────────────────┬───────────────────────┘
                  │
                  │
                  │
                  └─────┐
                        │
                        │
 ┌──────────────────────┴──────────────────────────────┐
 │                       TableMeta                     │
 │                                                     │
 │ SequenceNumber next_seq_num                         │
 │                                                     │
 │ SequenceNumber latest_mark_deleted                  │
 │                                                     │
 │ KafkaOffset high_watermark                          │
 │                                                     │
 │ Map&lt;SequenceNumber, KafkaOffset&gt; seq_offset_mapping │
 └─────────────────────────────────────────────────────┘
</code></pre>
<h2 id="main-process-1"><a class="header" href="#main-process-1">Main Process</a></h2>
<p>We focus on the main process in one region, following process will be introduced:</p>
<ul>
<li>Open or create region.</li>
<li>Write and read logs.</li>
<li>Delete logs.</li>
</ul>
<h3 id="open-or-create-region"><a class="header" href="#open-or-create-region">Open or Create Region</a></h3>
<h4 id="steps"><a class="header" href="#steps">Steps</a></h4>
<ul>
<li>Search the region in the opened namespace.</li>
<li>If the region found, the most important thing we need to do is to recover its metadata, we will introduce this later.</li>
<li>If the region not found and auto creating is defined, just create the corresponding topic in Kafka.</li>
<li>Add the found or created region to cache, return it afterwards.</li>
</ul>
<h4 id="recovery"><a class="header" href="#recovery">Recovery</a></h4>
<p>As mentioned above, the <code>RegionMeta</code> is actually a map of the <code>TableMeta</code>. So here we will focus on recovering a specific <code>TableMeta</code>, and examples will be given to better illustrate this process.</p>
<ul>
<li>First, recover the <code>RegionMeta</code> from snapshot. We will take a snapshot of the <code>RegionMeta</code> in some scenarios (e.g. mark logs deleted, clean logs) and put it to the meta topic. The snapshot is actually the <code>RegionMeta</code> at a particular point in time. When recovering a region, we can use it to avoid scanning all logs in the data topic. The following is the example, we recover from the snapshot taken at the time when Kafka high watermark is 64:</li>
</ul>
<pre><code class="language-text">high watermark in snapshot: 64

 ┌──────────────────────────────┐
 │         RegionMeta           │
 │                              │
 │          ......              │
 │ ┌──────────────────────────┐ │
 │ │       TableMeta          │ │
 │ │                          │ │
 │ │ next_seq_num: 5          │ │
 │ │                          │ │
 │ │ latest_mark_deleted: 2   │ │
 │ │                          │ │
 │ │ high_watermark: 32       │ │
 │ │                          │ │
 │ │ seq_offset_mapping:      │ │
 │ │                          │ │
 │ │ (2, 16) (3, 16) (4, 31)  │ │
 │ └──────────────────────────┘ │
 │          ......              │
 └──────────────────────────────┘
</code></pre>
<ul>
<li>Recovering from logs. After recovering from snapshot, we can continue to recover by scanning logs in data topic from the Kafka high watermark when snapshot is taken, and obviously that avoid scanning the whole data topic. Let's see the example:</li>
</ul>
<pre><code class="language-text">┌────────────────────────────────────┐
│                                    │
│    high_watermark in snapshot: 64  │
│                                    │
│  ┌──────────────────────────────┐  │
│  │         RegionMeta           │  │
│  │                              │  │
│  │          ......              │  │
│  │ ┌──────────────────────────┐ │  │
│  │ │       TableMeta          │ │  │
│  │ │                          │ │  │
│  │ │ next_seq_num: 5          │ │  │                  ┌────────────────────────────────┐
│  │ │                          │ │  │                  │          RegionMeta            │
│  │ │ latest_mark_deleted: 2   │ │  │                  │                                │
│  │ │                          │ │  │                  │            ......              │
│  │ │ high_watermark: 32       │ │  │                  │ ┌────────────────────────────┐ │
│  │ │                          │ │  │                  │ │         TableMeta          │ │
│  │ │ seq_offset_mapping:      │ │  │                  │ │                            │ │
│  │ │                          │ │  │                  │ │ next_seq_num: 8            │ │
│  │ │ (2, 16) (3, 16) (4, 31)  │ │  │                  │ │                            │ │
│  │ └──────────────────────────┘ │  │                  │ │ latest_mark_deleted: 2     │ │
│  │          ......              │  │                  │ │                            │ │
│  └──────────────────────────────┘  ├──────────────────► │ high_watermark: 32         │ │
│                                    │                  │ │                            │ │
│ ┌────────────────────────────────┐ │                  │ │ seq_offset_mapping:        │ │
│ │          Data topic            │ │                  │ │                            │ │
│ │                                │ │                  │ │ (2, 16) (3, 16) (4, 31)    │ │
│ │ ┌────────────────────────────┐ │ │                  │ │                            │ │
│ │ │        Partition           │ │ │                  │ │ (5, 72) (6, 81) (7, 90)    │ │
│ │ │                            │ │ │                  │ │                            │ │
│ │ │ ┌────┬────┬────┬────┬────┐ │ │ │                  │ └────────────────────────────┘ │
│ │ │ │ 64 │ 65 │ ...│ 99 │100 │ │ │ │                  │             ......             │
│ │ │ └────┴────┴────┴────┴────┘ │ │ │                  └────────────────────────────────┘
│ │ │                            │ │ │
│ │ └────────────────────────────┘ │ │
│ │                                │ │
│ └────────────────────────────────┘ │
│                                    │
└────────────────────────────────────┘
</code></pre>
<h3 id="write-and-read-logs"><a class="header" href="#write-and-read-logs">Write and Read Logs</a></h3>
<p>The writing and reading process in a region is simple.</p>
<p>For writing:</p>
<ul>
<li>Open the specified region (auto create it if necessary).</li>
<li>Put the logs to specified Kafka partition by client.</li>
<li>Update <code>next_seq_num</code>, <code>current_high_watermark</code> and <code>seq_offset_mapping</code> in corresponding <code>TableMeta</code>.</li>
</ul>
<p>For reading:</p>
<ul>
<li>Open the specified region.</li>
<li>Just read all the logs of the region, and the split and replay work will be done by the caller.</li>
</ul>
<h3 id="delete-logs"><a class="header" href="#delete-logs">Delete Logs</a></h3>
<p>Log deletion can be divided into two steps:</p>
<ul>
<li>Mark the logs deleted.</li>
<li>Do delayed cleaning work periodically in a background thread.</li>
</ul>
<h4 id="mark"><a class="header" href="#mark">Mark</a></h4>
<ul>
<li>Update <code>latest_mark_deleted</code> and <code>seq_offset_mapping</code>(just retain the entries whose's sequence &gt;= updated latest_mark_deleted) in <code>TableMeta</code>.</li>
<li>Maybe we need to make and sync the <code>RegionMeta</code> snapshot to Kafka while dropping table.</li>
</ul>
<h4 id="clean"><a class="header" href="#clean">Clean</a></h4>
<p>The cleaning logic done in a background thread called cleaner:</p>
<ul>
<li>Make <code>RegionMeta</code> snapshot.</li>
<li>Decide whether to clean the logs based on the snapshot.</li>
<li>If so, sync the snapshot to Kafka first, then clean the logs.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><p><strong>Note: This feature is still in development, and the API may change in the future.</strong></p>
<h1 id="table-partitioning"><a class="header" href="#table-partitioning">Table Partitioning</a></h1>
<p>This chapter discusses <code>PartitionTable</code>.</p>
<p>The partition table syntax used by HoraeDB is similar to that of <a href="https://dev.mysql.com/doc/refman/8.0/en/partitioning-types.html">MySQL</a>.</p>
<p>General partition tables include <code>Range Partitioning</code>, <code>List Partitoning</code>, <code>Hash Partitioning</code>, and <code>Key Partititioning</code>.</p>
<p>HoraeDB currently only supports <code>Key Partitioning</code>.</p>
<h2 id="architecture-3"><a class="header" href="#architecture-3">Architecture</a></h2>
<p>Similar to MySQL, different portions of a partition table are stored as separate tables in different locations.</p>
<p>Currently designed, a partition table can be opened on multiple HoraeDB nodes, supports writing and querying at the same time, and can be expanded horizontally.</p>
<p>As shown in the figure below, <code>PartitionTable</code> is opened on node0 and node1, and the physical subtables where the actual data are stored on node2 and node3.</p>
<pre><code>                        ┌───────────────────────┐      ┌───────────────────────┐
                        │Node0                  │      │Node1                  │
                        │   ┌────────────────┐  │      │  ┌────────────────┐   │
                        │   │ PartitionTable │  │      │  │ PartitionTable │   │
                        │   └────────────────┘  │      │  └────────────────┘   │
                        │            │          │      │           │           │
                        └────────────┼──────────┘      └───────────┼───────────┘
                                     │                             │
                                     │                             │
             ┌───────────────────────┼─────────────────────────────┼───────────────────────┐
             │                       │                             │                       │
┌────────────┼───────────────────────┼─────────────┐ ┌─────────────┼───────────────────────┼────────────┐
│Node2       │                       │             │ │Node3        │                       │            │
│            ▼                       ▼             │ │             ▼                       ▼            │
│ ┌─────────────────────┐ ┌─────────────────────┐  │ │  ┌─────────────────────┐ ┌─────────────────────┐ │
│ │                     │ │                     │  │ │  │                     │ │                     │ │
│ │     SubTable_0      │ │     SubTable_1      │  │ │  │     SubTable_2      │ │     SubTable_3      │ │
│ │                     │ │                     │  │ │  │                     │ │                     │ │
│ └─────────────────────┘ └─────────────────────┘  │ │  └─────────────────────┘ └─────────────────────┘ │
│                                                  │ │                                                  │
└──────────────────────────────────────────────────┘ └──────────────────────────────────────────────────┘
</code></pre>
<h3 id="key-partitioning"><a class="header" href="#key-partitioning">Key Partitioning</a></h3>
<p><code>Key Partitioning</code> supports one or more column calculations, using the hash algorithm provided by HoraeDB for calculations.</p>
<p>Use restrictions:</p>
<ul>
<li>Only tag column is supported as partition key.</li>
<li><code>LINEAR KEY</code> is not supported yet.</li>
</ul>
<p>The table creation statement for the key partitioning is as follows:</p>
<pre><code class="language-sql">CREATE TABLE `demo`(
    `name`string TAG,
    `id` int TAG,
    `value` double NOT NULL,
    `t` timestamp NOT NULL,
    TIMESTAMP KEY(t)
    ) PARTITION BY KEY(name) PARTITIONS 2 ENGINE = Analytic
</code></pre>
<p>Refer to <a href="https://dev.mysql.com/doc/refman/5.7/en/partitioning-key.html">MySQL key partitioning</a>.</p>
<h2 id="query-3"><a class="header" href="#query-3">Query</a></h2>
<p>Since the partition table data is actually stored in different physical tables, it is necessary to calculate the actual requested physical table according to the query request when querying.</p>
<p>The query will calculate the physical table to be queried according to the query parameters, and then remotely request the node where the physical table is located to obtain data through the HoraeDB internal service <a href="https://github.com/apache/incubator-horaedb/blob/89dca646c627de3cee2133e8f3df96d89854c1a3/server/src/grpc/remote_engine_service/mod.rs">remote engine</a> (support predicate pushdown).</p>
<p>The implementation of the partition table is in <a href="https://github.com/apache/incubator-horaedb/blob/89dca646c627de3cee2133e8f3df96d89854c1a3/analytic_engine/src/table/partition.rs">PartitionTableImpl</a>.</p>
<ul>
<li>Step 1: Parse query sql and calculate the physical table to be queried according to the query parameters.</li>
<li>Step 2: Query data of physical table.</li>
<li>Step 3: Compute with the raw data.</li>
</ul>
<pre><code>                       │
                     1 │
                       │
                       ▼
               ┌───────────────┐
               │Node0          │
               │               │
               │               │
               └───────────────┘
                       ┬
                2      │       2
        ┌──────────────┴──────────────┐
        │              ▲              │
        │       3      │       3      │
        ▼ ─────────────┴───────────── ▼
┌───────────────┐             ┌───────────────┐
│Node1          │             │Node2          │
│               │             │               │
│               │             │               │
└───────────────┘             └───────────────┘
</code></pre>
<h3 id="key-partitioning-1"><a class="header" href="#key-partitioning-1">Key partitioning</a></h3>
<ul>
<li>Filters like <code>and</code>, <code>or</code>, <code>in</code>, <code>=</code> will choose specific SubTables.</li>
<li>Fuzzy matching filters like <code>&lt;</code>, <code>&gt;</code> are also supported, but may have poor performance since it will scan all physical tables.</li>
</ul>
<p><code>Key partitioning</code> rule is implemented in <a href="https://github.com/apache/incubator-horaedb/blob/89dca646c627de3cee2133e8f3df96d89854c1a3/table_engine/src/partition/rule/key.rs">KeyRule</a>.</p>
<h2 id="write-4"><a class="header" href="#write-4">Write</a></h2>
<p>The write process is similar to the query process.</p>
<p>First, according to the partition rules, the write request is split into different partitioned physical tables, and then sent to different physical nodes through the remote engine for actual data writing.</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js" charset="utf-8"></script>
        <script src="mark.min.js" charset="utf-8"></script>
        <script src="searcher.js" charset="utf-8"></script>

        <script src="clipboard.min.js" charset="utf-8"></script>
        <script src="highlight.js" charset="utf-8"></script>
        <script src="book.js" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        <script src="sidebar.js"></script>

        <script>
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>

        <script type="text/javascript">
         let langs = [
                                'cn',
                                'en'
                            ];

        (function langs() {
            var html = document.querySelector('html');
            var langToggleButton = document.getElementById('lang-toggle');
            var langPopup = document.getElementById('lang-list');

            function showLangs() {
                langPopup.style.display = 'block';
                langToggleButton.setAttribute('aria-expanded', true);
            }

            function hideLangs() {
                langPopup.style.display = 'none';
                langToggleButton.setAttribute('aria-expanded', false);
                langToggleButton.focus();
            }

            langToggleButton.addEventListener('click', function () {
                if (langPopup.style.display === 'block') {
                    hideLangs();
                } else {
                    showLangs();
                }
            });

            langPopup.addEventListener('click', function (e) {
               let langs = [
                        'cn',
                        'en'
                    ];
                var lang = e.target.id || e.target.parentElement.id;
                var path_list=[];
                window.location.pathname.split('/').map((s, idx) => {
                    if (idx == 1){
                        if(!langs.includes(s)){
                            path_list.push(lang);
                            path_list.push(s);
                        }else{
                            path_list.push(lang);
                        }
                    }else{
                        path_list.push(s);
                    }
                });
                console.log("path:");
                window.location.pathname = path_list.join('/');
            });

            langPopup.addEventListener('focusout', function(e) {
                // e.relatedTarget is null in Safari and Firefox on macOS (see workaround below)
                if (!!e.relatedTarget && !langToggleButton.contains(e.relatedTarget) && !langPopup.contains(e.relatedTarget)) {
                    hideLangs();
                }
            });

            // Should not be needed, but it works around an issue on macOS & iOS: https://github.com/rust-lang-nursery/mdBook/issues/628
            document.addEventListener('click', function(e) {
                if (langPopup.style.display === 'block' && !langToggleButton.contains(e.target) && !langPopup.contains(e.target)) {
                    hideLangs();
                }
            });

        })();
        </script>


    </body>
</html>
